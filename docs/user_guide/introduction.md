## Introduction to LiveSYNC

### What's new

### What is LiveSYNC?

#### Human Communication is Visual

During the past few decades the way humans communicate with each other has permanently changed.
First we learned to connect computers together to make a giant web where messages can travel
from one corner of the planet to another in a fraction of a second. That was a major breakthrough
in communication, although in its early days data transfer rates were practical mainly for
transmitting text.

Along with many technological advancements, which made the web ever faster, step-by-step we started
to use it for other forms of communication that required *higher data rates*: photographs,
voice calls, music, and finally high-definition video.

As a consequence, we began to see a paradigm shift in human communication from textual to visual
representation: Instead of writing long carefully thought out emails we started to draft lots
of short messages, where tiny iconic images called smileys where injected to communicate common
feelings via facial expressions. Then, we further reduced textual part to a simple caption or a
foreword to a photograph we had taken: the image had become the message itself. And now we have
moved on to video and live in the age of Youtube stars when anyone can start their own TV show
with a press of a button.

> The world now is more visual that it has ever been in human history. 93% of all human
> communication is visual. ([Ref 1])

Why are we so fond of photos and videos? What is wrong in plain old books and letters? There is
clearly room for all kinds of communication methods, as the new ones do not *replace* the old ones;
they add to the options that we have. Yet, we must embrace those methods that come so natural from
us: the ones of visual kind. We can even argue that humans *are* visual by nature:

> Humans process visuals 60 000x faster than text. 81% of people only skim content they read online.
> People learn 40% better when there are visuals. ([Ref 1])

#### It's All About Presence

Looking back, it seems that whenever the transfer rates of Internet connections have taken a step
forward, we've immediately found a new, richer communication method that provides a better way to
capture and share what is happening around us: a way that communicates *presence* better by adding
some essential parameter to the equation that was missing before. That is how those much sought out
killer apps are made again and again.

Can you imagine what will be the next step on this path? One way to understand it all is to see
the changes as stepping stones towards true *telepresence*; a combination of technologies that
allow a person to *feel* as if they were present in another place, maybe also in another time.
This is where we are heading to, and it is not that far off anymore.

We can already see that during the past five years technology has matured enough to solve one
more profounding road block: the image frame that limits our field of view. Just think about it:
how could you ever feel *being there* if you weren't even able to turn your head to look around?
By minimum, you should be able to freely observe the surroundings by looking at any direction
you want, at any moment of time - and experience the view and soundscape change corresponding
to your head movements.

The technologies that allow capturing the world surrounding us are called 360-degree photography
or 360-degree videography, accompanied with spatial audio. The content is typically experienced
through ordinary computer screens, smartphones, tablets, or virtual reality headsets. For this
purpose, new kind of image viewers and video players have been developed to allow interacting
with the content at presentation time.

It is interesting to notice that moving from traditional video to 360-degree spherical video
requires over tenfold increase in data rate. We have been able to watch movies in 4K quality over
Internet quite some time now, so it is not a coincident that 360-degree content is becoming popular
just now when Internet is becoming fast enough to carry the amount of data required for it. Fiber and
5G networks are exactly what is needed:

>Consider a single frame of 4K video that contains 3840x2160=8.3M pixels. If the camera that
captured the frame was zoomed out to produce a wide-angle shot the frame could contain ~90-degree
field-of-view horizontally, resulting to 3840/90=~43 pixels per degree. In order to produce full
360-degree view at the same resolution (amount of pixels per degree), we'd need roughly four
similar frames side-by-side, resulting to 4x3840=15360 pixels horizontally. Since full spherical
image is 360 degrees horizontally but only 180 degrees vertically, we need only half of that number
vertically to  produce a full spherical image. Hence, to experience "4K quality in 360-degrees",
our image frame would have to be 15360x7680=118M pixels. This is over 14 times the amount of pixels
in a 4K video frame! However, if we settle with "FullHD quality in 360-degrees" or 1920/90=21.3
pixels per degree then our image frame size becomes 7680x3840=29.5M pixels, which is "only" 3.5
times the amount of  pixels in a 4K video frame. In early 2019 360-degree cameras that capture
video in 7680x3840 resolution are becoming commonplace, which means that we are now entering the
"FullHD" time of 360-degree video, and "4K" time of 360-degree video is still a few years away.
We can also calculate it like this: a compressed 4K video requires roughly 40 Mbit/s bitrate,
hence scaling up with factor 3.5 yields 140 Mbit/s data rate for "FullHD quality in 360-degrees",
which could pass through best current mobile LTE networks that offer 300 Mbit/s peak data rate.
Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for "4K quality in 360-degrees".
We will need one gigabit Internet connection to view 360-degree video in perceived quality that
we know as "4K". Upcoming mobile 5G networks promise such speeds.

This is the big picture and the part where LiveSYNC as a tool enters the scene. 360-degree
imaging can be now considered largely a solved problem, both from content production and
presentation point of view, and we are mostly improving quality now. It is mature and cost-effective
enough already to be used in daily activities.

We can also see the fog disappearing and revealing the next stepping stone: while 360-degree
video allows the viewer to turn around, *volumetric video* will remove the limitation that prevents
the viewer from *taking a step* inside the world captured with cameras, and thus will make us feel
being present even more. That technology is now taking its first steps and needs a few more years
to begin to "work", and some more before the cameras and networks catch up.

There is no reason to wait, though: the future will always bring better, more advanced technology.
So let us consider what it means that we can now let go of the limitations of image frame, after
200 years of photography. That is already a miracle of its own!

#### Removing the Frame

The advances in technology are pushing another paradigm shift: many features of a photo that have
been traditionally decided in advance or by latest when the camera shutter closes, can now
be changed afterwards - up to the point where the person *viewing* the content becomes the decision
maker.

For example, retouching images is so commonplace that photoshopping has become a household word.
Light-field cameras allow focusing the image *after* it has been taken - a stunt that seems almost
impossible.

Another miracle is 360-degree video, where every direction is captured simultaneously and there are
no traces of a camera nor cameramen: the point of view simply seems to float in the air.
This has profound effects as we will soon see.

Traditionally, a photographer or a videographer has decided where to point his camera and how to
frame the shot: what is necessary and what can be left out. This decision is locked at the time of
capture, and becomes a permanent limitation: at viewing time it is not possible to see beyond the
frame, in fact the only possibility to re-frame the shot is to limit the viewing angle *even more*
by cropping the image.

Imagine a zoom lens that has an unlimited range. If you kept zooming in, you could see a grain
of dust on the surface of the Moon. But what happens when you keep zooming out? You will see more
and more of the surrounding area become visible in the image, although heavily distorted. And then,
the two sides of the image suddenly meet each other, producing a full 360-degree view.

While we may never be able to physically manufacture a lens that could capture everything at once,
digital image processing comes to the rescue and effectively combines the images captured through
two or more lenses by stitching them together to a (more-or-less) seamless representation.

360-degree photo and video panoramas (also known as *immersive videos*, *spherical videos*, or
*VR videos*) capture the world in every direction.

During playback it is the viewer who has control of the viewing direction: he can freely turn around
and even look up to the sky or down to his feet. In fact, we are so used to the frame that limits
the view that it feels like magic when you experience this freedom the very first time.

Making the most out of such content requires new kinds of tools. By minimum, a photo/video player
application must support special projections that are used for storing wide-angle panoramic
content, and provide means for the viewer to pan the image during playback. LiveSYNC is such a
tool, but also much more, as you will soon learn.

While traditional photos are great for creating memories and videos are perfect for telling stories,
the main driving force for shooting something in 360° is the feeling of *presence* that you get when
viewing the content. Even more immersive experiences can be achieved when 360° video is combined
with 360° audio (also known as positional or spatial audio), and viewed through a virtual reality
(VR) headset. You'll feel like *being there*.

This experience is in fact so immersive that it also isolates the viewer from the rest of the world.
It becomes a real problem when you are presenting content to another person and need to communicate
with him during playback, for example, to give instructions, ask for an opinion, or simply to share
the experience by chatting about it as the story unfolds. Yet, not being able to see what he sees
forces you to continuously ask the viewer to explain what's on the screen. How awkward!

LiveSYNC removes this barrier by creating a screen mirroring style copy of the view from the VR
headset to another device, typically a tablet computer. This view is updated in near real-time and
can be further shared to a big screen, for example a TV or a projector, so that even a large
audience - perhaps waiting for their own turn - can share the experience. Nobody likes waiting,
though. Fortunately, with LiveSYNC you can connect to multiple headsets, play content synchronized
in time, and observe the views of all of the devices simultaneously in a gorgeous video mosaic.
You can also mix in traditional photos and videos, for example your company's slide deck and promo
video. This makes LiveSYNC a versatile all-around presentation tool.

Yet, not all 360° content is made for entertainment. Imagine that you are selling your summer
cottage and made a set of 360° photos and videos so that potential buyers can virtually visit
the place without spending hours in a car. But wouldn't it be convenient if you could also add
a few notes to the images, to highlight things you want them to notice and remember? This is
a built-in feature in LiveSYNC: when presenting content, simply drag & drop icons from included
clipart sets over the photo or video - or create your own signs and use them instead. Is it
possible to add text too? Of course.

The enterprise version of LiveSYNC goes much further: you can use a built-in editor to create a
project, import media content, add interactive hotspots that can even fetch data from the network
in real-time, import 2D and 3D maps with camera paths, etc. A really useful feature is to be able
to export all the notes you've made as screenshots into a visual PDF report, and send it to your
colleagues right away. Take a 5-minute tour on your construction site with a 360° camera, add new
work instructions, export to PDF and mail away - all in 15 minutes. That's what we call *instant
digitalization*.

This is LiveSYNC - and we are only getting started. Welcome aboard!

[Ref 1]: https://tech.co/news/world-now-visual-ever-2017-01

### Key Concepts

TODO

### Understanding view mirroring

How does LiveSYNC actually work? What is possible and what is not? In general, there are two ways to mirror a live view of one device's screen to another:

1. **Streaming video (screencasting) **

    *Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection.*

    One benefit of this approach is that the mirrored view is an *exact* copy of what the other device is drawing on its screen. Another is that the mirroring feature can be implemented on the platform level and is thus available for all apps or the whole desktop environment.

    There are many downsides, too. Because it is essentially streaming high resolution video from one device to another, the method consumes a fair amount of CPU/GPU resources and power in *both* devices. It also requires a lot of network bandwidth when there is movement on screen, and it does not scale well: the network and the video decoder in the receiving end quickly become a bottleneck when more devices connect.

    Furthermore, when using a VR headset, we do not actually *want* to stream an exact copy of what is on screen. Instead of the distorted double barrel view we want to mirror a normal view.

2. **Streaming commands**

    *Principle: Integrate deep into the app and transmit only commands that allow the receiving end to reconstruct the view from the same assets.*

    The benefits of this method include low CPU/GPU and power consumption, trivial use of network bandwidth, and scalability. It is also possible to render the mirrored view a bit differently compared to the source device.

    The downside is that all devices must have local copies of the assets required for reconstructing the view. Also, with video content the resources of the control device can become a limitation in certain use cases. A control device may not have enough hardware resources to decode and play multiple videos simultaneously. Or, the same video simultaneously from different stream positions. This is not a problem, though, when all devices watch the same content in sync.

Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. Wifi does not work well enough and/or the organizers do not even allow using own Wifi hotspots.

As you probably guessed already, LiveSYNC uses type 2 method: streaming commands. This approach allows operation in crowded trade shows, especially when Bluetooth technology is used for communication. Also, observing a large number of viewer devices is possible. Etc.
