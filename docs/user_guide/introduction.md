# 1. Introduction

## Welcome to LiveSYNC

Since you are reading this user guide, you must have decided to learn more about 360-degree photos, videos, and a tool called LiveSYNC. That is awesome! We, too, believe it is time to finally get rid of the limitations of image frame. It is time to start capturing the World as we humans experience it. In a way that a viewer can look around and feel like *being there*. In other words, in 360-degrees.

To make most out of 360-degree content you need proper tools. Not only cameras and software for *creating* content. It is even more important what comes after that: how you will *utilize* your content. This means carrying out tasks such as annotating, presenting and sharing. This is also where LiveSYNC as a product comes to picture. You will be surprised how much you can achieve with it.

We will start from the big picture to learn why this is the right time to start using  360-degree content. Then, we will discuss how LiveSYNC was born and what needs it was built to satisfy. We will conclude this chapter by going through some key concepts.

!!! tip
    If you can't wait to start using the app, begin reading from [*Chapter 2. Installing*](installing.md). Continuing from there provides answers to *how*-questions. Return here when you are ready to find answers to *why*-questions.

## The Big Picture

Let's begin by taking a few steps back to see the big picture. We will discuss trends in human communication and photography. Next, we go through recent advancements in network and camera technologies. Then, we will paint a picture of where we seem to be going.

### Communication Becomes Visual

During the past few decades the way humans communicate with each other has changed. First, we learned to connect computers together locally, then globally. We made a giant web of wires where messages can travel from one corner of the planet to another in a fraction of a second. Building *Internet* was a major breakthrough in communication. In its early days, bandwidth was low and it was practical only for transmitting *text*. You had to pay for usage by the minute, so it was wise to write an email first and go online only to send it. How odd that must sound to the children of the current age!

Along the years many technological advancements made the web faster and faster. We started to use it for other forms of communication that need higher bandwidth. Images and GIF animations started to appear on web pages. Kids learned to share music as downloadable MP3s, then full-length movies. Low-latency streaming changed the landscape even more. Voice calls via Skype started to become popular. On-demand video rentals began, and Youtube showed up.  Eventually, even traditional media gave up. They brought newspapers, radio shows, and live TV online. All forms of digital communication became available for a single low-cost data plan. Fixed monthly price made it possible to stay online all day long.

As a consequence, we saw a paradigm shift in human communication. Our messaging began to change *from textual to visual representation*. Young people didn't bother to write long, well thought out emails. They chose to send lots of short messages that were read and replied to almost instantly. Tiny images called smileys were added by combining special characters. They were used to communicate common *feelings* by mimicking facial expressions. Then more complicated, graphical emojis arrived and teens began to skip words. They communicated with cryptic abbreviations and sequences of emojis. Kind of a modern version of the Egyptian hieroglyphs! Later, textual part in many messages had reduced to a caption for a photograph. Now the image had become the message itself. Then we moved on to video. The age of Youtube stars had begun: anyone could start their own TV show with a press of a button.

> The world now is more visual that it has ever been in human history. 93% of all human communication is visual. ([Ref 1])

Why are we so fond of photos and videos? What is wrong with plain old books and letters, you may ask. There is clearly room for all kinds of communication methods. The new ones do not *replace* the old ones; they *add* to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of visual kind. We can even argue that humans *are* visual by nature:

> Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. ([Ref 1])

To simplify it, **the more visual a communication method is the faster we understand the message it carries**. Consider an ordinary video. If that small window is so effective, imagine the impact of widening the view to 360 degrees.

[Ref 1]: https://tech.co/news/world-now-visual-ever-2017-01

### Interactivity Arouses Interest

The advances in imaging technology are pushing another paradigm shift. Traditionally, many features of a photograph had to be decided in advance ie. before the camera shutter closed. More and more of these can be now changed after taking the shot. Up to the point where the person *viewing* the content becomes the decision maker.

Many new capabilities are empowering the photographers. They allow postponing artistic decisions after the shot. This has huge value in content production. As a consequence, retouching images has become so commonplace that *photoshopping* is now a household word. Many use software filters for spicing up their images. First take a picture and *then* select the result you like the most. Light-field cameras allow focusing the image *after* it has been taken. Photography today isn't what it used to be.

Also cameras are evolving, to the point where we have to ask: what exactly is a camera? Or, who exactly is the photographer? Consider new phones that contain many physical camera modules. When you press the trigger, a set of images is captured. The camera modules trigger simultaneously and take many shots in sequence. Then, artificial intelligence is used for composing one perfect shot. Some of the frames are retrieved from a memory buffer and were taken *before* you pressed the trigger. Such a camera is a complex combination of hardware, software, and mathematical algorithms. Software is adopting the role of the photographer. It has become a decision maker in the creation process. So, can we still justify calling the person that pressed the trigger a photographer? All he did was point the camera to a certain direction!

It is not surprising that also framing the shot is becoming history. Or, more specifically, we can leave that decision to the viewer of the image. In 360-degree video, every direction is captured simultaneously. There are no traces of a camera, a cameraman, or even a tripod. It is a kind of miracle. The point of view seems to float in the air. What kind of sorcery is that, you may ask. Only technology! Multiple camera modules and software that work together in a clever way. Now there is no need to decide where to point the camera as everything around the camera is captured. This has some profound effects, as we will soon see.

Let's take a closer look to this technology. Traditionally, a photographer or a videographer has decided how to frame the shot. What is necessary, what can be left out. This decision is locked at the time of capture. It becomes a permanent limitation. At viewing time, it is not possible to see beyond the frame. In fact, the only possibility to re-frame the shot is to limit the viewing angle *even more* by cropping the image (ie. digitally zoom in). The benefit is that the photographer has power to make his audience focus on exactly what he wants them to see. But that can also become a lie. Beautiful places you see in photos and movies often look very different when you visit them in real life. For example, the pyramids of Giza are always photographed so that they appear to stand in a middle of a desert. If you turn around, there is Cairo with its 20 million inhabitants!

Capturing in 360-degrees tends to show a much more realistic view to the World. But how exactly are those images made? Imagine a perfect zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what will happen if you keep zooming out? You will see more and more of the surrounding area become visible in the image. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. A 360-degree camera is essentially a camera with a *very* wide angle lens. When you are zooming in, there is no natural limit where to stop. But when you are zooming out, such a a limit does exist: it is where the opposing sides of the image meet. In that sense, a 360-degree camera has a perfect wide-angle lens: we cannot expand the view any further.

We may never be able to physically manufacture a lens that could capture everything at once. Yet, 360-degree cameras *do* exist. How is it possible, then? Again, digital image processing comes to the rescue! Almost seamless representation can be achieved by stitching together many images. These are captured with two or more lenses. Current 360-degree photo and video panoramas capture the world in every direction. In that sense, they are perfect: everything around the camera is in the image. In reality, there is a small distance around the camera where an object can be hidden between the lenses. This has one useful side effect: it allows hiding a small tripod or a selfie stick that is holding the camera.

What is the main difference of 360-degree imaging compared to old school photos? During playback, *the viewer is in control of the viewing direction*. He can turn around and even look up to the sky or down to his feet. It feels like magic when you experience this freedom the very first time. But soon you will get used to it and begin to *expect* that all images work this way. So much, that you feel annoyed when you attempt to pan an image and it won't budge; it is a plain old 2D image. You've probably seen how today's toddlers expect that *every* screen is a touch screen. Likewise, you begin to assume that *all* images can be panned. This **interactivity is what makes 360-degree media so interesting to us**. The ability to explore.

### Bandwidth Brings Presence

Being able to look around is not all that it takes to make us feel *being there*. Another key to immersion is *the amount of detail*. It is another aspect of the amount of information an image represents. A 360-degree image covers the whole field-of-view. But we must also preserve as much *detail per degree* as possible to make the image look realistic. Or, at least as much a human eye can distinguish! This is more easily said than done, since a human eye is an amazing camera and a lens in a very compact form. It is able to capture enormous amount of information in a fraction of a second.

This brings us back to bandwidth and Internet. Whenever transfer rates take a step forward, we find a richer communication method. Something that provides a better way to capture and share what is happening around us. Often it is a way that communicates *presence* better.

Can you imagine what will be the next step on this path? Well, one way to understand it all is to see the changes as stepping stones towards *telepresence*. It is a combination of technologies that allow a person to feel as if they were present in another place. And maybe also in another time. That other place can be real or imaginary. This is where we are heading to. And step by step, that feeling of *being there* is becoming stronger and stronger. We are not talking about a solution where a human would not be able to *recognize* if the world is real or virtual. Telepresence is good enough already when we allow ourselves to *forget* the fact that it isn't real.

Tracing the path backwards from the goal to the present, we are not there yet but not that far off either. During the past few years, technology has matured enough to solve one profound road block. This is the image frame that limits our field of view. Think about it: how could you ever feel *being there* if you weren't even able to turn your head to look around? Least, you should be able to observe the surroundings by looking at any direction you want. And experience the view and soundscape change corresponding to your head movements.

Virtual reality headsets are one way to make this workable. Computer games and movies are an endless source for imaginary worlds. They are the driving force for virtual reality. However, the real world surrounding us must not be forgotten: it is, after all, where we live in! It can be captured with 360-degree photography and 360-degree videography. Accompanied with 360-degree audio, perhaps.

Of course, 360-degree cameras and VR headsets are not enough. They are solutions for content creation and consuming. We must also be able to *transmit* the content from the camera to the headsets. But, moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate. It is not a coincident that 360-degree content is becoming popular *now*. Internet is just becoming fast enough to carry the amount of data it requires.

Traditional photos are great for saving memories. Videos are perfect for telling stories and documenting events. What is the main reason for shooting something in 360-degrees? It is the feeling of *presence*. And it is much stronger when video is combined with 360-degree audio and viewed through a VR headset. The memories are brought alive as a space and time where you can step inside. You'll feel like *being there*. But only if the image is detailed enough! This feeling of (tele)presence is what we want to achieve. We need enough bandwidth to make it look so good that our brains allow us to forget it isn't real. And we are gaining this capability very soon. Fiber and 5G networks are exactly what 360-degree content needs to thrive.

!!! example
    Consider a single frame of 4K video that contains 3840 x 2160 = 8.3M pixels. A wide-angle shot could contain 90-degree field-of-view horizontally. This yields 3840 / 90 = ~43 pixels per degree. To produce a 360-degree view horizontally, we need four similar frames side-by-side. This yields 4 x 3840 = 15360 pixels. A full spherical image is 360 degrees horizontally but only 180 degrees vertically. Thus, we need only half of that number vertically. Thus, to experience "4K quality in 360-degrees", our image frame would have to be 15360 x 7680 = 118M pixels. This is over 14 times the amount of pixels in a 4K video frame!

    What if we settle with "FullHD quality in 360-degrees" or 1920 / 90 = 21.3 pixels per degree? Then our image frame size becomes 7680 x 3840 = 29.5M pixels, which is "only" 3.5 times the amount of  pixels in a 4K video frame. 360-degree cameras that capture video in 7680x3840 resolution are becoming commonplace. This means that we are now entering the "FullHD" time of 360-degree video. "4K" time of 360-degree video is still a few years away.

    We can also calculate it in another way. A high quality compressed 4K video requires roughly 40 Mbit/s bitrate. Scaling up with factor 3.5 yields 140 Mbit/s data rate for "FullHD quality in 360-degrees". This could pass through best current mobile LTE networks that offer 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for "4K quality in 360-degrees". We will need one gigabit Internet connection to view 360-degree video in quality that we know as "4K". Upcoming mobile 5G networks promise such speeds.

    In reality, many clever solutions has and will be developed to reduce the need for bandwidth. The capability to transmit high quality 360-degree video is coming. And probably sooner than it appears when only bandwidth is considered.

### 360 Cameras for All

360-degree photography has come a long way in a short time. Only a few years ago it was unheard of to the general audience. Yet, we can hardly say that 360-degree photographs are anything *new*. Devoted photographers have been shooting in 360 degrees for quite some time.

For a long time you had to really love this form of art to go through all the trouble. Capturing 360-degree photographs required purchasing a lot of camera gear. A DSLR camera, an expensive fisheye lens, and a tripod with a special panoramic head. To create a single image, you had to take multiple shots. Then move the files from your camera to a PC and run them through a special stitching software. You could spend half an hour for adding control points, masking the tripod, and many other steps. It took several minutes from a powerful PC to render the image into a file. After a few years of trial-and-error you learned to tune your camera gear and shooting process. Quite a learning curve!

This is how professionals still work. Of course, everything has become faster, easier, and better looking. Yet, the real revolution was the arrival of integrated consumer and prosumer level 360-degree cameras. With a few hundred dollars you can now buy a point-and-shoot camera and control it with your phone. The phone also stitches the image and does a pretty good job. The result is not comparable to a professionally made panorama. But, it is quick, cheap, and easy enough for anyone to make. Moreover, it is *good enough to be useful*. That is the recipe for general adoption.

We cannot talk about 360-degree photography without also discussing 360-degree videos. It is, after all, a natural step forward. If you can create a single 360-degree photograph, you can also create a video. Just capture lots of photos in sequence, right? It is not that easy in practice. For video, you need many cameras which need to be synchronized. Because of their physical size, the cameras cannot be in the optimal pivoting point. You must deal with parallax and hence more difficult stitching issues. GoPro action cameras used to be very popular for 360-degree video. They were small, affordable, reliable, and produced good image quality. Popular layouts required using six cameras. Not one of them could fail without ruining the whole shot.

Today, integrated 360-degree cameras also capture video. All you need to do is press the REC button. After transferring the file to your phone, let it stitch the video together. You can even do live streams, straight from your phone to the tubes. The 360-degree camera technology definitely has matured. Monoscopic 360-degree video can be considered a solved problem, and it is ready to be used by anyone. Stereoscopic (3D) 360-degree video is only a little behind.

### What's Next?

This is the big picture: 360-degree imaging has already matured. It is easy and cost-effective enough to be used in daily activities. We can also see the fog disappearing and revealing the next stepping stone. 360-degree video allows the viewer to turn around, but not move. *Volumetric video* changes this. It will allow moving your head and even *taking a step* inside the world. This will make us feel being present even more. That technology is still in its infancy and needs a few more years to begin to work. And many more before the cameras and networks catch up!

There is no reason to wait, though. The future will always bring better, more advanced technology. So, let us consider what it means that we can now let go of the limitations of image frame. After 200 years of photography, that is already a miracle of its own!

## Making LiveSYNC

Now that we have covered the big picture, it is time to discuss the LiveSYNC application. First, we will take a look at the market situation of 360-degree software. Next, brief summary of the history of the LiveSYNC product follows. Then, use cases that had an impact in designing its user interface and original features.

### Market Situation

The rise of 360-degree content production did not went unnoticed. In the early days, special software was needed for stitching 360-degree content. And then again for embedding such content on your website. Now, traditional video editing tools have been extended to support also 360-degree video. You can even stitch a 360-degree photo with the actual Photoshop application. Previously, you needed a custom app if you wanted to play 360-degree video in a phone or a tablet. Today, most social media services support at least basic monoscopic 360-degree content. Some also support viewing in a VR headset. This is great if you shoot and share content to your friends or potential customers. In other words, if it is for fun or part of your mass-marketing strategy.

But what about content that is not intended to be publicly available? Or, not allowed to be uploaded to 3rd party services, to be shown next to your competitor's ad? How about controlled situations, such as private product presentations or employee training? What about using 360-degree media as part of speeches and lectures? Or, using 360-degree content as a *canvas* for annotating points-of-interest? And then sharing this information as visual memos or work instructions? Images and videos are *very* versatile, and 360-degree variants perhaps even more so. For industrial users, it is essential that tools can be customized and integrated. Clearly there are many, many needs to satisfy! This means that various kinds of professional tools and services are needed. Customizable solutions that go beyond what the freemium mass-market services can offer.

Moreover, the mass-market has started to grow slower than expected. Consumers have plenty of other options for entertainment. It is not easy to convince them to purchase yet another media device. Especially at times when technology develops fast many consumers tend to postpone purchasing. The next year's model is expected to be much better. Also the language of story telling must change to accommodate to this new medium. Consumers expect high quality experiences. But content producers need to experiment, to figure out what works and what doesn't. Telling stories in 360 degrees is different. Hence, the user experience has been lacking from what consumers are expecting.

The professional market has no reason to wait for the consumer market to mature. The benefits of 360-degree media are available for them *now*. From consumer point-of-view, it is a question of spending money to yet another entertainment option. From professional point-of-view, *it is an investment that can create great cost savings* when done correctly. Postponing *spending* money can be well justified, but postponing *saving* money cannot.

### LiveSYNC 1.0

In 2012 a Finnish software company Finwe Ltd. was looking for a new direction. They had years of experience in motion sensors, 3D graphics, and user interfaces. First as an experiment, they created a 360-degree photo player for demonstrating the state-of-art of sensor fusion on mobile devices. The player was soon extended to support 360-degree video playback, and presented in Mobile World Congress in 2013. The reception was good and started to turn Finwe's focus on 360-degree media. It also kick started the development of Finwe's own 360-degree rendering engine, *Orion360*.

A few years later, Finwe had developed over 100 custom apps for 360-degree video playback on iOS and Android platforms, and licensed their Orion360 engine to a few hundred more. However, the market was in change: the giants of Internet announced support for 360-degree media one after the other, meaning that the need for custom apps was in decline. These apps were also very similar to each other. Most of the market could be handled with simple template-based apps. There was also a possibility that recent developments in HTML5 could make these custom apps obsolete.

360-degree video was becoming commonplace. It was transforming from a novelty to a utility. Finwe wanted to create a product that would combine their best technologies into a generic player app with an easy-to-use remote control feature. The goal was to create a tool for presenting content live to different sizes of audiences. In the heart of the app was a technology that allowed mirroring the screen of one device to another in a way that *very* little data needed to be transmitted. This allowed controlling and observing multiple clients simultaneously, and the solution worked also in crowded places where Wifi was congested. Finwe demoed the concept at Mobile World Congress in February 2017. The feedback was very encouraging. After a few months of furious development, Finwe released LiveSYNC 1.0 for iOS and Android.

### Use Cases

360-degree content is perhaps best experienced using a virtual reality headset. The experience is so immersive that it also isolates the viewer from the rest of the world. It becomes a problem when you are presenting content to another person and need to communicate with him during playback. For example, to give instructions, ask for an opinion, or simply to share the experience by chatting about it as the story unfolds. Yet, not being able to see what he sees forces you to continuously ask what's on the screen. How awkward!

LiveSYNC removes this barrier by creating a screen mirroring style copy of the view from the VR headset to a controlling device, typically a tablet computer. This view is updated in near real-time and can be further shared to a big screen, for example a TV or a projector, so that even a large audience - perhaps waiting for their own turn - can share the experience. Nobody likes waiting, though. Fortunately, with LiveSYNC you can connect multiple headsets to a single control device, play content on all devices in sync, and observe the views of all devices simultaneously in a gorgeous video mosaic. You can also mix in ordinary 2D photos and videos, for example your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool.

During a presentation, another communication problem easily arises: by observing the other guy's view you know what he *is looking at*, but now you have to find the words to explain where he *should be looking at* to see something important. That becomes equally awkward.

With LiveSYNC, you can drag an arrow over the 360-degree view. This icon will appear on all connected devices. You can even move it in real-time by dragging it to another place. This feature makes communication much easier: look at this direction, see now, there it is under the arrow. Simple and elegant.

Imagine that you are selling your summer cottage and made a set of 360-degree photos and videos so that potential buyers can virtually visit the place without driving hours to the destination. Everybody wins when uninterested buyers can be filtered out early on. But wouldn't it be convenient if you could also add a few notes to your 360-degree images, to highlight things you want them to notice and remember? Or automatically stop your 360-degree video to a certain frame, where the notes would appear? Saving your annotations would also help a lot because you probably need to show the presentation multiple times.

All of these are built-in features in LiveSYNC. When presenting content, simply drag & drop icons from included clipart sets over the photo or video content. Add a caption for each icon, or create your own signs and use them instead. Save the configuration and they will be automatically loaded the next time you play this content. Powerful annotation features make LiveSYNC stand out.

The enterprise version of the LiveSYNC app goes much further. It has been developed in close co-operation with industrial clients. You can use a built-in editor to create a project, import media content, add interactive hotspots that can even fetch data from the network in real-time, import 2D and 3D maps with camera paths, etc. A really useful feature is to be able to export all the notes you've made as screenshots into a visual PDF report, and send it to your colleagues right away. Take a quick tour on your construction site with a 360-degree camera, add new work instructions, export to PDF and mail away - all in 15 minutes. That's what we call *instant digitalization*.

Since its initial release, LiveSYNC has come a long way. We are very excited about the features that are being developed right now, and feel that LiveSYNC is just spreading its wings. After years of working with 360-degree media, we are still excited about it and frequently get amazed what is possible when an image or a video covers the full 360-degree view.

## Key Concepts

As every software product, also LiveSYNC is based on a few key concepts. For the user of the product it is very useful to be aware of these: everything feels more logical. Also reading the documentation becomes easier, as some of the terms we use have a very specific meaning. Hence, it is highly recommended that the reader gets familiar with the terms and concepts presented next.

### Presenting

When we are presenting content to other people with LiveSYNC, we assume that one person is in control of the situation. Let us call this person **the director**. The people who are following the presentation are called **the audience**.

The director acts in the role of the presenter and uses a computing device for sharing content to the eyes of the audience. Let us call this device **the control device**. The control device is usually a tablet. Sometimes we may refer this devices as the director's device.

In a typical setting for giving a presentation, the room contains a large TV or a projector. For example, most meetings rooms have one. We will call this device **the big screen**. The connection between the control device and the big screen can be wired or wireless. Using the big screen is optional but often useful: everyone can see the same content.

However, with 360-degree media, is is essential that every member of the audience can explore the view *personally* to look at things they find interesting and at their own pace. Thus, every person in the audience should have a separate **viewing device** that allows exploring the content individually. These devices can be phones, tablets, or VR headsets. They can be personal devices owned by the audience members, or devices that are shared and managed by the presenter. The latter is often more practical.

In order to stay in control, the director's device must communicate with the audience's viewing devices, for example to send a command to switch to next slide. (This is the reason why it is called a control device). Before communication can begin, a **connection** must be established between the devices. It is always a two-way connection: the control device sends commands to the viewing devices, and the viewing devices send status messages back to the control device. Thus, the director is able to see that everything is OK on each viewing device and assist if necessary.

The communication between the control device and the viewing devices is wireless and based on common radio technologies. For example, **Bluetooth** allows controlling a small group of devices without data plan or any kind of network infrastructure. It works well also in crowded places. A large number of viewing devices, potentially geographically distributed to multiple sites, can be controlled via a cloud service called **GlobalSYNC**. This service is run by Finwe, the developer of LiveSYNC, and requires either Wifi access or mobile data access.

Multiple presentations can take place simultaneously, even in the same room. This is frequently needed for example in trade shows when multiple presentations are run at the same or adjacent booths. To make this feasible, the communication between device groups take place on different **LiveSYNC channels**. The messages sent on one channel are received only by devices that have connected to the same channel.

In order to assist a particular audience member who needs help, each device is given an ID. We call it the device's **LiveSYNC name**. Once connected, this name appears in the control device together with other status data from that particular device.

Thus, on each device one must select whether it is the director's device or an audience device, give a name for identification, select which connection method to use (e.g. Bluetooth or GlobalSYNC), and which channel number to join. This is typically pre-configured before a presentation, but a new channel can be reserved and taken in use also spontaneously. Created configuration is called **channel configuration**. A device can have multiple channel configurations but it can join only one channel at a time.

When a channel configuration is selected from the app's home screen, viewing devices will move to a **lobby** screen where they wait until a connection to the control device is established. The control device will move to the **director's workspace** that consists of multiple tabs. For example, the **mosaic** tab allows observing mirrored views from all connected devices. The **player** tab is used for controlling the presentation: changing content, controlling video playback, adding points-of-interest, etc.

These concepts will become more familiar in the next chapters. You don't need to memorize the terms or their meaning to be able to use the app; they are presented only to help you understand how the software works, to make it clear what we mean by these terms as their appear frequently in the documentation, and to make it easier to understand why certain things are asked during the setup phase.

### Understanding view mirroring

The ability to mirror a particular viewing device's view to the control device is very central in LiveSYNC. You might be wondering how does it actually work and what is possible with LiveSYNC and what is not. Next, we will discuss this further.

In general, there are two ways to mirror a live view of one device's screen to another:

1. **Streaming video (screencasting) **

    *Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection.*

    One benefit of this approach is that the mirrored view is an *exact* copy of what the other device is drawing on its screen. Another is that the mirroring feature can be implemented on the platform level and is thus available for all apps or the whole desktop environment.

    There are many downsides, too. Because it is essentially streaming high resolution video from one device to another, the method consumes a fair amount of CPU/GPU resources and power in *both* devices. It also requires a lot of network bandwidth when there is movement on screen, and it does not scale well: the network and the video decoder in the receiving end quickly become a bottleneck when more devices connect.

    Furthermore, when using a VR headset, we do not actually *want* to stream an exact copy of what is on screen. Instead of the distorted double barrel view we want to mirror a normal view.

2. **Streaming commands**

    *Principle: Integrate deep into the app and transmit only commands that allow the receiving end to reconstruct the view from the same assets.*

    The benefits of this method include low CPU/GPU and power consumption, trivial use of network bandwidth, and scalability. It is also possible to render the mirrored view a bit differently compared to the source device.

    The downside is that all devices must have local copies of the assets required for reconstructing the view. Also, with video content the resources of the control device can become a limitation in certain use cases. A control device may not have enough hardware resources to decode and play multiple videos simultaneously. Or, the same video simultaneously from different stream positions. This is not a problem, though, when all devices watch the same content in sync.

Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. Wifi does not work well enough and/or the organizers do not even allow using own Wifi hotspots.

**LiveSYNC uses type 1 method for mirroring the view of the control device to the big screen.** There is only one big screen to connect to, and we want to show an exact copy of the screen (or part of the screen) of the control device, hence type 1 method can be used.

**For playing content on the viewing devices and mirroring their view back to the control device, LiveSYNC uses type 2 method**. This approach allows operation in crowded trade shows, especially when Bluetooth technology is used for communication. Also, observing a large number of viewer devices is possible, among other benefits.

!!! note
    A common question is whether LiveSYNC can be used for mirroring the view of *other* applications such as games. It becomes clear from the explanation above that this is not feasible. Integrating to 3rd party apps is not possible, therefore LiveSYNC cannot offer more than platform's own type 1 screen mirroring already does.

    To recap, presenting with LiveSYNC requires using LiveSYNC app on each participating device and mirroring works only between the LiveSYNC app instances. The only exception is the big screen device, which does not need to run the LiveSYNC app. The image can be transferred from the control device to the big screen e.g. via an HDMI cable.
