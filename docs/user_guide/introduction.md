# 1. Introduction

## Welcome to LiveSYNC

Since you are reading this user guide, you must have decided to learn more about 360-degree photos, videos, and one very special tool called LiveSYNC. That is awesome! We, too, believe it is time to finally get rid of the frame and start capturing the World as we humans experience it: in a way that a viewer can look around and feel like *being there*. In other words, in 360-degrees.

To make most out of your 360-degree content you need proper tools. But not only cameras and software for *creating* content. It is perhaps even more important what comes after that: how you will *utilize* content. We are talking about annotating, presenting, sharing, etc. This is where LiveSYNC comes to picture. You will be surprised how much you can achieve with it.

In the introduction, we will start from the big picture to understand why 360-degree media is now ready for mass adoption, and what needs the LiveSYNC app was built to satisfy. This will give the reader basic understanding of the history and design decisions behind the product.

However, if you can't wait to start using the app, we suggest you start reading from [*Chapter 2. Installing*](installing.md). Continuing from there provides answers to *how*-questions. Return here when you are ready to find answers to *why*-questions, or if you are looking for motivation to invest your time in 360-degree media.

## The Big Picture

Let's begin by taking a few steps back to see the big picture. We will discuss trends in human communication and recent advancements especially in camera and network technologies. We will look back to see how we got here, paint a picture of where we seem to be going, and begin to understand why *now* is the perfect time to take 360-degree imaging seriously.

### Communication Becomes Visual

During the past few decades the way humans communicate with each other has permanently changed. First we learned to connect computers together locally, then globally. We made a giant web of wires where messages can travel from one corner of the planet to another in a fraction of a second. Building *Internet* was a major breakthrough in communication. Although, in its early days bandwidth was so low that the network was practical mainly for transmitting *text*. You had to pay for usage by the minute, so it was wise to write an email first and go online only to send it. How odd that must sound to the children of the current age!

Along the years many technological advancements made the web faster and faster. Step-by-step we started to use it for other forms of communication that require higher bandwidth. First images and GIF animations started to appear on web pages, then kids learned to share music as downloadable MP3s. Voice calls via Skype started to become popular, on-demand video clips appeared and Youtube showed up. Eventually even traditional media gave up and took their newspapers, radio shows, and live TV broadcasts online. All forms of human communication became available for a single low-cost data plan - although content creation was and still is heavily subsidized by advertisers. Nevertheless, this new pricing model and personal smartphones made it possible to stay online all the time.

As a consequence, we began to see a paradigm shift in human communication *from textual to visual representation*: Instead of writing long, carefully thought out emails we started to send lots of short messages that were delivered, read and replied to almost instantly. Tiny images called smileys were added by combining special characters. They were used to communicate common feelings by mimicking facial expressions. Then more complicated, graphical emojis arrived. Teens began to skip full sentences or words altogether by communicating with cryptic abbreviations and sequences of emojis. Soon the textual part of a message was reduced to a mere caption or a foreword to a photograph we had just taken. The image had become the message itself. Then we moved on to video and now live in the age of Youtube stars: basically anyone can start their own TV show with a press of a button.

> The world now is more visual that it has ever been in human history. 93% of all human communication is visual. ([Ref 1])

Why are we so fond of photos and videos? What is wrong with plain old books and letters? There is clearly room for all kinds of communication methods, as the new ones do not *replace* the old ones; they add to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of visual kind. We can even argue that humans *are* visual by nature:

> Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. ([Ref 1])

To simplify it, **the more visual a communication method is the faster we get the message**. We can only imagine the impact of widening the view from a small square to a full 360-degree experience. Or, we can start using it, and see the results.

[Ref 1]: https://tech.co/news/world-now-visual-ever-2017-01

### Interactivity Arouses Interest

The advances in imaging technology are pushing another paradigm shift: many features of photographs that have been traditionally decided *in advance* - or when the camera shutter closes by latest - can now be changed *afterwards*. Up to the point where the person *viewing* the content becomes the decision maker.

For example, retouching images is so easy and commonplace that *photoshopping* has become a household word and a basic skill. Social media services provide vast amount of filters for spicing up captured photos; you simply select which result you like the most. Light-field cameras allow focusing the image *after* it has been taken (a stunt that seems almost impossible even for seasoned photographers!) These and many other new capabilities are all empowering the photographers. They allow leaving many artistic decisions to be made after the shot.

Also the cameras are evolving, to the point where we have to ask: What exactly is a camera? Or, who exactly was the photographer? Consider new phones that contain multiple physical camera modules. When you press the trigger, a set of images is captured: the camera modules trigger simultaneously and take many shots in sequence. Then, artificial intelligence is used for composing one perfect shot. Some of the frames are actually coming from a memory buffer and where taken *before* you pressed the trigger. Here the camera is a complex combination of hardware, software, and mathematical algorithms. The software is adopting the role of the photographer and becomes the key decision maker in creating the image. So, can we still justify calling the person that pressed the trigger the photographer? All he did was point the camera to a certain direction!

After all this, it is not that surprising that also framing the shot is becoming history. Or, more specifically, we can leave that decision to the viewer of the image. 360-degree video, where every direction is captured simultaneously and there are no traces of a camera, a cameraman, or even a tripod, is a kind of miracle. The point of view simply seems to float in the air. What kind of sorcery is that? Well, it is technology: multiple camera modules and software that work together in a clever way. Now there is no need to decide where to point the camera, as everything around the camera can be captured. This has some profound effects, as we will soon see.

Let's take a closer to look to this technology. Traditionally, a photographer or a videographer has decided where to point his camera and how to frame the shot: what is necessary and what can be left out. This decision is locked at the time of capture, and becomes a permanent limitation: at viewing time it is not possible to see beyond the frame, in fact the only possibility to re-frame the shot is to limit the viewing angle *even more* by cropping the image (ie. digitally zoom in).

Now, imagine a perfect zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what will happen if you keep zooming out? You will see more and more of the surrounding area become visible in the image, although heavily distorted. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. A 360-degree camera is essentially a camera with a *very* wide angle lens. When you are zooming in, there is no natural limit where to stop. But when you are zooming out, such a a limit does exist: it is where the opposing sides of the image meet. In that sense, a 360-degree camera has a perfect wide-angle lens: we cannot expand the view any further.

While we may never be able to physically manufacture a lens that could capture everything at once, 360-degree cameras *do* exist. How is it possible, then? Again, digital image processing comes to the rescue! By combining the images captured through two or more lenses with a stitching algorithm, a more-or-less seamless representation can be achieved. These 360-degree photo and video panoramas (also known as *immersive videos*, *spherical videos*, or *VR videos*) capture the world in every direction. In that sense, they are perfect: everything around the camera is in the image. In reality, though, there is a small distance around the camera where an object can be hidden between the lenses. It is mostly irrelevant, but has one useful consequence: it allows hiding a small tripod or a selfie stick that is holding the camera.

One of main advantages of 360-degree imaging is that during playback *it is the viewer who is in control of the viewing direction*. He can freely turn around, and even look up to the sky or down to his feet. In fact, we are so used to the frame that limits the view that it feels like magic when you experience this freedom the very first time. But soon you will get used to it and begin to *expect* that all images work this way. So much, that you feel annoyed when you attempt to pan an image and it won't budge; it is a plain old 2D image. Just like today's toddlers expect that *every* screen is a touch screen. **Natural interactivity is what makes 360-degree media so interesting to us**. Scrolling a page on a touch screen feels very natural after you have been relieved from believing that it is not possible. Panning an image is just the same.

### Bandwidth Brings Presence

When we are viewing a 360-degree photo or video, being able to look around is not all that it takes to make us feel being there. Another key to immersion is *the amount of detail*. It is another aspect of the amount of information an image represents. While a 360-degree image covers the whole field-of-view there is to explore, we must also try to preserve as much *detail per degree* as possible. Or, at least as much a human eye can distinguish! This is more easily said than done, since a human eye is an amazing camera and a lens in a very compact form. It is able to capture enormous amount of information in a fraction of a second.

This brings us back to bandwidth and Internet. Looking back, it seems that whenever the transfer rates of Internet connections have taken a step forward, we've immediately found a new, richer communication method that provides a better way to capture and share what is happening around us: a way that communicates *presence* better by adding some essential parameter to the equation that was missing before. It seems that reinventing human communication is how killer apps and unicorn companies are made again and again.

Can you imagine what will be the next step on this path? One way to understand it all is to see the changes as stepping stones towards true *telepresence*; a combination of technologies that allow a person to feel as if they were present in another place, maybe also in another time. That other place can be real or imaginary. This is clearly where we are heading to. And step by step, that feeling of *being there* is becoming stronger and stronger.

Tracing the path backwards from the goal to the present, we are not there yet but not that far off either. During the past few years technology has matured enough to solve one profounding road block by removing the image frame that limits our field of view. Just think about it: how could you ever feel *being there* if you weren't even able to turn your head to look around? By minimum, you should be able to freely observe the surroundings by looking at any direction you want, at any moment of time - and experience the view and soundscape change corresponding to your head movements.

Virtual reality headsets are one way to make this feasible. Computer games and movies are an endless source for imaginary worlds and thus a perfect match with virtual reality. However, the real world surrounding us must not be forgotten: it is, after all, where we live in! It can be captured with 360-degree photography and 360-degree videography, accompanied with spatial audio. This content is typically experienced through ordinary computer screens, smartphones, tablets, or virtual reality headsets. New kind of image viewers and video players have been developed to allow interacting with the content at presentation time.

Of course, 360-degree cameras and VR headsets are not enough: we must be able to transmit the video stream from the camera to the headsets. It is interesting to notice that **moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate**. We have been able to watch movies in 4K quality over Internet quite some time now, so it is not a coincident that 360-degree content is becoming popular just now when Internet is becoming fast enough to carry the amount of data required for it.

While traditional photos are great for saving memories and videos are perfect for telling stories, the main driving force for shooting something in 360-degrees is the feeling of *presence* that you get when viewing the content. Even more immersive experiences can be achieved when 360-degree video is combined with 360-degree audio (also known as *positional audio* or *spatial audio*), and viewed through a virtual reality (VR) headset. The memories are brought alive as a space and time where you can step in. You'll feel like *being there*, but only if the image is detailed enough. Thus, the feeling of (tele)presence is what we want to achieve, we need enough bandwidth to make it feasible, and **we are gaining this capability just now**. Fiber and 5G networks are exactly what 360-degree content needs to thrive.

!!! example
    Consider a single frame of 4K video that contains 3840 x 2160 = 8.3M pixels. If the camera that captured the frame was zoomed out to produce a wide-angle shot the frame could contain ~90-degree field-of-view horizontally, resulting to 3840 / 90 = ~43 pixels per degree. In order to produce full 360-degree view at the same resolution (amount of pixels per degree), we'd need roughly four similar frames side-by-side, resulting to 4 x 3840 = 15360 pixels horizontally. Since full spherical image is 360 degrees horizontally but only 180 degrees vertically, we need only half of that number vertically. Hence, to experience "4K quality in 360-degrees", our image frame would have to be 15360 x 7680 = 118M pixels. This is over 14 times the amount of pixels in a 4K video frame!

    However, if we settle with "FullHD quality in 360-degrees" or 1920 / 90 = 21.3 pixels per degree, then our image frame size becomes 7680 x 3840 = 29.5M pixels, which is "only" 3.5 times the amount of  pixels in a 4K video frame. In early 2019 360-degree cameras that capture video in 7680x3840 resolution are becoming commonplace, which means that we are now entering the "FullHD" time of 360-degree video, and "4K" time of 360-degree video is still a few years away.

    We can also calculate it like this: a high quality compressed 4K video requires roughly 40 Mbit/s bitrate, hence scaling up with factor 3.5 yields 140 Mbit/s data rate for "FullHD quality in 360-degrees", which could pass through best current mobile LTE networks that offer 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for "4K quality in 360-degrees". We will need one gigabit Internet connection to view 360-degree video in perceived quality that we know as "4K". Upcoming mobile 5G networks promise such speeds.

### 360 Cameras for All

360-degree photography has come a long way in a short time. Only a few years ago it was mostly unheard of to the general audience. Yet, we can hardly say that 360-degree photographs are anything *new*. Devoted photographers have been shooting in 360 degrees for quite some time.

For a long time you had to really love this form of art to go through all the trouble. Capturing 360-degree photographs typically required purchasing a DSLR camera, an expensive fisheye lens, and a tripod with a special panoramic head. To create a single image, you had to take multiple shots, move the files from your camera to a PC, and run them through a special stitching software. You could easily spend half an hour for manually adding control points, masking the tripod, and for many other steps. Finally, it took several minutes from a powerful PC to render the image into a file. After a few years of trial-and-error, you learned to tune your gear and shooting process properly and started to make perfectly stitched spherical panoramas.

Essentially, this is how professionals still work. Of course, everything has become faster, easier, and better looking. However, the real revolution in 360-degree photography was the arrival of integrated consumer and prosumer level 360-degree cameras. With a few hundred dollars you can now buy a point-and-shoot type camera and control it with your phone. The phone also stitches the image automatically and instantly, and does a pretty good job. The result is not comparable to a professionally made panorama, but it is quick, cheap, and easy enough for anyone to make. Moreover, it is *good enough to be useful*. That is a recipe for general adoption.

We cannot talk about 360-degree photography without also discussing 360-degree videos. It is, after all, a natural step forward: if you can create a single 360-degree photograph, you can also create a video by capturing lots of photos in sequence, right? It is not that easy in practice, though. For video, you need multiple cameras which need to be synchronized. Because of their physical size, the cameras cannot be in the optimal pivoting point and you must deal with parallax and hence more difficult stitching issues. It is not surprising that GoPro action cameras used to be very popular for 360-degree video: they were small, affordable, fairly reliable, and produced good image quality. This was very important, because popular layouts required using six cameras, and not one of them could fail without ruining the whole shot.

Today, integrated 360-degree cameras also capture video. All you need to do is press the REC button, and after transferring the file to your phone let it stitch the video together. You can even do live streams, straight from your phone to the tubes. The 360-degree camera technology definitely has matured and can now be considered largely a solved problem, ready to be used by anyone.

### What's Next?

This is the big picture. 360-degree imaging can be now considered largely a solved problem and we are mostly seeing evolutionary steps to improve quality. It is already mature and cost-effective enough to be used in daily activities.

We can also see the fog disappearing and revealing the next stepping stone: while 360-degree video allows the viewer to turn around, *volumetric video* will remove the limitation that prevents the viewer from *taking a step* inside the world captured with cameras, and thus will make us feel being present even more. That technology is now taking its first steps and needs a few more years to begin to "work", and some more before the cameras and networks catch up.

There is no reason to wait, though: the future will always bring better, more advanced technology. So let us consider what it means that we can now let go of the limitations of image frame, after 200 years of photography. That is already a miracle of its own!

## Making LiveSYNC

In this chapter, we will briefly summarize the history behind the LiveSYNC product. We will discuss use cases that have been fundamental for designing the product's user interface and features.

### Release 1.0

Of course, the rise of 360-degree content production did not went unnoticed. In the early days you needed special software for embedding such content on your website, and a custom app if you wanted to play 360-degree video in a phone or a tablet. Today, most video sharing and social media services support at least basic monoscopic 360-degree photos and videos. Some services specialize in 360-degree content and typically allow creating tours where images and videos are linked to each other to make them navigable. All of this is great if you want to shoot and share content to your friends, followers, or potential customers. In other words, if it is mostly for fun or mass-marketing.

But what about content that is not intended to be public or not allowed to be uploaded to 3rd party services? How about controlled situations, such as private product presentations or employee training? What about using 360-degree media as part of speeches and lectures? And using 360-degree content as a canvas for annotating points-of-interest and sharing this information as visual memos or work instructions? Images and videos are *very* versatile, and 360-degree versions make no exception. Clearly there are many, many needs to satisfy. This means that professional tools and services are needed - solutions that go beyond what the freemium services can offer.

In 2012 a Finnish software company Finwe Ltd. was looking for a new direction. They had years of experience in motion sensors, 3D graphics, and user interfaces. First as an experiment, they created a 360-degree photo player. It was soon extended to support 360-degree video playback and presented in Mobile World Congress in 2013. This started to turn Finwe's focus on 360-degree media.

A few years later, Finwe had developed over 100 custom apps for 360 video playback. They wanted to create a product that would combine their best technologies into a generic player app with an easy-to-use remote control feature. Initially, the motivation was to help 360-degree video professionals in presenting their content to different audiences. Finwe demoed the concept at Mobile World Congress in February 2017 and released the first version a few months later for iOS and Android.

### Main Use Cases

360-degree content is best experienced using virtual reality headset. This experience is in fact so immersive that it also isolates the viewer from the rest of the world. It becomes a real problem when you are presenting content to another person and need to communicate with him during playback, for example, to give instructions, ask for an opinion, or simply to share the experience by chatting about it as the story unfolds. Yet, not being able to see what he sees forces you to continuously ask the viewer to explain what's on the screen. How awkward!

LiveSYNC removes this barrier by creating a screen mirroring style copy of the view from the VR headset to another device, typically a tablet computer. This view is updated in near real-time and can be further shared to a big screen, for example a TV or a projector, so that even a large audience - perhaps waiting for their own turn - can share the experience. Nobody likes waiting, though. Fortunately, with LiveSYNC you can connect to multiple headsets, play content synchronized in time, and observe the views of all of the devices simultaneously in a gorgeous video mosaic. You can also mix in traditional photos and videos, for example your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool.

Imagine that you are selling your summer cottage and made a set of 360° photos and videos so that potential buyers can virtually visit the place without spending hours in a car. But wouldn't it be convenient if you could also add a few notes to the images, to highlight things you want them to notice and remember? This is a built-in feature in LiveSYNC: when presenting content, simply drag & drop icons from included clipart sets over the photo or video - or create your own signs and use them instead. Is it possible to add text too? Of course. Powerful annotation features make LiveSYNC stand out.

The enterprise version of the LiveSYNC app goes much further: you can use a built-in editor to create a project, import media content, add interactive hotspots that can even fetch data from the network in real-time, import 2D and 3D maps with camera paths, etc. A really useful feature is to be able to export all the notes you've made as screenshots into a visual PDF report, and send it to your colleagues right away. Take a 5-minute tour on your construction site with a 360° camera, add new work instructions, export to PDF and mail away - all in 15 minutes. That's what we call *instant digitalization*.

## Key Concepts

TODO

### Understanding view mirroring

How does LiveSYNC actually work? What is possible and what is not? In general, there are two ways to mirror a live view of one device's screen to another:

1. **Streaming video (screencasting) **

    *Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection.*

    One benefit of this approach is that the mirrored view is an *exact* copy of what the other device is drawing on its screen. Another is that the mirroring feature can be implemented on the platform level and is thus available for all apps or the whole desktop environment.

    There are many downsides, too. Because it is essentially streaming high resolution video from one device to another, the method consumes a fair amount of CPU/GPU resources and power in *both* devices. It also requires a lot of network bandwidth when there is movement on screen, and it does not scale well: the network and the video decoder in the receiving end quickly become a bottleneck when more devices connect.

    Furthermore, when using a VR headset, we do not actually *want* to stream an exact copy of what is on screen. Instead of the distorted double barrel view we want to mirror a normal view.

2. **Streaming commands**

    *Principle: Integrate deep into the app and transmit only commands that allow the receiving end to reconstruct the view from the same assets.*

    The benefits of this method include low CPU/GPU and power consumption, trivial use of network bandwidth, and scalability. It is also possible to render the mirrored view a bit differently compared to the source device.

    The downside is that all devices must have local copies of the assets required for reconstructing the view. Also, with video content the resources of the control device can become a limitation in certain use cases. A control device may not have enough hardware resources to decode and play multiple videos simultaneously. Or, the same video simultaneously from different stream positions. This is not a problem, though, when all devices watch the same content in sync.

Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. Wifi does not work well enough and/or the organizers do not even allow using own Wifi hotspots.

As you probably guessed already, LiveSYNC uses type 2 method: streaming commands. This approach allows operation in crowded trade shows, especially when Bluetooth technology is used for communication. Also, observing a large number of viewer devices is possible. Etc.


