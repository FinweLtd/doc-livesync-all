{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to LiveSYNC\u2122 Learning Center \u00b6 This site contains the documentation for LiveSYNC Presentation Solution , a software tool developed by Finwe Ltd . Here you will find material for self-learning and problem-solving, as well as various downloadable resources. Solutions for learning \u00b6 How to Use This Site \u00b6 Read about different options for accessing this site, learn how to navigate, and get familiar with the used conventions. Quick Start \u00b6 Quick start guides are short introductions that summarize the steps to take to accomplish a task. These will get you going in minutes. Start from here if you are in a hurry. Tutorials \u00b6 Tutorials focus on a specific topic and collect the relevant information in one place. User Guide \u00b6 User Guide contains step-by-step instructions and deepens your understanding by providing background knowledge. Articles \u00b6 Articles are written by LiveSYNC experts. They focus on broader topics and specific use cases. Here you can learn best practices and find example configurations. FAQ \u00b6 Got a question? Check out frequently asked questions to get an answer quickly. Support \u00b6 Contact us to ask a question from the developers, send a feature request, suggest an article, or notify us of an error in the docs - we are listening. Resources \u00b6 Content Samples \u00b6 Download FREE sample files that demonstrate features and allow experimenting first hand. Templates \u00b6 Download FREE templates to be used as a basis for creating your own configuration. License \u00b6 Creative Commons \u00b6 This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License , excluding Twitter, Facebook, and other social media posts that link to this content Downloadable resource files, which are distributed under a different license","title":"Home"},{"location":"#welcome-to-livesynctm-learning-center","text":"This site contains the documentation for LiveSYNC Presentation Solution , a software tool developed by Finwe Ltd . Here you will find material for self-learning and problem-solving, as well as various downloadable resources.","title":"Welcome to LiveSYNC\u2122 Learning Center"},{"location":"#solutions-for-learning","text":"","title":"Solutions for learning"},{"location":"#how-to-use-this-site","text":"Read about different options for accessing this site, learn how to navigate, and get familiar with the used conventions.","title":"How to Use This Site"},{"location":"#quick-start","text":"Quick start guides are short introductions that summarize the steps to take to accomplish a task. These will get you going in minutes. Start from here if you are in a hurry.","title":"Quick Start"},{"location":"#tutorials","text":"Tutorials focus on a specific topic and collect the relevant information in one place.","title":"Tutorials"},{"location":"#user-guide","text":"User Guide contains step-by-step instructions and deepens your understanding by providing background knowledge.","title":"User Guide"},{"location":"#articles","text":"Articles are written by LiveSYNC experts. They focus on broader topics and specific use cases. Here you can learn best practices and find example configurations.","title":"Articles"},{"location":"#faq","text":"Got a question? Check out frequently asked questions to get an answer quickly.","title":"FAQ"},{"location":"#support","text":"Contact us to ask a question from the developers, send a feature request, suggest an article, or notify us of an error in the docs - we are listening.","title":"Support"},{"location":"#resources","text":"","title":"Resources"},{"location":"#content-samples","text":"Download FREE sample files that demonstrate features and allow experimenting first hand.","title":"Content Samples"},{"location":"#templates","text":"Download FREE templates to be used as a basis for creating your own configuration.","title":"Templates"},{"location":"#license","text":"","title":"License"},{"location":"#creative-commons","text":"This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License , excluding Twitter, Facebook, and other social media posts that link to this content Downloadable resource files, which are distributed under a different license","title":"Creative Commons"},{"location":"SUMMARY/","text":"Table of contents \u00b6 Initial page","title":"Table of contents"},{"location":"SUMMARY/#table-of-contents","text":"Initial page","title":"Table of contents"},{"location":"TODO/","text":"TODO \u00b6 User Guide \u00b6 Installing: subscription full procedure, both stores with screen captures Configuration: Oculus Go audience mode Managing assets: Windows shares, dropbox, etc. alternatives - proper instructions Downloads \u00b6 Add links to video streams (.mp4 and .m3u8) Add more resolution variants to downloadable files Add stereo (3D) test images Add spatial audio (.obb) samples Add hotspot set Encoding \u00b6 For Oculus Go: https://creator.oculus.com/blog/encoding-high-resolution-360-and-180-video-for-oculus-go/?fbclid=IwAR0oS59ITYOxxkXZ_Ags-JS4ZEnnFW_pEAOE4sK7EaNpp8Fn-mySdqapfTs","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#user-guide","text":"Installing: subscription full procedure, both stores with screen captures Configuration: Oculus Go audience mode Managing assets: Windows shares, dropbox, etc. alternatives - proper instructions","title":"User Guide"},{"location":"TODO/#downloads","text":"Add links to video streams (.mp4 and .m3u8) Add more resolution variants to downloadable files Add stereo (3D) test images Add spatial audio (.obb) samples Add hotspot set","title":"Downloads"},{"location":"TODO/#encoding","text":"For Oculus Go: https://creator.oculus.com/blog/encoding-high-resolution-360-and-180-video-for-oculus-go/?fbclid=IwAR0oS59ITYOxxkXZ_Ags-JS4ZEnnFW_pEAOE4sK7EaNpp8Fn-mySdqapfTs","title":"Encoding"},{"location":"articles/articles/","text":"Articles \u00b6 Articles focus on specific topics and use cases. Here you can learn best practises and find example configurations. Understanding Resolution in 360\u00b0 Images \u00b6 This article discusses what image resolution means for 360-degree content, how we can measure it, how it compares to resolution in traditional 2D content, and presents some example calculations.","title":"Index"},{"location":"articles/articles/#articles","text":"Articles focus on specific topics and use cases. Here you can learn best practises and find example configurations.","title":"Articles"},{"location":"articles/articles/#understanding-resolution-in-360-images","text":"This article discusses what image resolution means for 360-degree content, how we can measure it, how it compares to resolution in traditional 2D content, and presents some example calculations.","title":"Understanding Resolution in 360\u00b0 Images"},{"location":"articles/resolution/","text":"Understanding Resolution in 360\u00b0 Images \u00b6 Abstract \u00b6 This article discusses image quality. We focus on image resolution: the amount of detail and clarity. These are very significant to the perceived quality of an image. A content producer must find a reasonable balance between image quality and size. There are also many technical limitations to take into account. We have collected relevant information to one place to make it easier to choose a resolution. We also provide insights from Finwe experts. The article starts with the basics. We explore many different ways to understand, describe, and measure resolution. Then, we expand what we have learned to panoramic images. Devices used for content production and playback are presented and technical limitations discussed. Finally, we present our best practices and recommendations. Definition \u00b6 First, let us define what we mean by image resolution . Image resolution is the detail an image holds ... Higher resolution means more image detail. ( 1 ) The concept of image resolution applies to raster digital images ( 2 ), such as JPG, PNG and BMP images. Images created with vector graphics ( 3 ) are different: they are defined mathematically. One example is SVG images, which are used in apps and on web pages. For vector images, a resolution is meaningful only when they are saved as raster images. Digital cameras produce raster images. 360-degree photos and video frames make no exception. Sometimes a 360-degree image is rendered from a mathematical model, such as a CAD model of a building. The concept of resolution applies to the rasterized output file. To be specific, in this article we mostly discuss pixel resolution ie. the pixel count in the image. We also sometimes refer to spatial resolution ie. how closely lines can be resolved in an image, for example in a printed photograph. We do not discuss spectral resolution ie. the ability to resolve spectral features and bands, for example, colors. From now on, when we use the words image resolution or resolution alone, we mean pixel resolution. Pixel resolution illustration by Wikipedia ( 1 ) Resolution in 2D images \u00b6 Image resolution \u00b6 There are several methods to describe image resolution. In this article, we will use many of them: ... the convention is to describe the pixel resolution with the set of two positive integer numbers, where the first number is the number of pixel columns (width) and the second is the number of pixel rows (height), for example as 7680 \u00d7 6876. Another popular convention is to cite resolution as the total number of pixels in the image, typically given as number of megapixels, which can be calculated by multiplying pixel columns by pixel rows and dividing by one million. Other conventions include describing pixels per length unit or pixels per area unit, such as pixels per inch or per square inch. ( 1 ) As an example, the image below is 1024 pixels wide and 1024 pixels tall. We can describe its resolution as 1024 x 1024 pixels (width x height). We can also calculate the total number of pixels the image contains by multiplying width and height: 1024 px x 1024 px = 1048576 px After dividing this value by one million, we get the resolution in megapixels. It is usually rounded to the nearest integer, but sometimes to the first decimal: 1048576 px / 1000000 = 1.048576 MPx ~ 1.0 MPx ~ 1 MPx In other words, this image has a 1024x1024 or 1-megapixel resolution. It looks pretty sharp on the screen. It means that its resolution is high enough for the amount of detail it represents. We cannot say much about its number of pixels per length unit or per area unit . The reason is that on this website its size will be scaled to match with the width of this text column. The physical size of this text column depends on the size of the display device and application window. Hence, at the time of writing this, it is impossible to know the size of the image as it appears on your screen. As an example, let us consider a tablet's screen. The width of this column could be, say 163 mm. Hence, we could argue that its resolution must be about 6 pixels per millimeter: 1024 px / 163 mm = ~6.28 px/mm A more common representation uses the imperial notation, pixels per inch (ppi): 1024 px / 6.4 in = 160 ppi The value calculated above is not correct. If you are reading this from a computer screen, try changing the width of your browser window. You should see that the image above will be automatically resized. The resolution of the image file that your browser has downloaded has not changed. What you see on screen is actually a copy of the original image that your browser has created and scaled to match with this column's width. That copy probably has a different resolution than the original file does. Hence, our statement is using a wrong pixel count and is not true. How to find out the resolution of an image that appears on screen ? We need to know the physical size of the image on the screen and the pixel density of the display device. For example, a modern Apple iPad has 264 pixels per inch (ppi) ( 4 ). Thus, an image that is 6.4 inches wide should have about 1690 pixels when it is drawn on an iPad's screen. Curiously, this happens to be 666 pixels more than we have in the original image! Where did these extra pixels come from? 264 ppi * 6.4 in = ~1690 px 1690 px - 1024 px = +666 px Screen resolution \u00b6 When an image is captured and saved into a file, the file maintains the original image resolution ie. the number of pixels that were captured. When the image is loaded from a file and rendered on screen, it has a new property called screen resolution . This is the number of display pixels that are used for drawing the image on the screen. The former describes the amount of detail in the image but has nothing to do with its physical size. The latter has everything to do with physical size but says nothing about detail. Detail and size can be both expressed as a number of pixels, yet they are two different things. Now, although they are different things, they both play a part in the end result. Thus, we must understand the relation between image resolution and screen resolution. Consider our test image again: what happens if we reduce its image resolution to 100 x 100 pixels? Certainly, it would lose most of its detail! You cannot express such a complex image properly with so few pixels. Yet, on screen the browser would still scale it to match with the width of this column, no questions asked. This loss of detail is illustrated below. A 1024 x 1024 pixel image is first downscaled to 100 x 100 pixels and then upscaled back to 1024 x 1024 pixels. The result is a blurry mess. Left: Original image, 1024 x 1024 pixels. Center: Downscaled image, 100 x 100 pixels. Most of the detail has been permanently lost. Right: Upscaled image (1024 x 1024 pixels). Lost detail could not be recovered. How does this happen? Obviously, your computer must use much more than 100 x 100 pixels to draw the image on the screen. The display pixels are small and you need lots of them to draw a large image. But your computer cannot magically recreate the detail that was lost when we scaled down the original image. It is not possible, even if you see this happen regularly in crime series on TV. Hence, to create a larger image ie. a scaled-up version, the only option is to copy the same (or interpolated) pixels side by side. The resulting image on the screen will indeed have lots of pixels. But that only means it has high screen resolution ie. large size. Its image resolution is still 100 x 100 pixels, and contains very little detail. How to ensure that we get a sharp image on the screen? We must have enough space for detail (high enough image resolution). Or, we must decrease image size (low enough screen resolution). In the beginning, we briefly mentioned spatial resolution . This is what it describes: the ability to distinguish detail from an image that has a certain physical size. Or, more simply put: the clarity of the image. Spatial resolution is not just the value of pixels per inch. The maximum spatial resolution of an image is limited by the properties of the system that created the image. As an example, the lens of your camera is the first loop in a long chain. You can easily make things worse by manipulating image resolution and/or screen resolution, but you cannot make it better. (Well, maybe a little, by applying a filter that emphasizes certain features in the image.) What is the optimal relation between image resolution and screen resolution? When we prepare the final version of our image, then the optimal relation is that they are equal . Screen resolution defines what will be drawn on the screen ie. what user will see. Higher image resolution does not bring any benefit, as added detail cannot be seen. On the other hand, it may cause downscaling artifacts and will require a larger file size. Lower image resolution leads to upscaling ie. less detail than what an equal resolution would provide. Print resolution \u00b6 To complete our analysis for 2D images, let us briefly consider printing . If we printed our test image on paper in 100 x 100 mm size, we would have point density of ~10 points per millimeter. Correct? 1024 px / 100 mm = 10.24 points/mm A more common representation uses the imperial notation, dots per inch (dpi): 1024 px / 3.94 in = ~260 dpi It is not correct. A printer cannot print any number of points per millimeter. It has a resolution of its own, just like a display device has. The calculated value is a bit less than what a typical 300 dpi resolution laser printer is capable of. If we wish to use the printer's maximum resolution to reach the best possible print quality, we must satisfy with smaller image size than 100 mm x 100 mm: 1024 px / 300 dpi = 3.41 in ~ 87 mm 1024 x 1024 pixel image (left) printed from Photoshop in 300 dpi with a laser printer (right) is about 87 x 87 mm on paper. If we decide to print the image in 100 x 100 mm size, then the printer's driver must scale up our image to produce content for the \"missing\" pixels (actually dots). This is similar to what a web browser does when it stretches an image to a larger size on display by copying/interpolating pixels. 300 dpi * 3.94 in = 1182 px 1182 px - 1024 px = +152 px In reality, a laser printer users a rasterization algorithm to produce gray colors. It does this by alternating empty space between black printed dots. Hence, there will not be a 1:1 mapping from pixels to dots when you print a photograph. You can see this in action in the printed image above by looking at the grayscale squares. Nowadays many laser printers have 600 dpi or higher resolution. Inkjet printers often claim much higher dpi values. This is most aggressive marketing, but they also measure dpi as the number of drops of ink per inch. Moreover, they use algorithms that are supposed to improve print quality via optimization. Furthermore, in an image file the pixels are next to each other but on paper, the drops of ink have space between them. All in all, it is not straightforward to understand what is the actual mapping of image pixels to dots on paper. One rule of thumb is that most book publishers require images in 300 ppi resolution. Hence, if you want to print an image in 100 x 100 mm size on a book page, your image file should be at least 1182 x 1182 pixels. 300 dpi * 3.94 in = 1182 px Resolution in 360-degree images \u00b6 Equirectangular projection \u00b6 Now we will move on to discuss image resolution in 360-degree images. Let us begin by creating a wide-angle version of our test image. We will use our original test image as a single cube face and make five copies of it to produce the six faces of a cube. Then we will put a virtual camera to its center point and render a spherical 360-degree image. To save that 3D image into a flat PNG file, we must choose a projection . It must be one of those that are designed for projecting a sphere into a planar surface. (This is similar to how we project a map of our dear planet Earth to a piece of paper.) Here we will use the most common panorama projection: the equirectangular. The result looks like this: 360-degree image rendered from inside a test cube using equirectangular projection. Take a moment to examine the image above and find the four cube faces at the horizon level (front, right, back, left) as well as the cube's top and bottom faces (they are heavily distorted). The image covers 360 degrees horizontally: at the center of the cube, our virtual camera has turned around full 360 degrees along the yaw angle. As a consequence, left and right edges of the image match with each other seamlessly. The image also covers 180 degrees vertically: at the center of the cube, our virtual camera has turned around 180 degrees along the pitch angle. The top row of pixels are all the same and they illustrate what is exactly above the camera (zenith). The bottom row of pixels are also all the same and they illustrate what is exactly below the camera (natural direction, nadir). Thus, this is a full spherical 360x180 degree image and it covers every possible viewing direction that can be seen from the center of the cube. Of course, the described camera rotation is just a convention. It could have turned 180 degrees horizontally and 360 degrees vertically to produce a spherical image with a 1:2 aspect ratio. Also, the rotation angles do not need to align with the main axis at all to produce a spherical image, they just need to be perpendicular to each other. Notice that a spherical image is NOT 360x360 degrees; that is mathematically incorrect representation although it is sometimes seen in marketing materials. A 720-degree image is even more wrong! To see how a video player projects equirectangular video frames on screen, play the video below. Try interacting with it by panning. It is easier if you first select the fullscreen mode. While playing the video you may notice that image clarity suddenly changes. This is adaptive streaming in action: the player measures network bandwidth during playback and changes to the maximum resolution that bandwidth allows. Aspect ratio \u00b6 Equirectangular projection looks perhaps a bit weird at first sight. To understand it better, let us remind ourselves how the map of the World looks like in this projection. You have probably seen it before: An example of equirectangular projection by Wikipedia ( 5 ) In our previous example, we started from a 3D cube . We saw how straight lines got bent when the equirectangular projection was applied. This time we start from a 3D sphere (the Earth). Notice how the meridians are projected as vertical straight lines of constant spacing . And circles of latitude as horizontal straight lines of constant spacing . The equirectangular projection has many fallacies. It is not an equal area projection (see how large the Antarctic appears). It is not conformal (angles are not preserved). It is wasteful (lots of redundant pixels). Yet, the equirectangular projection is the de-facto standard in 360-degree imaging. This is probably related to the particularly simple relationship between the position of a pixel in the image and corresponding point on the surface of a sphere. It makes the life of software developers easy. Consider a full spherical equirectangular image whose resolution is 1024 x 512 pixels. (The map above will do.) Choose any pixel in the image. Now, if you want to move 30 degrees east, you can calculate this in pixels as shown below. To move in a vertical direction, you use 180\u00b0 degrees instead of 360\u00b0. If you pass the edge of the image, continue in the same direction from the opposite edge of the image. That's it. Simple linear relationship. It may not be obvious, but other projections are considerably more complex to handle. 1024 px * (30\u00b0/360\u00b0) = 85 px 2D images have a property called aspect ratio . It describes the proportional relationship between its width and its height. For example, a square has aspect ratio 1:1 because width and height are the same. An image whose resolution is 1920 x 1080 pixels has aspect ratio 16:9 or 1.77:1. 1920 x 1080 = (120 * 16) x (120 * 9) 16 : 9 = 1.77 In the World map image, horizontal and vertical lines appear at 15-degree intervals. Count the lines. You will notice that there is twice the number of vertical lines than horizontal lines. We can make a conclusion that proper aspect ratio for a full spherical equirectangular image must be 2:1 . This is because horizontally there are twice the amount of degrees than vertically (360 x 180). Also, we want to treat horizontal and vertical degrees equally ie. use the same amount of pixels per degree. 2:1 aspect ratio is a bit different than what we have used to, but not that far off from common video formats: Aspect ratio Normalized ratio Typical use 4:3 1.33:1 Traditional TV 16:9 1.78:1 HD video 1.85:1 1.85:1 Most movies 2:1 2.00:1 Equirectangular panoramas 2.39:1 2.39:1 Widescreen movies When you deliver 360-degree images or videos using the equirectangular projection, use 2:1 aspect ratio. Image width should be two times its height. For example, 1920 x 960 has proper aspect ratio 2:1. Commonly used FullHD resolution 1920 x 1080 is 16:9. It is taller . With that resolution, your image has more capacity for detail vertically than horizontally. 360-degree video players can handle that, but from the content point of view it makes little sense. Likewise, if you aim for 4K quality use 3840x1920 (2:1), not 3840x2160 (16:9). Left: 360x180 equirectangular image in correct aspect ratio 2:1. Right: The same image in incorrect 16:9 aspect ratio. You may wonder what harm is in using one of 16:9 resolutions. The answer is: not much. Consider a case where you need to downscale a video from 6K camera to 4K output. With 3840x2160 (16:9) you have better panorama resolution vertically than horizontally. You can do that, but why would you? Often this happens accidentally when you transcode a video into a different format. The default values in video software tend to lead to a 16:9 aspect ratio. If you are not careful, your 3840x1920 (2:1) video may become 3840x2160 (16:9). In such a case, you will add redundant data and create a larger file size without any benefit. Field-of-view \u00b6 We often refer to full spherical panorama images as 360x180 degree image. Or shortly, 360-degree image. What exactly does this value describe? The correct term is field-of-view or shortly FOV. Field-of-view is simply the size of an observable area, described in degrees ( 8 ). You have probably used a camera that has a zoom lens. When you utilize the zoom feature, you are effectively changing the field-of-view. When you zoom in , objects appear closer but the view becomes narrower. Hence, you have smaller field-of-view. When you zoom out , you see wider area through the viewfinder. You have larger field-of-view. Horizontal, vertical, and diagonal field-of-view illustraded by Wikipedia ( 8 ) We must be careful with field-of-view values, though. It can be horizontal, vertical, or diagonal measure of a rectangular viewable area. Unfortunately, it is not always mentioned which of these the given number is referring to. If you know one value and the aspect ratio, you can calculate the others using simple trigonometry. Abbreviation Meaning FOV Field-of-view in general (ambiguous) HFOV Horizontal field-of-view VFOV Vertical field-of-view DFOV Diagonal field-of-view When we view 360-degree content, there are effectively two different FOV values that play a part. Image field-of-view refers to the total observable area in the whole image frame. For example, in a spherical panorama image HFOV is 360 degrees and VFOV is 180 degrees. Diagonal FOV is not used. Viewport field-of-view is the part that is visible in the panorama image viewer or video player. For example, a pair of VR glasses could have HFOV 94 degrees. In many players this value is adjustable: user can zoom by changing the viewport's FOV. Panorama resolution \u00b6 Now, scroll back up and take a closer look at our equirectangular image of a cube. If you observe it carefully, you will notice that this image does not look as sharp as our original cube face. They are both 1024 pixels wide images and also scaled to the same width on the screen. Hence, both image resolution and screen resolution are exactly the same (horizontally). What is different? Recall that we added 5 more cube faces - much more information - to this new image. Yet we didn't increase image resolution ie. the amount of detail the image can hold. Major error. Besides, we changed the projection and halved the vertical resolution from 1024 px to 512 px. How can we measure the amount of detail in panoramic images, where field-of-view (the observable area) varies? As shown above, image resolution and screen resolution do not take this into account. We need something else. Consider that four cube faces (front, right, back, left) cover 360-degree spin at the horizon level. This means that one cube face must cover 90 degrees field-of-view (360/4=90). The width of our equirectangular image of a cube is 1024 pixels and its height is 512 pixels (2:1 aspect ratio). Thus, we can calculate its resolution per degree as follows and express it in pixels per degree (ppd): 1024 px / 360\u00b0 = ~2.84 ppd 512 px / 180\u00b0 = ~2.84 ppd To compare, our original cube covers only 90 degrees and has a higher resolution per degree: 1024 px / 90\u00b0 = ~11.38 ppd How can we make our 360-degree cube image carry as much detail as the original 90-degree cube face? Let us focus on the horizon level where the projection does not distort the image much. We need 4x the resolution that we have now: 360\u00b0 * 11.38 ppd = ~4096 px What if we made a 180-degree version, assuming viewers will focus to the front direction? How much resolution do we need to maintain detail? Simple: 180\u00b0 * 11.38 ppd = ~2048 px Resolution per degree appears to be a useful method for expressing the amount of detail in panoramic images. The benefit is that values are comparable despite used field-of-view. In this article, we will use the term panorama resolution to reference it. Notice that there are projections that do not provide constant resolution per degree throughout the image. To be specific, we probably should use the term equirectangular resolution . Here we use the easier and shorter term because equirectangular projection is so widely used. We have also learned that 360-degree images need much higher image resolution than ordinary 2D images to \"look the same quality\". This is often hard to understand for end users. People have a mental model of what \"FullHD\" or \"4K\" content is supposed to look like. But through a 360-degree player, you are likely seeing about 1/8th of the total amount of pixels at any moment of time! Let us rephrase the primary concepts of resolution for 360-degree images: Image resolution = total amount of detail in an image file / in a decoded video frame, in pixels Image field-of-view = total size of an observable area captured into an image, in degrees Panorama resolution = amount of detail per degree, in ppd Screen resolution = viewport size on screen, in pixels Viewport field-of-view = size of an observable area through a viewport, in degrees Towards perfect resolution \u00b6 Pixel perfect \u00b6 Retina resolution \u00b6 In order to make an image look sharp on screen, both image resolution and screen resolution must be high enough: you need enough pixels to store the details and enough pixels to draw them on screen so that they remain distinguishable. We have already said that it is best if they match perfectly: you can avoid aliasing errors that come from scaling if your image has exactly the same amount of pixels in file and on screen. You will also preserve all the detail without wasting memory for something that cannot be seen by user. A well known example is matching image resolutions of iOS app icons and button graphics with their screen resolutions so that they will look \"perfect\" as no scaling artifacts will occur. Nowadays it is becoming less common to aim for pixel perfect presentation as there are so many different aspect ratios and screen resolutions that need to be supported. For example, on Android the approach is different: to support thousands of different device models developers cannot make large amount of variants of each app icon; instead they are supposed to use vector graphics and let the device rasterize a pixel perfect copy at runtime. This approach of course does not work with images that have been captured with a camera and are raster images by nature. The solution is simple: use images whose pixel resolution matches with the highest screen resolution that will be needed. The operating system will downscale the images to lower resolutions on other device models. You can also choose to provide a fistful of different sizes and let the system select the nearest match. It is not perfect solution but good enough. It is also worth to realize that the concept of pixel perfect imaging mainly applies to traditional 2D images; pixel-to-pixel matching is not possible with spherical 360-degree images as they need to be projected from a spherical (curved) surface to a flat surface in order to be stored in common image and video formats, then projected back to spherical (curved) surface for 3D presentation within the image viewer / video player application, and once again to flat display surface when it is time to render part of the 360-degree image on screen. Try to preserve pixel-to-pixel matching through those operations! This leads us to a question: what would be the equivalent of pixel perfect presentation in 360-degree images? Since we cannot know all the internal processing that occurs when a specific pipeline renders our image on screen, in practice we just need an image that has enough resolution so that all the processing will not become visible in the end result. Then, what resolution is needed to make a 360-degree image look \"perfect\"? There is no single answer, as it depends on many parameters such as display resolution, used field-of-view, viewing distance, and projection. Let's discuss a few selected cases. When Apple introduced iPhone4, they claimed that their new Retina displays have high enough pixel density that the human eye cannot notice pixelation at a typical viewing distance. In other words, the display would appear to look perfect (when it comes to resolution) since you are unable to see the small dots that the image is made of. According to Apple the spatial resolution required for this is about 300 ppi for a device held 10-12 inches from the eye . In reality, a typical iPad has 9.7\" inch screen size (diagonally), 264 ppi display panel and 2048 x 1536 resolution. Now, if we want to view a 360-degree photograph from an iPad's screen that we will hold 10 inches from the eye, what would be the minimum pixel resolution for our 360-degree image so that it would provide optimal quality ie. have enough pixels for that display when we turn full 360 degrees around? Let's do some math: Consider a circle that aligns with the horizon around a spectators head. This is the path the iPad travels when we hold it in front of our eyes and turn around 360 degrees to see the complete 360-degree image. To simplify matters, we will assume a single eye at the center of the circle. The radius of the circle is 10 inches (25.4 cm) and its circumference is: 2 * PI * 10 in = 62.8 in ~160 cm Let us imagine that instead of moving a real iPad along this circular path, Apple would produce a round cylindrical display and we would simply go stand at the center of it. Our imaginary round iPad retina display would be 62.8 inches \"wide\" (circumference). Then, how many pixels would it have (horizontally)? 62.8 in * 264 ppi = ~16588 px If our imaginary display would be spherical , its resolution would be 16588 x 8294 pixels (because of aspect ratio 2:1). That would be a big display with almost 140 megapixels and not practical at all (how would you even get inside?) Of course, in reality we will move an ordinary tablet along a circular path, detect the movement with sensors, and update image content on screen. 16588 x 8294 = 137580872 ~ 138 Mpx Yet from content creation point of view our target is that imaginary spherical display. Creating content for such a display is not impossible at all: we can easily create much, much larger 360-degree photographs, and 8K 360-degree video is becoming commonplace - when we double that to 16K a few years later, we have achieved retina resolution for iPads. Neat. What if our 360-degree image is viewed through VR glasses? It is difficult to say what would be the correct distance for the radius of the circle that we used in the previous analysis: the display itself is really close to the eye, so the radius appears to be a small value. But in reality the image is viewed through a lens, and that must be taken into account. The focal distances in different headsets is usually not mentioned in the product specifications. Fortunately, there is a nice shortcut. Let us consider Oculus Go, whose display resolution is 2560 x 1440 in total, and 1280 x 1440 per eye. The horizontal field-of-view that a single eye covers in Oculus Go is said to be about the same as in Oculus Rift, where it was 94 degrees. Therefore, a 360-degree panorama image for Oculus Go should have at least 4902 x 2451 resolution or about 12 megapixels. This can be easily satisfied already by filming 360-degree photos and videos in 8K (or 6K) and providing final output in about 5K. 1280 px / 94\u00b0 = 13.61 px/\u00b0 360\u00b0 * 13.61 px/\u00b0 = 4902 px 4902 px / 2 = 2451 px 4902 px * 2451 px = ~12015115 px ~ 12.0 Mpx Human eye resolution \u00b6 However, Oculus Go is a nice headset but far from the concept of a retina display. If we want to future-proof our content and target for VR headsets that have \"retina displays\", what resolution do we need? This is the same question as what is the resolution of a human eye itself, how many megapixels? Human eye illustration by Wikipedia ( 6 ) According to ( 7 ), let us consider a view in front of a spectator that is 90 degrees by 90 degrees. The number of pixels a human eye could see through such window is ~324 Mpx. Remember our cube face that was also 90 by 90 degrees? To have full 360-degree view we need 6 cube faces, resulting to 6 * 324 Mpx = 1944 Mpx or 1.9 Gpx. If we wanted to use equirectangular projection instead, we will find that if square contains 324 Mpx then its side length is 18000 px and at horizon line we need 4 x 90 degrees for a 360-degree images, therefore 4 x 18000 px = 72000 px. Aspect ratio 2:1 yields 72000 x 36000 px and in total 2592 Mpx or 2.6 Gpx. Why do we need so much more pixels with equirectangular projection, compared to a cube map? This is because equirectangular projection is wasteful: there are multiple copies of same pixels ie. pixels that provide no new information. Cubemap projection requires 25% less pixels to produce same output. The interesting point is that about 2-3 Gpx image (72000 x 36000 px) should be enough for human eye, now and in the future. Hey, wait a minute? What about Apple's retina displays - we already calculated that 138 Mpx should be enough, right? Well, it is kind of apples and oranges case: The claim about retina displays is that at the typical viewing distance of 10-12 inches user is not suppose to see individual pixels, which is not exactly the same thing as how much details a human eye can see in best conditions. Also, ( 7 ) adds an interesting scientific point of view - apparently humans can see more details than our retina alone is able to distinguish: The eye is not a single frame snapshot camera. It is more like a video stream. The eye moves rapidly in small angular amounts and continually updates the image in one's brain to \"paint\" the detail. We also have two eyes, and our brains combine the signals to increase the resolution further. We also typically move our eyes around the scene to gather more information. Because of these factors, the eye plus brain assembles a higher resolution image than possible with the number of photoreceptors in the retina. So the megapixel equivalent numbers below refer to the spatial detail in an image that would be required to show what the human eye could see when you view a scene.","title":"Understanding resolution"},{"location":"articles/resolution/#understanding-resolution-in-360-images","text":"","title":"Understanding Resolution in 360\u00b0 Images"},{"location":"articles/resolution/#abstract","text":"This article discusses image quality. We focus on image resolution: the amount of detail and clarity. These are very significant to the perceived quality of an image. A content producer must find a reasonable balance between image quality and size. There are also many technical limitations to take into account. We have collected relevant information to one place to make it easier to choose a resolution. We also provide insights from Finwe experts. The article starts with the basics. We explore many different ways to understand, describe, and measure resolution. Then, we expand what we have learned to panoramic images. Devices used for content production and playback are presented and technical limitations discussed. Finally, we present our best practices and recommendations.","title":"Abstract"},{"location":"articles/resolution/#definition","text":"First, let us define what we mean by image resolution . Image resolution is the detail an image holds ... Higher resolution means more image detail. ( 1 ) The concept of image resolution applies to raster digital images ( 2 ), such as JPG, PNG and BMP images. Images created with vector graphics ( 3 ) are different: they are defined mathematically. One example is SVG images, which are used in apps and on web pages. For vector images, a resolution is meaningful only when they are saved as raster images. Digital cameras produce raster images. 360-degree photos and video frames make no exception. Sometimes a 360-degree image is rendered from a mathematical model, such as a CAD model of a building. The concept of resolution applies to the rasterized output file. To be specific, in this article we mostly discuss pixel resolution ie. the pixel count in the image. We also sometimes refer to spatial resolution ie. how closely lines can be resolved in an image, for example in a printed photograph. We do not discuss spectral resolution ie. the ability to resolve spectral features and bands, for example, colors. From now on, when we use the words image resolution or resolution alone, we mean pixel resolution. Pixel resolution illustration by Wikipedia ( 1 )","title":"Definition"},{"location":"articles/resolution/#resolution-in-2d-images","text":"","title":"Resolution in 2D images"},{"location":"articles/resolution/#image-resolution","text":"There are several methods to describe image resolution. In this article, we will use many of them: ... the convention is to describe the pixel resolution with the set of two positive integer numbers, where the first number is the number of pixel columns (width) and the second is the number of pixel rows (height), for example as 7680 \u00d7 6876. Another popular convention is to cite resolution as the total number of pixels in the image, typically given as number of megapixels, which can be calculated by multiplying pixel columns by pixel rows and dividing by one million. Other conventions include describing pixels per length unit or pixels per area unit, such as pixels per inch or per square inch. ( 1 ) As an example, the image below is 1024 pixels wide and 1024 pixels tall. We can describe its resolution as 1024 x 1024 pixels (width x height). We can also calculate the total number of pixels the image contains by multiplying width and height: 1024 px x 1024 px = 1048576 px After dividing this value by one million, we get the resolution in megapixels. It is usually rounded to the nearest integer, but sometimes to the first decimal: 1048576 px / 1000000 = 1.048576 MPx ~ 1.0 MPx ~ 1 MPx In other words, this image has a 1024x1024 or 1-megapixel resolution. It looks pretty sharp on the screen. It means that its resolution is high enough for the amount of detail it represents. We cannot say much about its number of pixels per length unit or per area unit . The reason is that on this website its size will be scaled to match with the width of this text column. The physical size of this text column depends on the size of the display device and application window. Hence, at the time of writing this, it is impossible to know the size of the image as it appears on your screen. As an example, let us consider a tablet's screen. The width of this column could be, say 163 mm. Hence, we could argue that its resolution must be about 6 pixels per millimeter: 1024 px / 163 mm = ~6.28 px/mm A more common representation uses the imperial notation, pixels per inch (ppi): 1024 px / 6.4 in = 160 ppi The value calculated above is not correct. If you are reading this from a computer screen, try changing the width of your browser window. You should see that the image above will be automatically resized. The resolution of the image file that your browser has downloaded has not changed. What you see on screen is actually a copy of the original image that your browser has created and scaled to match with this column's width. That copy probably has a different resolution than the original file does. Hence, our statement is using a wrong pixel count and is not true. How to find out the resolution of an image that appears on screen ? We need to know the physical size of the image on the screen and the pixel density of the display device. For example, a modern Apple iPad has 264 pixels per inch (ppi) ( 4 ). Thus, an image that is 6.4 inches wide should have about 1690 pixels when it is drawn on an iPad's screen. Curiously, this happens to be 666 pixels more than we have in the original image! Where did these extra pixels come from? 264 ppi * 6.4 in = ~1690 px 1690 px - 1024 px = +666 px","title":"Image resolution"},{"location":"articles/resolution/#screen-resolution","text":"When an image is captured and saved into a file, the file maintains the original image resolution ie. the number of pixels that were captured. When the image is loaded from a file and rendered on screen, it has a new property called screen resolution . This is the number of display pixels that are used for drawing the image on the screen. The former describes the amount of detail in the image but has nothing to do with its physical size. The latter has everything to do with physical size but says nothing about detail. Detail and size can be both expressed as a number of pixels, yet they are two different things. Now, although they are different things, they both play a part in the end result. Thus, we must understand the relation between image resolution and screen resolution. Consider our test image again: what happens if we reduce its image resolution to 100 x 100 pixels? Certainly, it would lose most of its detail! You cannot express such a complex image properly with so few pixels. Yet, on screen the browser would still scale it to match with the width of this column, no questions asked. This loss of detail is illustrated below. A 1024 x 1024 pixel image is first downscaled to 100 x 100 pixels and then upscaled back to 1024 x 1024 pixels. The result is a blurry mess. Left: Original image, 1024 x 1024 pixels. Center: Downscaled image, 100 x 100 pixels. Most of the detail has been permanently lost. Right: Upscaled image (1024 x 1024 pixels). Lost detail could not be recovered. How does this happen? Obviously, your computer must use much more than 100 x 100 pixels to draw the image on the screen. The display pixels are small and you need lots of them to draw a large image. But your computer cannot magically recreate the detail that was lost when we scaled down the original image. It is not possible, even if you see this happen regularly in crime series on TV. Hence, to create a larger image ie. a scaled-up version, the only option is to copy the same (or interpolated) pixels side by side. The resulting image on the screen will indeed have lots of pixels. But that only means it has high screen resolution ie. large size. Its image resolution is still 100 x 100 pixels, and contains very little detail. How to ensure that we get a sharp image on the screen? We must have enough space for detail (high enough image resolution). Or, we must decrease image size (low enough screen resolution). In the beginning, we briefly mentioned spatial resolution . This is what it describes: the ability to distinguish detail from an image that has a certain physical size. Or, more simply put: the clarity of the image. Spatial resolution is not just the value of pixels per inch. The maximum spatial resolution of an image is limited by the properties of the system that created the image. As an example, the lens of your camera is the first loop in a long chain. You can easily make things worse by manipulating image resolution and/or screen resolution, but you cannot make it better. (Well, maybe a little, by applying a filter that emphasizes certain features in the image.) What is the optimal relation between image resolution and screen resolution? When we prepare the final version of our image, then the optimal relation is that they are equal . Screen resolution defines what will be drawn on the screen ie. what user will see. Higher image resolution does not bring any benefit, as added detail cannot be seen. On the other hand, it may cause downscaling artifacts and will require a larger file size. Lower image resolution leads to upscaling ie. less detail than what an equal resolution would provide.","title":"Screen resolution"},{"location":"articles/resolution/#print-resolution","text":"To complete our analysis for 2D images, let us briefly consider printing . If we printed our test image on paper in 100 x 100 mm size, we would have point density of ~10 points per millimeter. Correct? 1024 px / 100 mm = 10.24 points/mm A more common representation uses the imperial notation, dots per inch (dpi): 1024 px / 3.94 in = ~260 dpi It is not correct. A printer cannot print any number of points per millimeter. It has a resolution of its own, just like a display device has. The calculated value is a bit less than what a typical 300 dpi resolution laser printer is capable of. If we wish to use the printer's maximum resolution to reach the best possible print quality, we must satisfy with smaller image size than 100 mm x 100 mm: 1024 px / 300 dpi = 3.41 in ~ 87 mm 1024 x 1024 pixel image (left) printed from Photoshop in 300 dpi with a laser printer (right) is about 87 x 87 mm on paper. If we decide to print the image in 100 x 100 mm size, then the printer's driver must scale up our image to produce content for the \"missing\" pixels (actually dots). This is similar to what a web browser does when it stretches an image to a larger size on display by copying/interpolating pixels. 300 dpi * 3.94 in = 1182 px 1182 px - 1024 px = +152 px In reality, a laser printer users a rasterization algorithm to produce gray colors. It does this by alternating empty space between black printed dots. Hence, there will not be a 1:1 mapping from pixels to dots when you print a photograph. You can see this in action in the printed image above by looking at the grayscale squares. Nowadays many laser printers have 600 dpi or higher resolution. Inkjet printers often claim much higher dpi values. This is most aggressive marketing, but they also measure dpi as the number of drops of ink per inch. Moreover, they use algorithms that are supposed to improve print quality via optimization. Furthermore, in an image file the pixels are next to each other but on paper, the drops of ink have space between them. All in all, it is not straightforward to understand what is the actual mapping of image pixels to dots on paper. One rule of thumb is that most book publishers require images in 300 ppi resolution. Hence, if you want to print an image in 100 x 100 mm size on a book page, your image file should be at least 1182 x 1182 pixels. 300 dpi * 3.94 in = 1182 px","title":"Print resolution"},{"location":"articles/resolution/#resolution-in-360-degree-images","text":"","title":"Resolution in 360-degree images"},{"location":"articles/resolution/#equirectangular-projection","text":"Now we will move on to discuss image resolution in 360-degree images. Let us begin by creating a wide-angle version of our test image. We will use our original test image as a single cube face and make five copies of it to produce the six faces of a cube. Then we will put a virtual camera to its center point and render a spherical 360-degree image. To save that 3D image into a flat PNG file, we must choose a projection . It must be one of those that are designed for projecting a sphere into a planar surface. (This is similar to how we project a map of our dear planet Earth to a piece of paper.) Here we will use the most common panorama projection: the equirectangular. The result looks like this: 360-degree image rendered from inside a test cube using equirectangular projection. Take a moment to examine the image above and find the four cube faces at the horizon level (front, right, back, left) as well as the cube's top and bottom faces (they are heavily distorted). The image covers 360 degrees horizontally: at the center of the cube, our virtual camera has turned around full 360 degrees along the yaw angle. As a consequence, left and right edges of the image match with each other seamlessly. The image also covers 180 degrees vertically: at the center of the cube, our virtual camera has turned around 180 degrees along the pitch angle. The top row of pixels are all the same and they illustrate what is exactly above the camera (zenith). The bottom row of pixels are also all the same and they illustrate what is exactly below the camera (natural direction, nadir). Thus, this is a full spherical 360x180 degree image and it covers every possible viewing direction that can be seen from the center of the cube. Of course, the described camera rotation is just a convention. It could have turned 180 degrees horizontally and 360 degrees vertically to produce a spherical image with a 1:2 aspect ratio. Also, the rotation angles do not need to align with the main axis at all to produce a spherical image, they just need to be perpendicular to each other. Notice that a spherical image is NOT 360x360 degrees; that is mathematically incorrect representation although it is sometimes seen in marketing materials. A 720-degree image is even more wrong! To see how a video player projects equirectangular video frames on screen, play the video below. Try interacting with it by panning. It is easier if you first select the fullscreen mode. While playing the video you may notice that image clarity suddenly changes. This is adaptive streaming in action: the player measures network bandwidth during playback and changes to the maximum resolution that bandwidth allows.","title":"Equirectangular projection"},{"location":"articles/resolution/#aspect-ratio","text":"Equirectangular projection looks perhaps a bit weird at first sight. To understand it better, let us remind ourselves how the map of the World looks like in this projection. You have probably seen it before: An example of equirectangular projection by Wikipedia ( 5 ) In our previous example, we started from a 3D cube . We saw how straight lines got bent when the equirectangular projection was applied. This time we start from a 3D sphere (the Earth). Notice how the meridians are projected as vertical straight lines of constant spacing . And circles of latitude as horizontal straight lines of constant spacing . The equirectangular projection has many fallacies. It is not an equal area projection (see how large the Antarctic appears). It is not conformal (angles are not preserved). It is wasteful (lots of redundant pixels). Yet, the equirectangular projection is the de-facto standard in 360-degree imaging. This is probably related to the particularly simple relationship between the position of a pixel in the image and corresponding point on the surface of a sphere. It makes the life of software developers easy. Consider a full spherical equirectangular image whose resolution is 1024 x 512 pixels. (The map above will do.) Choose any pixel in the image. Now, if you want to move 30 degrees east, you can calculate this in pixels as shown below. To move in a vertical direction, you use 180\u00b0 degrees instead of 360\u00b0. If you pass the edge of the image, continue in the same direction from the opposite edge of the image. That's it. Simple linear relationship. It may not be obvious, but other projections are considerably more complex to handle. 1024 px * (30\u00b0/360\u00b0) = 85 px 2D images have a property called aspect ratio . It describes the proportional relationship between its width and its height. For example, a square has aspect ratio 1:1 because width and height are the same. An image whose resolution is 1920 x 1080 pixels has aspect ratio 16:9 or 1.77:1. 1920 x 1080 = (120 * 16) x (120 * 9) 16 : 9 = 1.77 In the World map image, horizontal and vertical lines appear at 15-degree intervals. Count the lines. You will notice that there is twice the number of vertical lines than horizontal lines. We can make a conclusion that proper aspect ratio for a full spherical equirectangular image must be 2:1 . This is because horizontally there are twice the amount of degrees than vertically (360 x 180). Also, we want to treat horizontal and vertical degrees equally ie. use the same amount of pixels per degree. 2:1 aspect ratio is a bit different than what we have used to, but not that far off from common video formats: Aspect ratio Normalized ratio Typical use 4:3 1.33:1 Traditional TV 16:9 1.78:1 HD video 1.85:1 1.85:1 Most movies 2:1 2.00:1 Equirectangular panoramas 2.39:1 2.39:1 Widescreen movies When you deliver 360-degree images or videos using the equirectangular projection, use 2:1 aspect ratio. Image width should be two times its height. For example, 1920 x 960 has proper aspect ratio 2:1. Commonly used FullHD resolution 1920 x 1080 is 16:9. It is taller . With that resolution, your image has more capacity for detail vertically than horizontally. 360-degree video players can handle that, but from the content point of view it makes little sense. Likewise, if you aim for 4K quality use 3840x1920 (2:1), not 3840x2160 (16:9). Left: 360x180 equirectangular image in correct aspect ratio 2:1. Right: The same image in incorrect 16:9 aspect ratio. You may wonder what harm is in using one of 16:9 resolutions. The answer is: not much. Consider a case where you need to downscale a video from 6K camera to 4K output. With 3840x2160 (16:9) you have better panorama resolution vertically than horizontally. You can do that, but why would you? Often this happens accidentally when you transcode a video into a different format. The default values in video software tend to lead to a 16:9 aspect ratio. If you are not careful, your 3840x1920 (2:1) video may become 3840x2160 (16:9). In such a case, you will add redundant data and create a larger file size without any benefit.","title":"Aspect ratio"},{"location":"articles/resolution/#field-of-view","text":"We often refer to full spherical panorama images as 360x180 degree image. Or shortly, 360-degree image. What exactly does this value describe? The correct term is field-of-view or shortly FOV. Field-of-view is simply the size of an observable area, described in degrees ( 8 ). You have probably used a camera that has a zoom lens. When you utilize the zoom feature, you are effectively changing the field-of-view. When you zoom in , objects appear closer but the view becomes narrower. Hence, you have smaller field-of-view. When you zoom out , you see wider area through the viewfinder. You have larger field-of-view. Horizontal, vertical, and diagonal field-of-view illustraded by Wikipedia ( 8 ) We must be careful with field-of-view values, though. It can be horizontal, vertical, or diagonal measure of a rectangular viewable area. Unfortunately, it is not always mentioned which of these the given number is referring to. If you know one value and the aspect ratio, you can calculate the others using simple trigonometry. Abbreviation Meaning FOV Field-of-view in general (ambiguous) HFOV Horizontal field-of-view VFOV Vertical field-of-view DFOV Diagonal field-of-view When we view 360-degree content, there are effectively two different FOV values that play a part. Image field-of-view refers to the total observable area in the whole image frame. For example, in a spherical panorama image HFOV is 360 degrees and VFOV is 180 degrees. Diagonal FOV is not used. Viewport field-of-view is the part that is visible in the panorama image viewer or video player. For example, a pair of VR glasses could have HFOV 94 degrees. In many players this value is adjustable: user can zoom by changing the viewport's FOV.","title":"Field-of-view"},{"location":"articles/resolution/#panorama-resolution","text":"Now, scroll back up and take a closer look at our equirectangular image of a cube. If you observe it carefully, you will notice that this image does not look as sharp as our original cube face. They are both 1024 pixels wide images and also scaled to the same width on the screen. Hence, both image resolution and screen resolution are exactly the same (horizontally). What is different? Recall that we added 5 more cube faces - much more information - to this new image. Yet we didn't increase image resolution ie. the amount of detail the image can hold. Major error. Besides, we changed the projection and halved the vertical resolution from 1024 px to 512 px. How can we measure the amount of detail in panoramic images, where field-of-view (the observable area) varies? As shown above, image resolution and screen resolution do not take this into account. We need something else. Consider that four cube faces (front, right, back, left) cover 360-degree spin at the horizon level. This means that one cube face must cover 90 degrees field-of-view (360/4=90). The width of our equirectangular image of a cube is 1024 pixels and its height is 512 pixels (2:1 aspect ratio). Thus, we can calculate its resolution per degree as follows and express it in pixels per degree (ppd): 1024 px / 360\u00b0 = ~2.84 ppd 512 px / 180\u00b0 = ~2.84 ppd To compare, our original cube covers only 90 degrees and has a higher resolution per degree: 1024 px / 90\u00b0 = ~11.38 ppd How can we make our 360-degree cube image carry as much detail as the original 90-degree cube face? Let us focus on the horizon level where the projection does not distort the image much. We need 4x the resolution that we have now: 360\u00b0 * 11.38 ppd = ~4096 px What if we made a 180-degree version, assuming viewers will focus to the front direction? How much resolution do we need to maintain detail? Simple: 180\u00b0 * 11.38 ppd = ~2048 px Resolution per degree appears to be a useful method for expressing the amount of detail in panoramic images. The benefit is that values are comparable despite used field-of-view. In this article, we will use the term panorama resolution to reference it. Notice that there are projections that do not provide constant resolution per degree throughout the image. To be specific, we probably should use the term equirectangular resolution . Here we use the easier and shorter term because equirectangular projection is so widely used. We have also learned that 360-degree images need much higher image resolution than ordinary 2D images to \"look the same quality\". This is often hard to understand for end users. People have a mental model of what \"FullHD\" or \"4K\" content is supposed to look like. But through a 360-degree player, you are likely seeing about 1/8th of the total amount of pixels at any moment of time! Let us rephrase the primary concepts of resolution for 360-degree images: Image resolution = total amount of detail in an image file / in a decoded video frame, in pixels Image field-of-view = total size of an observable area captured into an image, in degrees Panorama resolution = amount of detail per degree, in ppd Screen resolution = viewport size on screen, in pixels Viewport field-of-view = size of an observable area through a viewport, in degrees","title":"Panorama resolution"},{"location":"articles/resolution/#towards-perfect-resolution","text":"","title":"Towards perfect resolution"},{"location":"articles/resolution/#pixel-perfect","text":"","title":"Pixel perfect"},{"location":"articles/resolution/#retina-resolution","text":"In order to make an image look sharp on screen, both image resolution and screen resolution must be high enough: you need enough pixels to store the details and enough pixels to draw them on screen so that they remain distinguishable. We have already said that it is best if they match perfectly: you can avoid aliasing errors that come from scaling if your image has exactly the same amount of pixels in file and on screen. You will also preserve all the detail without wasting memory for something that cannot be seen by user. A well known example is matching image resolutions of iOS app icons and button graphics with their screen resolutions so that they will look \"perfect\" as no scaling artifacts will occur. Nowadays it is becoming less common to aim for pixel perfect presentation as there are so many different aspect ratios and screen resolutions that need to be supported. For example, on Android the approach is different: to support thousands of different device models developers cannot make large amount of variants of each app icon; instead they are supposed to use vector graphics and let the device rasterize a pixel perfect copy at runtime. This approach of course does not work with images that have been captured with a camera and are raster images by nature. The solution is simple: use images whose pixel resolution matches with the highest screen resolution that will be needed. The operating system will downscale the images to lower resolutions on other device models. You can also choose to provide a fistful of different sizes and let the system select the nearest match. It is not perfect solution but good enough. It is also worth to realize that the concept of pixel perfect imaging mainly applies to traditional 2D images; pixel-to-pixel matching is not possible with spherical 360-degree images as they need to be projected from a spherical (curved) surface to a flat surface in order to be stored in common image and video formats, then projected back to spherical (curved) surface for 3D presentation within the image viewer / video player application, and once again to flat display surface when it is time to render part of the 360-degree image on screen. Try to preserve pixel-to-pixel matching through those operations! This leads us to a question: what would be the equivalent of pixel perfect presentation in 360-degree images? Since we cannot know all the internal processing that occurs when a specific pipeline renders our image on screen, in practice we just need an image that has enough resolution so that all the processing will not become visible in the end result. Then, what resolution is needed to make a 360-degree image look \"perfect\"? There is no single answer, as it depends on many parameters such as display resolution, used field-of-view, viewing distance, and projection. Let's discuss a few selected cases. When Apple introduced iPhone4, they claimed that their new Retina displays have high enough pixel density that the human eye cannot notice pixelation at a typical viewing distance. In other words, the display would appear to look perfect (when it comes to resolution) since you are unable to see the small dots that the image is made of. According to Apple the spatial resolution required for this is about 300 ppi for a device held 10-12 inches from the eye . In reality, a typical iPad has 9.7\" inch screen size (diagonally), 264 ppi display panel and 2048 x 1536 resolution. Now, if we want to view a 360-degree photograph from an iPad's screen that we will hold 10 inches from the eye, what would be the minimum pixel resolution for our 360-degree image so that it would provide optimal quality ie. have enough pixels for that display when we turn full 360 degrees around? Let's do some math: Consider a circle that aligns with the horizon around a spectators head. This is the path the iPad travels when we hold it in front of our eyes and turn around 360 degrees to see the complete 360-degree image. To simplify matters, we will assume a single eye at the center of the circle. The radius of the circle is 10 inches (25.4 cm) and its circumference is: 2 * PI * 10 in = 62.8 in ~160 cm Let us imagine that instead of moving a real iPad along this circular path, Apple would produce a round cylindrical display and we would simply go stand at the center of it. Our imaginary round iPad retina display would be 62.8 inches \"wide\" (circumference). Then, how many pixels would it have (horizontally)? 62.8 in * 264 ppi = ~16588 px If our imaginary display would be spherical , its resolution would be 16588 x 8294 pixels (because of aspect ratio 2:1). That would be a big display with almost 140 megapixels and not practical at all (how would you even get inside?) Of course, in reality we will move an ordinary tablet along a circular path, detect the movement with sensors, and update image content on screen. 16588 x 8294 = 137580872 ~ 138 Mpx Yet from content creation point of view our target is that imaginary spherical display. Creating content for such a display is not impossible at all: we can easily create much, much larger 360-degree photographs, and 8K 360-degree video is becoming commonplace - when we double that to 16K a few years later, we have achieved retina resolution for iPads. Neat. What if our 360-degree image is viewed through VR glasses? It is difficult to say what would be the correct distance for the radius of the circle that we used in the previous analysis: the display itself is really close to the eye, so the radius appears to be a small value. But in reality the image is viewed through a lens, and that must be taken into account. The focal distances in different headsets is usually not mentioned in the product specifications. Fortunately, there is a nice shortcut. Let us consider Oculus Go, whose display resolution is 2560 x 1440 in total, and 1280 x 1440 per eye. The horizontal field-of-view that a single eye covers in Oculus Go is said to be about the same as in Oculus Rift, where it was 94 degrees. Therefore, a 360-degree panorama image for Oculus Go should have at least 4902 x 2451 resolution or about 12 megapixels. This can be easily satisfied already by filming 360-degree photos and videos in 8K (or 6K) and providing final output in about 5K. 1280 px / 94\u00b0 = 13.61 px/\u00b0 360\u00b0 * 13.61 px/\u00b0 = 4902 px 4902 px / 2 = 2451 px 4902 px * 2451 px = ~12015115 px ~ 12.0 Mpx","title":"Retina resolution"},{"location":"articles/resolution/#human-eye-resolution","text":"However, Oculus Go is a nice headset but far from the concept of a retina display. If we want to future-proof our content and target for VR headsets that have \"retina displays\", what resolution do we need? This is the same question as what is the resolution of a human eye itself, how many megapixels? Human eye illustration by Wikipedia ( 6 ) According to ( 7 ), let us consider a view in front of a spectator that is 90 degrees by 90 degrees. The number of pixels a human eye could see through such window is ~324 Mpx. Remember our cube face that was also 90 by 90 degrees? To have full 360-degree view we need 6 cube faces, resulting to 6 * 324 Mpx = 1944 Mpx or 1.9 Gpx. If we wanted to use equirectangular projection instead, we will find that if square contains 324 Mpx then its side length is 18000 px and at horizon line we need 4 x 90 degrees for a 360-degree images, therefore 4 x 18000 px = 72000 px. Aspect ratio 2:1 yields 72000 x 36000 px and in total 2592 Mpx or 2.6 Gpx. Why do we need so much more pixels with equirectangular projection, compared to a cube map? This is because equirectangular projection is wasteful: there are multiple copies of same pixels ie. pixels that provide no new information. Cubemap projection requires 25% less pixels to produce same output. The interesting point is that about 2-3 Gpx image (72000 x 36000 px) should be enough for human eye, now and in the future. Hey, wait a minute? What about Apple's retina displays - we already calculated that 138 Mpx should be enough, right? Well, it is kind of apples and oranges case: The claim about retina displays is that at the typical viewing distance of 10-12 inches user is not suppose to see individual pixels, which is not exactly the same thing as how much details a human eye can see in best conditions. Also, ( 7 ) adds an interesting scientific point of view - apparently humans can see more details than our retina alone is able to distinguish: The eye is not a single frame snapshot camera. It is more like a video stream. The eye moves rapidly in small angular amounts and continually updates the image in one's brain to \"paint\" the detail. We also have two eyes, and our brains combine the signals to increase the resolution further. We also typically move our eyes around the scene to gather more information. Because of these factors, the eye plus brain assembles a higher resolution image than possible with the number of photoreceptors in the retina. So the megapixel equivalent numbers below refer to the spatial detail in an image that would be required to show what the human eye could see when you view a scene.","title":"Human eye resolution"},{"location":"articles/video_encoding/","text":"Download and install HandBrake . HandBrake is a Free and open source tool for converting video from nearly any format to a selection of modern, widely supported codecs. Launch the app File - Open - choose the video you want to encode From the tabs select Dimensions Select resolution from Storage Size (Make sure the values on Storage Size are the same as Display Size) Anamorpic: Always keep it off Cropping: Make sure all values are 0 (choose custom and manually edit the values to 0 if needed) From the tabs select Video Video Encoder: H.264 (x264) is what supported by most iPads Framerate (FPS): 30 Press Start","title":"Video encoding"},{"location":"downloads/downloads/","text":"Downloads \u00b6 Here you will find an archive of downloadable files. Please read the license below before using them. Content Samples \u00b6 Demo Photo \u00b6 Photo Copyright (c) Tapani Rantakokko This photo was shot with Canon 6D DSLR camera through a 10mm fisheye lens, and stitched with PtGui. The same photo is bundled with the app (\"Olos\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 photo, equirectangular projection, 24-bit JPG Download 4096x2048 (2.6 MB) Download 8192x4096 (9.2 MB) Demo Video \u00b6 Video Copyright (c) Juha Kela This video was shot with 6x GoPro Hero cameras, and stitched with Kolor AVP. The same video is bundled with the app (\"PowerPark\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 video, equirectangular projection, MP4 Download 1920x1080 25fps 55sec h264 (81.5 MB) Download 3840x2160 25fps 55sec h264 (155.7 MB) Test Image \u00b6 Image Copyright (c) Finwe Ltd. This image was drawn with Inkscape (6x cube walls) and stitched with PtGui. Full spherical 360x180\u00b0 image, equirectangular projection, 24-bit JPG Download 2048x1024 (2.2 MB) Download 4096x2048 (7.7 MB) Download 8192x4096 (22.9 MB) Test Video \u00b6 Video Copyright (c) Finwe Ltd. This video was first drawn with Inkscape (6x cube walls) and stitched with PtGui, then encoded to video with ffmpeg. Download 1920x960 30fps 30sec h264 (2.6 MB) Download 1920x960 60fps 30sec h264 (4.5 MB) Download 3840x1920 30fps 30sec h264 (8.3 MB) Download 3840x1920 60fps 30sec h264 (14.5 MB) Stereo (3D) Test Video \u00b6 Video Copyright (c) Finwe Ltd. This image was first drawn with Blender and rendered to stereo (3D) image, then encoded to video with ffmpeg. Notice that top half contains left eye image and bottom half right eye image. Use a VR headset for viewing this video in stereo; on phone/tablet only left eye image is shown. This video was encoded with more efficient h265 encoder. Not all devices support it. Download 3D 3840x1920 30fps 30sec h265 (3.4 MB) Download 3D 3840x3840 30fps 30sec h265 (8.3 MB) Templates \u00b6 settings.ini \u00b6 settings.ini is an optional configuration file that you can copy to the LiveSYNC root folder (same location where content files are copied to). The file contains many configuration options in a user-friendly windows ini-file format. Each setting is described with a comment. Download latest version exampleProject.json \u00b6 This JSON is autogenerated by LiveSYNC and saved in the directory when a user saves a project after adding hotspot icon/s. License \u00b6 Creative Commons \u00b6 This work is licensed under a Creative Commons Attribution-Share Alike 3.0 Unported License","title":"Index"},{"location":"downloads/downloads/#downloads","text":"Here you will find an archive of downloadable files. Please read the license below before using them.","title":"Downloads"},{"location":"downloads/downloads/#content-samples","text":"","title":"Content Samples"},{"location":"downloads/downloads/#demo-photo","text":"Photo Copyright (c) Tapani Rantakokko This photo was shot with Canon 6D DSLR camera through a 10mm fisheye lens, and stitched with PtGui. The same photo is bundled with the app (\"Olos\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 photo, equirectangular projection, 24-bit JPG Download 4096x2048 (2.6 MB) Download 8192x4096 (9.2 MB)","title":"Demo Photo"},{"location":"downloads/downloads/#demo-video","text":"Video Copyright (c) Juha Kela This video was shot with 6x GoPro Hero cameras, and stitched with Kolor AVP. The same video is bundled with the app (\"PowerPark\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 video, equirectangular projection, MP4 Download 1920x1080 25fps 55sec h264 (81.5 MB) Download 3840x2160 25fps 55sec h264 (155.7 MB)","title":"Demo Video"},{"location":"downloads/downloads/#test-image","text":"Image Copyright (c) Finwe Ltd. This image was drawn with Inkscape (6x cube walls) and stitched with PtGui. Full spherical 360x180\u00b0 image, equirectangular projection, 24-bit JPG Download 2048x1024 (2.2 MB) Download 4096x2048 (7.7 MB) Download 8192x4096 (22.9 MB)","title":"Test Image"},{"location":"downloads/downloads/#test-video","text":"Video Copyright (c) Finwe Ltd. This video was first drawn with Inkscape (6x cube walls) and stitched with PtGui, then encoded to video with ffmpeg. Download 1920x960 30fps 30sec h264 (2.6 MB) Download 1920x960 60fps 30sec h264 (4.5 MB) Download 3840x1920 30fps 30sec h264 (8.3 MB) Download 3840x1920 60fps 30sec h264 (14.5 MB)","title":"Test Video"},{"location":"downloads/downloads/#stereo-3d-test-video","text":"Video Copyright (c) Finwe Ltd. This image was first drawn with Blender and rendered to stereo (3D) image, then encoded to video with ffmpeg. Notice that top half contains left eye image and bottom half right eye image. Use a VR headset for viewing this video in stereo; on phone/tablet only left eye image is shown. This video was encoded with more efficient h265 encoder. Not all devices support it. Download 3D 3840x1920 30fps 30sec h265 (3.4 MB) Download 3D 3840x3840 30fps 30sec h265 (8.3 MB)","title":"Stereo (3D) Test Video"},{"location":"downloads/downloads/#templates","text":"","title":"Templates"},{"location":"downloads/downloads/#settingsini","text":"settings.ini is an optional configuration file that you can copy to the LiveSYNC root folder (same location where content files are copied to). The file contains many configuration options in a user-friendly windows ini-file format. Each setting is described with a comment. Download latest version","title":"settings.ini"},{"location":"downloads/downloads/#exampleprojectjson","text":"This JSON is autogenerated by LiveSYNC and saved in the directory when a user saves a project after adding hotspot icon/s.","title":"exampleProject.json"},{"location":"downloads/downloads/#license","text":"","title":"License"},{"location":"downloads/downloads/#creative-commons","text":"This work is licensed under a Creative Commons Attribution-Share Alike 3.0 Unported License","title":"Creative Commons"},{"location":"faq/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 This is a collection of frequently asked questions, answered by our experts. If you cannot find an answer to your question, please contact us via the form on the support page . TIP: Try the search feature in the top right corner! Hardware \u00b6 What iOS devices are supported? \u00b6 LiveSYNC can be installed on iPhones and iPads, but currently not on AppleTV. The app requires iOS 8.0 or later. While LiveSYNC works fairly well also on iOS devices that are several years old, some feature limitations exist: Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your iOS device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your iPad. I need to buy an iPad to be used as the director device. Which model do you recommend? \u00b6 In general, a new standard size (9.7\") iPad is a good value for money. You can also choose Pro version, but it is not necessary unless you want a larger size model (11\" or 12.9\"). What Android devices are supported? \u00b6 LiveSYNC can be installed on Android phones and tablets, but currently not on AndroidTV. The app requires Android 5.0 (API level 21) or later. While LiveSYNC works fairly well also on Android devices that are several years old, some feature limitations exist: Director mode with Bluetooth communication may not work on some old devices that have been upgraded to Android 5.0, due to Bluetooth chipset that does not support all required features. Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your Android device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your tablet. I need to buy an Android tablet to be used as the director device. Which model do you recommend? \u00b6 Just to name one, we've had good experience with Samsung Galaxy Tab S2 or newer. However, there are plenty of options. Installing \u00b6 Can I install LiveSYNC on my iPhone or iPad? \u00b6 Yes. On your iOS device, open App Store , tap Search , and type 'livesync'. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Can I install LiveSYNC on my Android phone or tablet? \u00b6 Yes. On your Android device, open Play Store , and type 'livesync' to the search field. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Can I install LiveSYNC on my Windows phone or tablet? \u00b6 No. Windows is currently not supported. Can I install LiveSYNC on my Windows/Mac/Linux computer? \u00b6 No. Desktop computer operating systems are currently not supported. Can I install LiveSYNC on my GearVR/Oculus Go headset? \u00b6 Yes, but currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus device, navigate to Oculus Store and type 'livesync' to the search field. Select 'LiveSYNC Oculus Go'. If you cannot find it, don't forget to check also the 'Not installed' tab. The app can be installed for FREE. Can I install LiveSYNC on my Oculus Rift/HTC Vive headset? \u00b6 No. LiveSYNC is currently available for standalone VR headsets, not for devices that depend on a desktop/laptop computer. I have a Cardboard (or compatible) VR headset. Can I use that with LiveSYNC? \u00b6 Yes. Both iOS and Android versions have built-in support for VR mode on phones (disabled in tablets). During channel configuration select 'VR' for viewing mode, or during playback long tap the screen to toggle between normal and VR mode. If you have a GearVR headset, please use our separate Oculus Go version from Oculus Store instead (invite only). The rendering quality and head tracking are far superior! Beta versions \u00b6 Can I test new features before they are released by installing a beta version? \u00b6 Yes. Please follow the instructions below: Android: We have an open beta channel in Google Play. Anyone can join simply by opting-in here and then installing the beta version by following instructions on that page. iOS: We have a closed beta channel in Apple TestFlight. If you wish to participate, please use the contact form in the support page for requesting an invite. GearVR/Oculus Go: We have a closed beta channel in Oculus Store. If you wish to participate, please use the contact form in the support page for requesting an invite. Notice: We need your Oculus username/email for sending the invite. Can I install BOTH the beta version and the store version into the same device? \u00b6 No. All the stores allow having only one version installed at a time. In general this will be the newest version ie. the one with the highest version number. Configuration \u00b6 Can I create multiple channel configurations, and swap between them during my presentation? \u00b6 Yes, sort of. Each channel has a channel number, and only those audience devices that are listening on the same channel that you are on will respond to your commands. You can control device group 1 first on channel 1234, and then move on to control device group 2 on channel 5678, for instance. However, when you leave a presentation to switch to another one, the audience devices that you were previously controlling will lose the connection and return to the lobby. Hence, you cannot have more than one presentation ongoing simultaneously with only one controller device. Content \u00b6 What file formats are supported? \u00b6 In general, LiveSYNC recognizes .mp4 video files, .jpg image files, and .m3u8 HLS streams. For custom hotspot images please use 32-bit .png with alpha. Notice that for video content the support depends on used hardware (HW decoder), .mp4 is just a container. A good rule of thumb is: if it plays with your device's own media player, it likely works with LiveSYNC, too. Where should I put my content files so that LiveSYNC can pick them up? \u00b6 On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start) Can I use an SD card on my Android device for storing content files? \u00b6 Yes. You need to manually create /Movies/LiveSYNC folder on your SD card and copy the files there. LiveSYNC will treat this folder on SD card similar to /Movies/LiveSYNC folder in internal storage, except that if same filename appears in both then internal memory takes precedence. Can I use folders to organize my content files? \u00b6 Yes, but only one level: \\LiveSYNC\\Summer is OK but \\LiveSYNC\\Summer\\June is not. Of course you can have as many subfolders directly under LiveSYNC root folder, for example \\LiveSYNC\\January, \\LiveSYNC\\February, etc. On iOS devices, create the folders on your PC/Mac and then drag the folder to iTunes. Notice that it is not possible to view the contents of already copied folder via iTunes - unless you drag the folder back from iTunes, for example, to your desktop. On Android devices, you can use the same method OR create the folders directly to your device and then copy there individual files. You can also view the contents of the folders via file manager.","title":"Index"},{"location":"faq/faq/#frequently-asked-questions-faq","text":"This is a collection of frequently asked questions, answered by our experts. If you cannot find an answer to your question, please contact us via the form on the support page . TIP: Try the search feature in the top right corner!","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/faq/#hardware","text":"","title":"Hardware"},{"location":"faq/faq/#what-ios-devices-are-supported","text":"LiveSYNC can be installed on iPhones and iPads, but currently not on AppleTV. The app requires iOS 8.0 or later. While LiveSYNC works fairly well also on iOS devices that are several years old, some feature limitations exist: Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your iOS device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your iPad.","title":"What iOS devices are supported?"},{"location":"faq/faq/#i-need-to-buy-an-ipad-to-be-used-as-the-director-device-which-model-do-you-recommend","text":"In general, a new standard size (9.7\") iPad is a good value for money. You can also choose Pro version, but it is not necessary unless you want a larger size model (11\" or 12.9\").","title":"I need to buy an iPad to be used as the director device. Which model do you recommend?"},{"location":"faq/faq/#what-android-devices-are-supported","text":"LiveSYNC can be installed on Android phones and tablets, but currently not on AndroidTV. The app requires Android 5.0 (API level 21) or later. While LiveSYNC works fairly well also on Android devices that are several years old, some feature limitations exist: Director mode with Bluetooth communication may not work on some old devices that have been upgraded to Android 5.0, due to Bluetooth chipset that does not support all required features. Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your Android device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your tablet.","title":"What Android devices are supported?"},{"location":"faq/faq/#i-need-to-buy-an-android-tablet-to-be-used-as-the-director-device-which-model-do-you-recommend","text":"Just to name one, we've had good experience with Samsung Galaxy Tab S2 or newer. However, there are plenty of options.","title":"I need to buy an Android tablet to be used as the director device. Which model do you recommend?"},{"location":"faq/faq/#installing","text":"","title":"Installing"},{"location":"faq/faq/#can-i-install-livesync-on-my-iphone-or-ipad","text":"Yes. On your iOS device, open App Store , tap Search , and type 'livesync'. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Can I install LiveSYNC on my iPhone or iPad?"},{"location":"faq/faq/#can-i-install-livesync-on-my-android-phone-or-tablet","text":"Yes. On your Android device, open Play Store , and type 'livesync' to the search field. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Can I install LiveSYNC on my Android phone or tablet?"},{"location":"faq/faq/#can-i-install-livesync-on-my-windows-phone-or-tablet","text":"No. Windows is currently not supported.","title":"Can I install LiveSYNC on my Windows phone or tablet?"},{"location":"faq/faq/#can-i-install-livesync-on-my-windowsmaclinux-computer","text":"No. Desktop computer operating systems are currently not supported.","title":"Can I install LiveSYNC on my Windows/Mac/Linux computer?"},{"location":"faq/faq/#can-i-install-livesync-on-my-gearvroculus-go-headset","text":"Yes, but currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus device, navigate to Oculus Store and type 'livesync' to the search field. Select 'LiveSYNC Oculus Go'. If you cannot find it, don't forget to check also the 'Not installed' tab. The app can be installed for FREE.","title":"Can I install LiveSYNC on my GearVR/Oculus Go headset?"},{"location":"faq/faq/#can-i-install-livesync-on-my-oculus-rifthtc-vive-headset","text":"No. LiveSYNC is currently available for standalone VR headsets, not for devices that depend on a desktop/laptop computer.","title":"Can I install LiveSYNC on my Oculus Rift/HTC Vive headset?"},{"location":"faq/faq/#i-have-a-cardboard-or-compatible-vr-headset-can-i-use-that-with-livesync","text":"Yes. Both iOS and Android versions have built-in support for VR mode on phones (disabled in tablets). During channel configuration select 'VR' for viewing mode, or during playback long tap the screen to toggle between normal and VR mode. If you have a GearVR headset, please use our separate Oculus Go version from Oculus Store instead (invite only). The rendering quality and head tracking are far superior!","title":"I have a Cardboard (or compatible) VR headset. Can I use that with LiveSYNC?"},{"location":"faq/faq/#beta-versions","text":"","title":"Beta versions"},{"location":"faq/faq/#can-i-test-new-features-before-they-are-released-by-installing-a-beta-version","text":"Yes. Please follow the instructions below: Android: We have an open beta channel in Google Play. Anyone can join simply by opting-in here and then installing the beta version by following instructions on that page. iOS: We have a closed beta channel in Apple TestFlight. If you wish to participate, please use the contact form in the support page for requesting an invite. GearVR/Oculus Go: We have a closed beta channel in Oculus Store. If you wish to participate, please use the contact form in the support page for requesting an invite. Notice: We need your Oculus username/email for sending the invite.","title":"Can I test new features before they are released by installing a beta version?"},{"location":"faq/faq/#can-i-install-both-the-beta-version-and-the-store-version-into-the-same-device","text":"No. All the stores allow having only one version installed at a time. In general this will be the newest version ie. the one with the highest version number.","title":"Can I install BOTH the beta version and the store version into the same device?"},{"location":"faq/faq/#configuration","text":"","title":"Configuration"},{"location":"faq/faq/#can-i-create-multiple-channel-configurations-and-swap-between-them-during-my-presentation","text":"Yes, sort of. Each channel has a channel number, and only those audience devices that are listening on the same channel that you are on will respond to your commands. You can control device group 1 first on channel 1234, and then move on to control device group 2 on channel 5678, for instance. However, when you leave a presentation to switch to another one, the audience devices that you were previously controlling will lose the connection and return to the lobby. Hence, you cannot have more than one presentation ongoing simultaneously with only one controller device.","title":"Can I create multiple channel configurations, and swap between them during my presentation?"},{"location":"faq/faq/#content","text":"","title":"Content"},{"location":"faq/faq/#what-file-formats-are-supported","text":"In general, LiveSYNC recognizes .mp4 video files, .jpg image files, and .m3u8 HLS streams. For custom hotspot images please use 32-bit .png with alpha. Notice that for video content the support depends on used hardware (HW decoder), .mp4 is just a container. A good rule of thumb is: if it plays with your device's own media player, it likely works with LiveSYNC, too.","title":"What file formats are supported?"},{"location":"faq/faq/#where-should-i-put-my-content-files-so-that-livesync-can-pick-them-up","text":"On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start)","title":"Where should I put my content files so that LiveSYNC can pick them up?"},{"location":"faq/faq/#can-i-use-an-sd-card-on-my-android-device-for-storing-content-files","text":"Yes. You need to manually create /Movies/LiveSYNC folder on your SD card and copy the files there. LiveSYNC will treat this folder on SD card similar to /Movies/LiveSYNC folder in internal storage, except that if same filename appears in both then internal memory takes precedence.","title":"Can I use an SD card on my Android device for storing content files?"},{"location":"faq/faq/#can-i-use-folders-to-organize-my-content-files","text":"Yes, but only one level: \\LiveSYNC\\Summer is OK but \\LiveSYNC\\Summer\\June is not. Of course you can have as many subfolders directly under LiveSYNC root folder, for example \\LiveSYNC\\January, \\LiveSYNC\\February, etc. On iOS devices, create the folders on your PC/Mac and then drag the folder to iTunes. Notice that it is not possible to view the contents of already copied folder via iTunes - unless you drag the folder back from iTunes, for example, to your desktop. On Android devices, you can use the same method OR create the folders directly to your device and then copy there individual files. You can also view the contents of the folders via file manager.","title":"Can I use folders to organize my content files?"},{"location":"quick_start/presentation_channel/","text":"Quick start: Presentation \u00b6 Channel \u00b6 Follow these steps to start a presentation on the device configured to Director Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Director Mode and starts from the Mosaic tab Wait here until audience devices have joined (their views will appear on your screen) Note In case some viewer devices arrive late (during a presentation) they can still join the presentation.","title":"Channel"},{"location":"quick_start/presentation_channel/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_channel/#channel","text":"Follow these steps to start a presentation on the device configured to Director Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Director Mode and starts from the Mosaic tab Wait here until audience devices have joined (their views will appear on your screen) Note In case some viewer devices arrive late (during a presentation) they can still join the presentation.","title":"Channel"},{"location":"quick_start/presentation_configuration/","text":"Quick start: Presentation \u00b6 Configuration \u00b6 Director Mode \u00b6 To configure a device for the Director Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Director) Type device name or use the offered default name Select connection type Write down the given channel number Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Audience Mode \u00b6 To configure a device for the Audience Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Audience) Type device name or use the offered default name Select connection type Type the channel number (the one you wrote down earlier) If you are setting a phone, select normal or VR mode (not asked on tablets) Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Note On GearVR/Oculus Go Director Mode is not available and Audience Mode configuration is simpler.","title":"Configuration"},{"location":"quick_start/presentation_configuration/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_configuration/#configuration","text":"","title":"Configuration"},{"location":"quick_start/presentation_configuration/#director-mode","text":"To configure a device for the Director Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Director) Type device name or use the offered default name Select connection type Write down the given channel number Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen.","title":"Director Mode"},{"location":"quick_start/presentation_configuration/#audience-mode","text":"To configure a device for the Audience Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Audience) Type device name or use the offered default name Select connection type Type the channel number (the one you wrote down earlier) If you are setting a phone, select normal or VR mode (not asked on tablets) Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Note On GearVR/Oculus Go Director Mode is not available and Audience Mode configuration is simpler.","title":"Audience Mode"},{"location":"quick_start/presentation_devices/","text":"Quick start: Presentation \u00b6 Devices \u00b6 In a LiveSYNC-enabled presentation, the person giving the presentation has a control device and the audience members each have a viewer device . Hence, to give a presentation using the LiveSYNC tool you will need: 1 device for controlling the presentation (LiveSYNC tool in Director Mode ) 1-n devices for viewing the presentation (LiveSYNC tool in Audience Mode ) Director Mode \u00b6 The Director Mode is currently available for iOS and Android . Thus, you can use either a phone or a tablet for controlling a presentation. Tip We highly recommend using a tablet for the Director Mode because of its larger screen. Audience Mode \u00b6 The Audience Mode is currently available for iOS , Android , GearVR , and Oculus Go . Thus, you can view a presentation either from a phone screen, a tablet screen, or via a phone based or standalone VR headset. Note VR mode is available also for iOS and Android phones. A Google Cardboard or a compatible VR headset can be used, but user experience is much superior with a high quality headset and dedicated app version (GearVR, Oculus Go). Networking \u00b6 Communication between the control device and the viewer devices is wireless . You can choose from available options during channel configuration . Make sure that the necessary radio technology is enabled on all devices. Depending on your selection this can be for example Bluetooth, Wifi or mobile data. Power \u00b6 During a presentation the devices will consume a lot of power. Power consumption is related to active radios (communication), photo and video decoding, continuous screen updates, display brightness, and disabled power saving (sleep). We recommend that you recharge the batteries of all devices before a presentation . If you use VR headsets with hand remotes, check the batteries inside the remotes. If you plan to run a presentation repeatedly, you probably need spare devices that are being recharged while others are in use. Big screen \u00b6 In addition to personal viewing devices you can use a big screen. This can be for example a large TV screen or a projector. We recommend a wired connection between the control device and the big screen. For example, an iPad can be connected to a TV using an HDMI cable via a separately sold adapter. Speakers or headphones \u00b6 Many presentations use sound. You must decide whether you want to use room audio (audience devices muted) or viewer device audio. In the latter case, headphones are highly recommended. Chairs \u00b6 If your presentation takes more than a few minutes, the audience probably wants to sit. In order to allow comfortable 360-degree photo/video exploration, swivel chairs with enough spacing between them are recommended.","title":"Devices"},{"location":"quick_start/presentation_devices/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_devices/#devices","text":"In a LiveSYNC-enabled presentation, the person giving the presentation has a control device and the audience members each have a viewer device . Hence, to give a presentation using the LiveSYNC tool you will need: 1 device for controlling the presentation (LiveSYNC tool in Director Mode ) 1-n devices for viewing the presentation (LiveSYNC tool in Audience Mode )","title":"Devices"},{"location":"quick_start/presentation_devices/#director-mode","text":"The Director Mode is currently available for iOS and Android . Thus, you can use either a phone or a tablet for controlling a presentation. Tip We highly recommend using a tablet for the Director Mode because of its larger screen.","title":"Director Mode"},{"location":"quick_start/presentation_devices/#audience-mode","text":"The Audience Mode is currently available for iOS , Android , GearVR , and Oculus Go . Thus, you can view a presentation either from a phone screen, a tablet screen, or via a phone based or standalone VR headset. Note VR mode is available also for iOS and Android phones. A Google Cardboard or a compatible VR headset can be used, but user experience is much superior with a high quality headset and dedicated app version (GearVR, Oculus Go).","title":"Audience Mode"},{"location":"quick_start/presentation_devices/#networking","text":"Communication between the control device and the viewer devices is wireless . You can choose from available options during channel configuration . Make sure that the necessary radio technology is enabled on all devices. Depending on your selection this can be for example Bluetooth, Wifi or mobile data.","title":"Networking"},{"location":"quick_start/presentation_devices/#power","text":"During a presentation the devices will consume a lot of power. Power consumption is related to active radios (communication), photo and video decoding, continuous screen updates, display brightness, and disabled power saving (sleep). We recommend that you recharge the batteries of all devices before a presentation . If you use VR headsets with hand remotes, check the batteries inside the remotes. If you plan to run a presentation repeatedly, you probably need spare devices that are being recharged while others are in use.","title":"Power"},{"location":"quick_start/presentation_devices/#big-screen","text":"In addition to personal viewing devices you can use a big screen. This can be for example a large TV screen or a projector. We recommend a wired connection between the control device and the big screen. For example, an iPad can be connected to a TV using an HDMI cable via a separately sold adapter.","title":"Big screen"},{"location":"quick_start/presentation_devices/#speakers-or-headphones","text":"Many presentations use sound. You must decide whether you want to use room audio (audience devices muted) or viewer device audio. In the latter case, headphones are highly recommended.","title":"Speakers or headphones"},{"location":"quick_start/presentation_devices/#chairs","text":"If your presentation takes more than a few minutes, the audience probably wants to sit. In order to allow comfortable 360-degree photo/video exploration, swivel chairs with enough spacing between them are recommended.","title":"Chairs"},{"location":"quick_start/presentation_files/","text":"Quick start: Presentation \u00b6 Files \u00b6 LiveSYNC does not stream the content from the control device to audience devices. Only playback commands are sent and received. Thus, each audience device must have its own copy of the files that are used in a presentation. This ensures scalability and good user experience. Presentation files typically consist of a set of 360-degree photos (.jpg), 360-degree videos (.mp4), and custom hotspots (.png). Also ordinary 2D photos and videos can be used. Note To prepare the devices for a presentation you must copy the necessary files to all devices in advance. iOS (iPhones & iPads) \u00b6 Copy the presentation files using Apple's iTunes application. Install iTunes application to your PC or Mac computer Start iTunes application Connect your iOS device to your computer using a Lightning to USB cable Once the device is detected in iTunes, click the phone/tablet icon Click File Sharing from left side menu Select LiveSYNC from the apps list Drag'n drop files and folders to the app's file area Android (phones & tablets) \u00b6 Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC Oculus (GearVR & Oculus Go) \u00b6 Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Files"},{"location":"quick_start/presentation_files/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_files/#files","text":"LiveSYNC does not stream the content from the control device to audience devices. Only playback commands are sent and received. Thus, each audience device must have its own copy of the files that are used in a presentation. This ensures scalability and good user experience. Presentation files typically consist of a set of 360-degree photos (.jpg), 360-degree videos (.mp4), and custom hotspots (.png). Also ordinary 2D photos and videos can be used. Note To prepare the devices for a presentation you must copy the necessary files to all devices in advance.","title":"Files"},{"location":"quick_start/presentation_files/#ios-iphones-ipads","text":"Copy the presentation files using Apple's iTunes application. Install iTunes application to your PC or Mac computer Start iTunes application Connect your iOS device to your computer using a Lightning to USB cable Once the device is detected in iTunes, click the phone/tablet icon Click File Sharing from left side menu Select LiveSYNC from the apps list Drag'n drop files and folders to the app's file area","title":"iOS (iPhones &amp; iPads)"},{"location":"quick_start/presentation_files/#android-phones-tablets","text":"Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Android (phones &amp; tablets)"},{"location":"quick_start/presentation_files/#oculus-gearvr-oculus-go","text":"Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Oculus (GearVR &amp; Oculus Go)"},{"location":"quick_start/presentation_joining/","text":"Quick start: Presentation \u00b6 Joining \u00b6 Follow these steps to join a presentation on a device configured to Audience Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Audience Mode and starts from the Lobby Wait here until connection to the control device has been established Once connected, you are ready and the presentation begins when the presenter decides to do so","title":"Joining"},{"location":"quick_start/presentation_joining/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_joining/#joining","text":"Follow these steps to join a presentation on a device configured to Audience Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Audience Mode and starts from the Lobby Wait here until connection to the control device has been established Once connected, you are ready and the presentation begins when the presenter decides to do so","title":"Joining"},{"location":"quick_start/presentation_mirroring/","text":"Quick start: Presentation \u00b6 Mirroring \u00b6 To mirror content from your control device to a big screen (a secondary display), follow the steps below. iOS \u00b6 Plug your Digital AV or VGA adapter into the charging port at the bottom of your iOS device. Connect an HDMI or VGA cable to your adapter. Notice that the adapter has also a charging port; you can charge your iPad during mirroring. Connect the other end of your HDMI or VGA cable to your secondary display (TV, monitor, or projector). Turn on your secondary display. If necessary, switch to the correct video source on your secondary display. If you need help, use your display's manual. Now the screen on your iPad should appear on your TV, display, or projector. Tip It is also possible to connect wirelessly using AirPlay if you connect an Apple TV to your secondary display. Modes \u00b6 LiveSYNC provides two screen mirroring modes: Presentation : shows only the Presentation area on the secondary display Mirroring : show the complete app on the secondary display Typically, Presentation mode is the correct one. However, if you want to show how you use the app (training) or for example share the Mosaic screen, then switch to Mirroring mode. There is also a third option Show touches . This visualizes touches on your control device's screen, so that audience watching the secondary display can better understand your actions. What's next \u00b6 You have barely scratched the surface of what you can do with the LiveSYNC tool. Learn more by reading some of the tutorials or dive into the User Guide .","title":"Mirroring"},{"location":"quick_start/presentation_mirroring/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_mirroring/#mirroring","text":"To mirror content from your control device to a big screen (a secondary display), follow the steps below.","title":"Mirroring"},{"location":"quick_start/presentation_mirroring/#ios","text":"Plug your Digital AV or VGA adapter into the charging port at the bottom of your iOS device. Connect an HDMI or VGA cable to your adapter. Notice that the adapter has also a charging port; you can charge your iPad during mirroring. Connect the other end of your HDMI or VGA cable to your secondary display (TV, monitor, or projector). Turn on your secondary display. If necessary, switch to the correct video source on your secondary display. If you need help, use your display's manual. Now the screen on your iPad should appear on your TV, display, or projector. Tip It is also possible to connect wirelessly using AirPlay if you connect an Apple TV to your secondary display.","title":"iOS"},{"location":"quick_start/presentation_mirroring/#modes","text":"LiveSYNC provides two screen mirroring modes: Presentation : shows only the Presentation area on the secondary display Mirroring : show the complete app on the secondary display Typically, Presentation mode is the correct one. However, if you want to show how you use the app (training) or for example share the Mosaic screen, then switch to Mirroring mode. There is also a third option Show touches . This visualizes touches on your control device's screen, so that audience watching the secondary display can better understand your actions.","title":"Modes"},{"location":"quick_start/presentation_mirroring/#whats-next","text":"You have barely scratched the surface of what you can do with the LiveSYNC tool. Learn more by reading some of the tutorials or dive into the User Guide .","title":"What's next"},{"location":"quick_start/presentation_presenting/","text":"Quick start: Presentation \u00b6 Presenting \u00b6 When viewer devices have connected, select Player tab from the bottom bar. To start the presentation, drag an item from the Content tab and drop it to the Presentation area at the center of the screen. The status of each viewer device appears in the Devices tab. If you want to draw the attention of the audience to something, drag an item from the Tags tab and drop it to a position where you want it to appear over the content being presented. To change content, drag another item from the Content tab and drop it to the Presentation area . Once your presentation is over, tap the Home button in the top left corner to exit presentation. Connections to viewer devices will be disconnected and they will return to the Lobby. Note If you haven't purchased a license, watermarks will be shown when you are presenting your own content files. This concerns both the control and the viewer devices. Only the control device requires a license. Watermarks will not appear on viewer devices when they are connected to a control device that has a valid license.","title":"Presenting"},{"location":"quick_start/presentation_presenting/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_presenting/#presenting","text":"When viewer devices have connected, select Player tab from the bottom bar. To start the presentation, drag an item from the Content tab and drop it to the Presentation area at the center of the screen. The status of each viewer device appears in the Devices tab. If you want to draw the attention of the audience to something, drag an item from the Tags tab and drop it to a position where you want it to appear over the content being presented. To change content, drag another item from the Content tab and drop it to the Presentation area . Once your presentation is over, tap the Home button in the top left corner to exit presentation. Connections to viewer devices will be disconnected and they will return to the Lobby. Note If you haven't purchased a license, watermarks will be shown when you are presenting your own content files. This concerns both the control and the viewer devices. Only the control device requires a license. Watermarks will not appear on viewer devices when they are connected to a control device that has a valid license.","title":"Presenting"},{"location":"quick_start/presentation_software/","text":"Quick start: Presentation \u00b6 Software \u00b6 The same application is used both in the control device (Director Mode) and viewer devices (Audience Mode). iOS (iPhones & iPads) \u00b6 On your iOS device, open App Store , tap Search , and type livesync . Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Android (phones and tablets) \u00b6 On your Android device, open Play Store , and type livesync to the search field. Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Oculus (GearVR & Oculus Go) \u00b6 LiveSYNC has not officially launched on these platforms yet. However, you can already start using it. Currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus Go device, navigate to Oculus Store and type livesync to the search field. Select LiveSYNC Oculus Go . If you cannot find it, don't forget to check also the Not installed tab. The app can be installed for FREE.","title":"Software"},{"location":"quick_start/presentation_software/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_software/#software","text":"The same application is used both in the control device (Director Mode) and viewer devices (Audience Mode).","title":"Software"},{"location":"quick_start/presentation_software/#ios-iphones-ipads","text":"On your iOS device, open App Store , tap Search , and type livesync . Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"iOS (iPhones &amp; iPads)"},{"location":"quick_start/presentation_software/#android-phones-and-tablets","text":"On your Android device, open Play Store , and type livesync to the search field. Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Android (phones and tablets)"},{"location":"quick_start/presentation_software/#oculus-gearvr-oculus-go","text":"LiveSYNC has not officially launched on these platforms yet. However, you can already start using it. Currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus Go device, navigate to Oculus Store and type livesync to the search field. Select LiveSYNC Oculus Go . If you cannot find it, don't forget to check also the Not installed tab. The app can be installed for FREE.","title":"Oculus (GearVR &amp; Oculus Go)"},{"location":"quick_start/quick_start/","text":"Quick Start \u00b6 Presentation \u00b6 This is a quick start guide for giving a presentation with the LiveSYNC tool.","title":"Index"},{"location":"quick_start/quick_start/#quick-start","text":"","title":"Quick Start"},{"location":"quick_start/quick_start/#presentation","text":"This is a quick start guide for giving a presentation with the LiveSYNC tool.","title":"Presentation"},{"location":"site_howto/site_howto/","text":"How to Use This Site \u00b6 Accessing \u00b6 Web browser \u00b6 This site is publicly available in HTML format in this URL: https://docs.livesync.app You can use any modern web browser for accessing the site. Simply type the site's URL to the browser's address bar or click the link above. Your computing platform probably has a built-in web browser. If not, there are multiple supported options for each platform. These are listed below. Note This site does not require any specific browser application, any specific version, or any third party plugins. Windows PC \u00b6 Use the built-in Internet Explorer or Microsoft Edge browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome Mac \u00b6 Use the built-in Safari browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome Linux \u00b6 Use the browser included to your distribution (varies). Or, install a 3rd party application: Mozilla Firefox Google Chrome iOS (iPhone & iPad) \u00b6 Use the built-in Safari browser. Or, install a 3rd party application from Apple Appstore : Mozilla Firefox Google Chrome Android (phones & tablets) \u00b6 Use the built-in browser (varies, look for Internet app from the menu). Or, install a 3rd party application from Google Play : Mozilla Firefox Google Chrome GearVR \u00b6 Use the built-in Oculus Browser or Samsung Internet app. If you don't have them installed search from the Oculus Store . There are two ways to start them: a) Starting a browser from the Oculus GearVR app (outside VR): Remove the phone from the GearVR headset. In the Android app menu, find Oculus app (GearVR text in the icon), and start it. Once started, navigate to Library tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Put the phone into the GearVR headset when a dialog suggests that. Note If an application does not start when you tap it, you probably need to update your GearVR software. Put the phone into the GearVR headset. Update the software if a dialog suggests that (you have to remove the phone from the headset to do that). b) Starting a browser from the Oculus Home (inside VR): Put the phone into the GearVR headset. In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Tip You can also start the default browser by selecting Internet from the Navigate tab in the bottom bar. Oculus Go \u00b6 Use the built-in Oculus Browser . Or, install a 3rd party application: Firefox Reality Starting a browser from the Oculus Home (inside VR): In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Firefox Reality from the app grid, and start it. Tip You can also start the default browser by selecting Browser from the Navigate tab in the bottom bar. LiveSYNC App \u00b6 iOS & Android \u00b6 On iOS and Android phones and tablets, the documentation is accessible via the LiveSYNC app. In fact, there are two locations inside the app where you can access the site: a. Start the LiveSYNC app and enter the Home screen. Select Learning Center from the bottom bar. The landing page of the site is loaded into an internal browser view: b. On the Home screen, select a configured channel (Director Mode) and enter the Mosaic view. From the bottom bar, select Help tab. The landing page of the site is loaded into an internal browser view: Tip You can access the documentation during a presentation using the Help tab. Note Network connection is required for accessing the documentation via the app. GearVR & Oculus Go \u00b6 On GearVR and Oculus Go headsets, the documentation cannot be accessed via the LiveSYNC app. Please use the headset's native or 3rd party browser as explained above, or another device. Navigating \u00b6 Site menu \u00b6 Different areas of the LiveSYNC Learning Center are accessible via the site menu . The site menu appears in one of two different ways: Tabs & menu \u00b6 If your browser window is wide, different areas appear as tabs in the top bar. Simply click any of the tabs to navigate to another area of the site. When you scroll down, tabs disappear to save space. Scroll back up to reveal them again. The pages of an active area appear in a menu at the left side of the page. Click a title to navigate to another page. This option is typical for desktop and laptop computers. On wide screens, different site areas appear as tabs in the top bar. Pages that belong to an area are listed on the left. Collapsed menu \u00b6 If your browser window is narrow, the menu appears as a collapsed hamburger menu in the top bar . Click the hamburger icon to temporarily expand the site menu. Different areas of the site appear as submenus, which you can open by clicking them. To navigate to a page, click its title. Once selected, the menu will collapse again. This option is typical for phones, tablets, and VR headsets. On narrow screens, the site menu appears collapsed. It can be opened by clicking the hamburger menu icon. Clicking a submenu reveals pages that belong to that area. Clicking a page title opens that page. Table of contents \u00b6 Each page on this site has an automatically generated table-of-contents (TOC). You can navigate within a page by clicking the titles in the page's TOC. The TOC will appear on the right side of the page, but only if the browser window is wide enough. If not, it will be integrated to the site menu. On wide screens, table-of-contents appears on the right. On narrow screens, table-of-contents appears integrated to the site menu. Navigation arrows \u00b6 Some pages have one or two navigation arrows that appear at the bottom of the page. Some pages do not have them. Use the navigation arrows for moving to the next or to the previous page when you have read the current page. On some pages you can move forward or backward by clicking the navigation arrows. Search \u00b6 The top bar contains the search field . The search field may appear in one of two different ways: Expanded search field \u00b6 If your browser window is wide, the search bar appears permanently expanded on the right side of the top bar. Simply click the text field and begin to type. The results will appear in a floating window as you type. This option is typical for desktop and laptop computers, as well as tablets. On wide screens, the search field is expanded and results appear floating over the page. Collapsed search field \u00b6 If your browser window is narrow, the search bar appears as a collapsed widget on the right side of the top bar. You can recognize it from the magnifier glass icon. Simply click the icon and begin to type. The results will appear in full window mode as you type. This option is typical for phones and VR headsets. On narrow screens, the search field is collapsed and results appear in full window mode. Reading \u00b6 Languages \u00b6 The documentation is currently available only in English. Translating the documentation to other selected languages will be considered if there is enough customer demand. You can contact us via our contact form . Product variants \u00b6 Different customer segments need different features. For example, most 360-degree video professionals are interested in using LiveSYNC's presentation capabilities. On the other hand, industrial users focus on tagging and reporting. Thus, the LiveSYNC app is offered in multiple variants. The variants have a common core, but available features vary. The purchased license defines what features are available in a particular app installation. The documentation site is common to all versions. This means that parts of the documentation discuss features that are available only for a subset of customers. To differentiate such parts, they are clearly marked with banners . For example, users who do not have the Enterprise version of the app should skip sections that are marked with this banner: An example of a product variant specific documentation. The content below this banner, up to the next section/title, is relevant only for users of the Enterprise version of the LiveSYNC app. Product versions \u00b6 The LiveSYNC solution is continuously under development and updates are released frequently. We recommend that you keep the app up-to-date. There is one exception to this rule. Do you have an important presentation coming up soon and everything is already set up and working smoothly? Then we recommend that you do not update the app before the presentation. The documentation site contains documentation for the latest version of the LiveSYNC app. Conventions \u00b6 The goal when writing the documentation was to make it as readable as possible. Hence, we use many conventions throughout the text. We also wanted to provide the facility for readers to learn at their own pace. There are plenty of cross-references (links). Use them for learning more about a specific topic. Example title \u00b6 There are multiple levels of titles. All titles appear in the table of contents. Simply click a title to scroll the page there. The titles are also permalinks (permanent links). You can see the link symbol by hovering over the title with mouse cursor. Then, hover over the link symbol. The link appears in the bottom bar of your browser (if supported). You can also copy the link with the right mouse button. Regular text appears like this. Important terms and text that appears in a user interface are written in italics . Product names are often written in black . Links appear like this . Numbered references appear inside parentheses (1.1) . Mathematical operations, configuration file contents, and software code appear in code blocks. Notice that you can easily copy its contents to the clipboard. Simply click the copy icon at the right edge of the block. 1 + 1 = 2 Quotations are indented: Life is short. Data is often presented as a table: Hours of sleep Laziness 5 Severe 7 Mild 8 None Lists can be ordered (numbered): Skip the manual Try to use the product Read the manual Lists can also be unordered (bulleted): First things first Second things second Footnotes are used for providing additional information. They appear at the end of the page. You can read a footnote by clicking the link that appears as a number: 1 Sometimes, a horizontal rule is used for separating things: Throughout the documentation, you'll find various types of notes complementing the regular text: Note A note is designed to provide an important piece of information. Tip A tip is something that will help when you need to perform the task being described. Alternatively, it can be something can make your life easier when using the LiveSYNC tool. Example An example applies theory to practice. It will help your understanding of the topic being discussed. Question A question is a short practice task for the reader. Summary A summary is a collection of the key points, typically at the end of a section. Caution A caution is something you should certainly pay attention to. It warns of a hidden danger or caveat. Danger A danger is a something that can potentially harm you. You probably should not do what is being discussed. This is a foot note. You can return to the text by clicking the return arrow link: \u21a9","title":"How to use this site"},{"location":"site_howto/site_howto/#how-to-use-this-site","text":"","title":"How to Use This Site"},{"location":"site_howto/site_howto/#accessing","text":"","title":"Accessing"},{"location":"site_howto/site_howto/#web-browser","text":"This site is publicly available in HTML format in this URL: https://docs.livesync.app You can use any modern web browser for accessing the site. Simply type the site's URL to the browser's address bar or click the link above. Your computing platform probably has a built-in web browser. If not, there are multiple supported options for each platform. These are listed below. Note This site does not require any specific browser application, any specific version, or any third party plugins.","title":"Web browser"},{"location":"site_howto/site_howto/#windows-pc","text":"Use the built-in Internet Explorer or Microsoft Edge browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Windows PC"},{"location":"site_howto/site_howto/#mac","text":"Use the built-in Safari browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Mac"},{"location":"site_howto/site_howto/#linux","text":"Use the browser included to your distribution (varies). Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Linux"},{"location":"site_howto/site_howto/#ios-iphone-ipad","text":"Use the built-in Safari browser. Or, install a 3rd party application from Apple Appstore : Mozilla Firefox Google Chrome","title":"iOS (iPhone &amp; iPad)"},{"location":"site_howto/site_howto/#android-phones-tablets","text":"Use the built-in browser (varies, look for Internet app from the menu). Or, install a 3rd party application from Google Play : Mozilla Firefox Google Chrome","title":"Android (phones &amp; tablets)"},{"location":"site_howto/site_howto/#gearvr","text":"Use the built-in Oculus Browser or Samsung Internet app. If you don't have them installed search from the Oculus Store . There are two ways to start them: a) Starting a browser from the Oculus GearVR app (outside VR): Remove the phone from the GearVR headset. In the Android app menu, find Oculus app (GearVR text in the icon), and start it. Once started, navigate to Library tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Put the phone into the GearVR headset when a dialog suggests that. Note If an application does not start when you tap it, you probably need to update your GearVR software. Put the phone into the GearVR headset. Update the software if a dialog suggests that (you have to remove the phone from the headset to do that). b) Starting a browser from the Oculus Home (inside VR): Put the phone into the GearVR headset. In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Tip You can also start the default browser by selecting Internet from the Navigate tab in the bottom bar.","title":"GearVR"},{"location":"site_howto/site_howto/#oculus-go","text":"Use the built-in Oculus Browser . Or, install a 3rd party application: Firefox Reality Starting a browser from the Oculus Home (inside VR): In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Firefox Reality from the app grid, and start it. Tip You can also start the default browser by selecting Browser from the Navigate tab in the bottom bar.","title":"Oculus Go"},{"location":"site_howto/site_howto/#livesync-app","text":"","title":"LiveSYNC App"},{"location":"site_howto/site_howto/#ios-android","text":"On iOS and Android phones and tablets, the documentation is accessible via the LiveSYNC app. In fact, there are two locations inside the app where you can access the site: a. Start the LiveSYNC app and enter the Home screen. Select Learning Center from the bottom bar. The landing page of the site is loaded into an internal browser view: b. On the Home screen, select a configured channel (Director Mode) and enter the Mosaic view. From the bottom bar, select Help tab. The landing page of the site is loaded into an internal browser view: Tip You can access the documentation during a presentation using the Help tab. Note Network connection is required for accessing the documentation via the app.","title":"iOS &amp; Android"},{"location":"site_howto/site_howto/#gearvr-oculus-go","text":"On GearVR and Oculus Go headsets, the documentation cannot be accessed via the LiveSYNC app. Please use the headset's native or 3rd party browser as explained above, or another device.","title":"GearVR &amp; Oculus Go"},{"location":"site_howto/site_howto/#navigating","text":"","title":"Navigating"},{"location":"site_howto/site_howto/#site-menu","text":"Different areas of the LiveSYNC Learning Center are accessible via the site menu . The site menu appears in one of two different ways:","title":"Site menu"},{"location":"site_howto/site_howto/#tabs-menu","text":"If your browser window is wide, different areas appear as tabs in the top bar. Simply click any of the tabs to navigate to another area of the site. When you scroll down, tabs disappear to save space. Scroll back up to reveal them again. The pages of an active area appear in a menu at the left side of the page. Click a title to navigate to another page. This option is typical for desktop and laptop computers. On wide screens, different site areas appear as tabs in the top bar. Pages that belong to an area are listed on the left.","title":"Tabs &amp; menu"},{"location":"site_howto/site_howto/#collapsed-menu","text":"If your browser window is narrow, the menu appears as a collapsed hamburger menu in the top bar . Click the hamburger icon to temporarily expand the site menu. Different areas of the site appear as submenus, which you can open by clicking them. To navigate to a page, click its title. Once selected, the menu will collapse again. This option is typical for phones, tablets, and VR headsets. On narrow screens, the site menu appears collapsed. It can be opened by clicking the hamburger menu icon. Clicking a submenu reveals pages that belong to that area. Clicking a page title opens that page.","title":"Collapsed menu"},{"location":"site_howto/site_howto/#table-of-contents","text":"Each page on this site has an automatically generated table-of-contents (TOC). You can navigate within a page by clicking the titles in the page's TOC. The TOC will appear on the right side of the page, but only if the browser window is wide enough. If not, it will be integrated to the site menu. On wide screens, table-of-contents appears on the right. On narrow screens, table-of-contents appears integrated to the site menu.","title":"Table of contents"},{"location":"site_howto/site_howto/#navigation-arrows","text":"Some pages have one or two navigation arrows that appear at the bottom of the page. Some pages do not have them. Use the navigation arrows for moving to the next or to the previous page when you have read the current page. On some pages you can move forward or backward by clicking the navigation arrows.","title":"Navigation arrows"},{"location":"site_howto/site_howto/#search","text":"The top bar contains the search field . The search field may appear in one of two different ways:","title":"Search"},{"location":"site_howto/site_howto/#expanded-search-field","text":"If your browser window is wide, the search bar appears permanently expanded on the right side of the top bar. Simply click the text field and begin to type. The results will appear in a floating window as you type. This option is typical for desktop and laptop computers, as well as tablets. On wide screens, the search field is expanded and results appear floating over the page.","title":"Expanded search field"},{"location":"site_howto/site_howto/#collapsed-search-field","text":"If your browser window is narrow, the search bar appears as a collapsed widget on the right side of the top bar. You can recognize it from the magnifier glass icon. Simply click the icon and begin to type. The results will appear in full window mode as you type. This option is typical for phones and VR headsets. On narrow screens, the search field is collapsed and results appear in full window mode.","title":"Collapsed search field"},{"location":"site_howto/site_howto/#reading","text":"","title":"Reading"},{"location":"site_howto/site_howto/#languages","text":"The documentation is currently available only in English. Translating the documentation to other selected languages will be considered if there is enough customer demand. You can contact us via our contact form .","title":"Languages"},{"location":"site_howto/site_howto/#product-variants","text":"Different customer segments need different features. For example, most 360-degree video professionals are interested in using LiveSYNC's presentation capabilities. On the other hand, industrial users focus on tagging and reporting. Thus, the LiveSYNC app is offered in multiple variants. The variants have a common core, but available features vary. The purchased license defines what features are available in a particular app installation. The documentation site is common to all versions. This means that parts of the documentation discuss features that are available only for a subset of customers. To differentiate such parts, they are clearly marked with banners . For example, users who do not have the Enterprise version of the app should skip sections that are marked with this banner: An example of a product variant specific documentation. The content below this banner, up to the next section/title, is relevant only for users of the Enterprise version of the LiveSYNC app.","title":"Product variants"},{"location":"site_howto/site_howto/#product-versions","text":"The LiveSYNC solution is continuously under development and updates are released frequently. We recommend that you keep the app up-to-date. There is one exception to this rule. Do you have an important presentation coming up soon and everything is already set up and working smoothly? Then we recommend that you do not update the app before the presentation. The documentation site contains documentation for the latest version of the LiveSYNC app.","title":"Product versions"},{"location":"site_howto/site_howto/#conventions","text":"The goal when writing the documentation was to make it as readable as possible. Hence, we use many conventions throughout the text. We also wanted to provide the facility for readers to learn at their own pace. There are plenty of cross-references (links). Use them for learning more about a specific topic.","title":"Conventions"},{"location":"site_howto/site_howto/#example-title","text":"There are multiple levels of titles. All titles appear in the table of contents. Simply click a title to scroll the page there. The titles are also permalinks (permanent links). You can see the link symbol by hovering over the title with mouse cursor. Then, hover over the link symbol. The link appears in the bottom bar of your browser (if supported). You can also copy the link with the right mouse button. Regular text appears like this. Important terms and text that appears in a user interface are written in italics . Product names are often written in black . Links appear like this . Numbered references appear inside parentheses (1.1) . Mathematical operations, configuration file contents, and software code appear in code blocks. Notice that you can easily copy its contents to the clipboard. Simply click the copy icon at the right edge of the block. 1 + 1 = 2 Quotations are indented: Life is short. Data is often presented as a table: Hours of sleep Laziness 5 Severe 7 Mild 8 None Lists can be ordered (numbered): Skip the manual Try to use the product Read the manual Lists can also be unordered (bulleted): First things first Second things second Footnotes are used for providing additional information. They appear at the end of the page. You can read a footnote by clicking the link that appears as a number: 1 Sometimes, a horizontal rule is used for separating things: Throughout the documentation, you'll find various types of notes complementing the regular text: Note A note is designed to provide an important piece of information. Tip A tip is something that will help when you need to perform the task being described. Alternatively, it can be something can make your life easier when using the LiveSYNC tool. Example An example applies theory to practice. It will help your understanding of the topic being discussed. Question A question is a short practice task for the reader. Summary A summary is a collection of the key points, typically at the end of a section. Caution A caution is something you should certainly pay attention to. It warns of a hidden danger or caveat. Danger A danger is a something that can potentially harm you. You probably should not do what is being discussed. This is a foot note. You can return to the text by clicking the return arrow link: \u21a9","title":"Example title"},{"location":"support/support/","text":"Support \u00b6 Got a question that is not answered in the documentation? A feature request? Or maybe you found an error in the docs or wish to suggest a topic for an article? Feel free to contact us by leaving a message! * { box-sizing: border-box; } input[class=contact], textarea { width: 100%; padding: 12px; border: 1px solid #ccc; margin-top: 6px; margin-bottom: 16px; resize: vertical; } input[type=submit] { background-color: #4CAF50; color: white; padding: 12px 20px; margin-top: 10px; border: none; cursor: pointer; } input[type=submit]:hover { background-color: #45a049; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 10px; } Contact Us Name Company Email Message","title":"Index"},{"location":"support/support/#support","text":"Got a question that is not answered in the documentation? A feature request? Or maybe you found an error in the docs or wish to suggest a topic for an article? Feel free to contact us by leaving a message! * { box-sizing: border-box; } input[class=contact], textarea { width: 100%; padding: 12px; border: 1px solid #ccc; margin-top: 6px; margin-bottom: 16px; resize: vertical; } input[type=submit] { background-color: #4CAF50; color: white; padding: 12px 20px; margin-top: 10px; border: none; cursor: pointer; } input[type=submit]:hover { background-color: #45a049; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 10px; }","title":"Support"},{"location":"tutorials/oculus_go_device/","text":"LiveSYNC on Oculus Go \u00b6 Oculus Go standalone VR headset. The Device \u00b6 What is it? \u00b6 Oculus Go is an affordable standalone VR headset from Oculus , a company owned by Facebook . Oculus was founded in 2012 and allegedly started the current VR boom. Oculus released their first headset Oculus DK1 for developers back in 2013. Facebook famously acquired the company in 2014 for US$2.3 billion. Oculus is considered one of the leading VR companies. Oculus Go was released 5 years after DK1 in may 2018. It is mainly targeted for 360-degree media and simple VR games. The headset has many similarities to GearVR , a product made by Samsung . In fact, GearVR uses technology and software from Oculus. It is also marketed under Powered by Oculus term. GearVR was released already in November 2015, but it is still very similar to the original version despite of yearly updates. The main difference between GearVR and Oculus Go? Oculus Go is a new standalone device whereas GearVR consists of a combination of a VR frame and a compatible Samsung phone. In both cases applications are installed from Oculus Store and same app builds can work on both products. The user experience, image quality and interaction mechanism are almost the same. Both products are 3DOF devices (three degrees-of-freedom). This means that user can look around in VR by turning his head, but not walk in VR by physically taking steps. In September 2018, Oculus announced a new headset Oculus Quest . It will be a 6DOF device and thus able to track walking. Yet, it is more expensive, mainly targeted for games, and brings little benefit for 360-degree media users. Thus, Oculus Go is not going to become outdated anytime soon. Compared to PC or game console based VR, mobile headsets have less rendering power. From 360-degree media point-of-view this is not very significant. Mobile headsets also have a couple of huge benefits: no computer, no wires. You can go anywhere you like, take the headset out of your bag, put it on your face, and start using it right away. Usage is simple with the bundled hand remote. It works as a pointer when selecting objects from the screen. You can also swipe the touchpad with your thumb to scroll. The touchpad can be clicked but there is also a selection button for your index finger. For navigation, the remote offers a separate Back button and an Oculus button. The latter is used for calibrating the hand remote (long press) or returning to Oculus Home (short press). The headset itself has only power and volume up/down buttons. Built-in speakers are cleverly hidden inside the head strap. They provide stereo or spatial audio without a need to plug in external headphones. The head strap is flexible and easy to adjust. The headset has a detachable soft face mask, which makes it fairly comfortable to wear. Oculus Go is quite heavy, though. Normal size eyeglasses fit inside, but you can also order prescription lens inserts. All in all, Oculus Go is a great VR headset especially for 360-degree media. Should I get an Oculus Go or a GearVR? \u00b6 If you already own a compatible Samsung phone, purchasing only the VR frame is an affordable choice (about $110). GearVR headset is a lightweight addition to your bag, which you will probably value if you travel a lot and carry a large-screen phone anyway. However, in most cases Oculus Go is a better choice. Since it is a standalone device it has a battery of its own and you can dedicate the device for its sole purpose: VR. You will also save a fortune if you need a large number of headsets. To learn more about their differences, read comparison articles: Android Central: Oculus Go vs Samsung Gear VR How-To Geek: Gear VR vs Oculus Go: Which One is Better? ThreeSixtyCameras.com: Gear VR vs Oculus Go: Which is the better VR experience? How do I get one? \u00b6 You can purchase an Oculus Go directly from Oculus , from various online stores such as Amazon , or from local computer and electronics stores. A 32 GB model costs $199 (\u20ac219) and a 64 GB model $249 (\u20ac269).","title":"Device"},{"location":"tutorials/oculus_go_device/#livesync-on-oculus-go","text":"Oculus Go standalone VR headset.","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_device/#the-device","text":"","title":"The Device"},{"location":"tutorials/oculus_go_device/#what-is-it","text":"Oculus Go is an affordable standalone VR headset from Oculus , a company owned by Facebook . Oculus was founded in 2012 and allegedly started the current VR boom. Oculus released their first headset Oculus DK1 for developers back in 2013. Facebook famously acquired the company in 2014 for US$2.3 billion. Oculus is considered one of the leading VR companies. Oculus Go was released 5 years after DK1 in may 2018. It is mainly targeted for 360-degree media and simple VR games. The headset has many similarities to GearVR , a product made by Samsung . In fact, GearVR uses technology and software from Oculus. It is also marketed under Powered by Oculus term. GearVR was released already in November 2015, but it is still very similar to the original version despite of yearly updates. The main difference between GearVR and Oculus Go? Oculus Go is a new standalone device whereas GearVR consists of a combination of a VR frame and a compatible Samsung phone. In both cases applications are installed from Oculus Store and same app builds can work on both products. The user experience, image quality and interaction mechanism are almost the same. Both products are 3DOF devices (three degrees-of-freedom). This means that user can look around in VR by turning his head, but not walk in VR by physically taking steps. In September 2018, Oculus announced a new headset Oculus Quest . It will be a 6DOF device and thus able to track walking. Yet, it is more expensive, mainly targeted for games, and brings little benefit for 360-degree media users. Thus, Oculus Go is not going to become outdated anytime soon. Compared to PC or game console based VR, mobile headsets have less rendering power. From 360-degree media point-of-view this is not very significant. Mobile headsets also have a couple of huge benefits: no computer, no wires. You can go anywhere you like, take the headset out of your bag, put it on your face, and start using it right away. Usage is simple with the bundled hand remote. It works as a pointer when selecting objects from the screen. You can also swipe the touchpad with your thumb to scroll. The touchpad can be clicked but there is also a selection button for your index finger. For navigation, the remote offers a separate Back button and an Oculus button. The latter is used for calibrating the hand remote (long press) or returning to Oculus Home (short press). The headset itself has only power and volume up/down buttons. Built-in speakers are cleverly hidden inside the head strap. They provide stereo or spatial audio without a need to plug in external headphones. The head strap is flexible and easy to adjust. The headset has a detachable soft face mask, which makes it fairly comfortable to wear. Oculus Go is quite heavy, though. Normal size eyeglasses fit inside, but you can also order prescription lens inserts. All in all, Oculus Go is a great VR headset especially for 360-degree media.","title":"What is it?"},{"location":"tutorials/oculus_go_device/#should-i-get-an-oculus-go-or-a-gearvr","text":"If you already own a compatible Samsung phone, purchasing only the VR frame is an affordable choice (about $110). GearVR headset is a lightweight addition to your bag, which you will probably value if you travel a lot and carry a large-screen phone anyway. However, in most cases Oculus Go is a better choice. Since it is a standalone device it has a battery of its own and you can dedicate the device for its sole purpose: VR. You will also save a fortune if you need a large number of headsets. To learn more about their differences, read comparison articles: Android Central: Oculus Go vs Samsung Gear VR How-To Geek: Gear VR vs Oculus Go: Which One is Better? ThreeSixtyCameras.com: Gear VR vs Oculus Go: Which is the better VR experience?","title":"Should I get an Oculus Go or a GearVR?"},{"location":"tutorials/oculus_go_device/#how-do-i-get-one","text":"You can purchase an Oculus Go directly from Oculus , from various online stores such as Amazon , or from local computer and electronics stores. A 32 GB model costs $199 (\u20ac219) and a 64 GB model $249 (\u20ac269).","title":"How do I get one?"},{"location":"tutorials/oculus_go_preparing/","text":"LiveSYNC on Oculus Go \u00b6 Preparing \u00b6 Next, we will go through basic preparations before a presentation. As a reminder, presenting with the LiveSYNC tool works as follows: The presenter controls the presentation with a separate control device. This is typically an iOS or an Android tablet. The presenter will reserve a communication channel for the presentation. A channel number consists of four digits from range 1000-9999. Each member of the audience uses a personal viewing device. This can be an iOS/Android phone or tablet, or GearVR/Oculus Go headset. The viewer devices will join the communication channel using the channel number. Channel configuration \u00b6 Note Here we assume that you have already set up a channel in Director Mode on your control device (tablet). As an example, we will use channel number 5034 . Refer to User Guide for more information. To view a presentation, the headset must join the same channel that the control device is using. Follow these steps to configure a new presentation channel to your Oculus Go headset: On your Oculus Go device, start the LiveSYNC app, and select '+' from the Home screen. Point it with the hand remote controller and click the controller's selection button. Type in the channel number using the virtual numeric keypad, and click OK . The configured channel appears on the Home screen. Tip If you have multiple control devices (tablets), you can configure a separate channel for each by repeating the steps 1-3. This way you can easily choose which channel number to join ie. whose presentation to follow. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. When one of the representatives needs a break, they will temporarily connect all headsets to one iPad. Their iPads are using channels 2054 and 5039. Joan configures both channels to all six headsets. Now they can quickly swap a headset from one iPad to the other. Note Bluetooth connections are local and hence a channel number is reserved from a control device's own pool. Thus, you can reuse a configured channel as many times as you want. This is different when you use Finwe's GlobalSYNC connection (a cloud service). A channel number is reserved from a globally shared pool for a limited period of time. After your lease time expires, the channel will be returned to the pool. When this happens, you must reserve a new channel for your next presentation. Connection test \u00b6 To test a configured channel, perform the following steps: On your control device (tablet), start the LiveSYNC app, and select a channel from the Home screen. The Mosaic tab will appear, showing My device item only (tablet's own view). On your Oculus Go device, select the same channel from the Home screen. Note If this is the first time you join a channel from this device, a permission dialog appears. It is shown also if you have removed file access permission. Select Allow to grant file access permission. Else, LiveSYNC app cannot access your own presentation files. Next, the Lobby appears. Audience members will wait here for a connection to the control device. The Lobby is a 360-degree environment where users can look around. The front wall contains a 2D screen panel, where notifications to the user are presented. The headset connects to the control device automatically as soon as it is available and within reach. User does not have to do anything . Note Usually, this takes only a few seconds. However, with Bluetooth technology and multiple devices, it can take up to tens of seconds. The devices share the same radio frequencies. Because of this, connection times become longer when the number of devices increases. Once the connection is established, live view from the Oculus Go headset appears on the control device's screen. The Mosaic view now contains a new item: Simultaneously on the headset, a notification tells that the device is ready for presentation: Tip If necessary, you can connect more viewing devices by repeating steps 2-4. They will all appear in the Mosaic view. You can freely mix all kinds of viewing devices: phones, tablets, and VR headsets. Note Your license type, used connection method, and used hardware set an upper limit for the number of simultaneous connections. You should test this in advance, before your presentation. Tip If you use a Bluetooth connection, be aware that Bluetooth implementations (chipsets and drivers) vary. Some perform better than others and have different maximum capabilities. When a Bluetooth chipset cannot handle the load, problems may occur. Usually disabling and enabling Bluetooth feature helps, but sometimes the device needs to be restarted. In case you encounter repeated connection problems, follow these steps: On your control device, disable Bluetooth, wait 10 seconds, and enable it again. All devices will reconnect. If the problem is not solved, restart the Oculus Go headset that has trouble connecting. If the problem is still not solved, restart also the control device. If the problem is still not solved, try to use a smaller amount of devices. Bluetooth 4.x compatible control devices typically allow connecting to 4-8 devices simultaneously. Warning The Oculus accompanying app uses Bluetooth for communicating with the Oculus Go headset. When the app is running, it tends to keep a Bluetooth connection open. Or, automatically opens a new connection when the headset appears within range. This can interfere with LiveSYNC when you try to connect to a channel using Bluetooth. The Oculus app does not provide a method for manually disconnecting. Also, it does not automatically disconnect when it is sent to the background. Thus, we recommend that you kill the Oculus app after use to disconnect it from the headset : Launch the recent applications menu (a.k.a task list / overview). How to do this depends on the Android version and phone brand. Often it is a simple square icon or an icon that resembles two rectangles overlapping each other. In portrait orientation, the button is located at the bottom of the screen next to the home button. Scroll through the apps until you find the Oculus app. Drag the app off the screen. In portrait orientation, swipe it to the right. When it disappears from the screen it will be closed and the Bluetooth connection will be freed. Presentation test \u00b6 When devices are connected you can test presenting by using bundled demo content: On your control device, switch to the Player tab from the bottom bar. Drag an item from the Content tab to the Presentation area at the center of the screen, and drop it there. On the headset, the playback command will be received, requested media item loaded, and the image rendered on screen. The headset will notify the control device of success or failure. (For example, if the content file could not be found). The control device shows a live view from the headset in the Devices tab and in the Mosaic tab. This allows monitoring the headset's operation. Leaving a presentation \u00b6 To leave an ongoing presentation, follow these steps: Press the Back button (arrow to left) from your Oculus Go headset's hand remote controller. A confirmation dialog will appear. After selecting OK , the headset will disconnect from the control device and return to Home . Simultaneously, the headset will disappear from the control device's Mosaic and Devices tabs. Other connected devices (if any) are not affected. Note The presenter can stop the presentation from the control device by pressing the Home icon (top left corner). A confirmation dialog will appear before the presentation is stopped. This method disconnects all connected viewing devices. Copying content files \u00b6 The LiveSYNC app contains sample files for testing. You can, of course, present also your own content. To do this, copy your content files (photos, videos, hotspot icons) to each device that will be used in your presentation . This means the control device and every viewing device. Read more from the User Guide . Next, we will go through how to copy content files to your Oculus Go headset. Windows PC \u00b6 Connect your Oculus Go headset to your PC using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Windows detects the headset and you choose to handle this device by exploring its files, it appears as a new device in Explorer . For example, on Windows 10 you'll find it under This PC -> VR-Headset . Copy your content files and folders to your Oculus Go just like you'd copy them into a USB flash drive. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your PC does not recognize the headset when you connect it with a USB cable, check that Developer Mode isn't enabled in your headset. On the Oculus companion app, navigate to Settings -> Your Oculus Go -> More Settings -> Developer Mode . Read more from Tips & Tricks . If it is enabled, the device will be detected in a different a USB mode, and it will not appear in Explorer. You can temporarily disable Developer Mode . Or, use the Android Debug Bridge (ADB) command line tool for transferring files. Mac \u00b6 Install Android File Transfer app for transferring files: https://www.android.com/filetransfer/ Connect your Oculus Go headset to your Mac using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Android File Transfer detects the headset, a new file management window appears. Copy your content files and folders to your Oculus Go by dragging them from your Mac's Finder window. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your headset is not detected and you are using a USB hub or an extension cable, try connecting the headset's cable directly to your computer. If this doesn't help, try connecting the cable to a different USB port. Storage Permission \u00b6 Check permissions in case you cannot find the LiveSYNC folder under Movies , or the LiveSYNC app cannot find your own files from that folder. /LiveSYNC folder is automatically generated when the app is run (if the folder is not found). To be able to create the folder under /Movies , the LiveSYNC app needs permission from the user to access the file system (read/write). This permission is first asked when you attempt to join a presentation. The permission remains until you manually remove it or re-install the app. You can also set the permission on your headset via Oculus Home as follows: From the bottom bar, select Navigate tab and then Library . From the left menu, select Apps . Find the LiveSYNC app from the app grid. Tap the three dots next to text LiveSYNC Oculus Go , and select Permissions from the appearing menu. Note the position of the switches. Both switches should be enabled: Location is required by Bluetooth connectivity and Storage for file access. Run the LiveSYNC app again to trigger folder creation. Note You can also use Explorer on Windows or Android File Transfer on Mac to create the folder yourself. LiveSYNC will still need the Storage permission for reading the files from this location. File naming conventions \u00b6 The folder /Movies/LiveSYNC is the content root folder for the LiveSYNC app on all Android-based devices (Oculus Go runs on Android OS). When you copy your content files there, make sure to use exactly the same filenames that you use on your control device . For example, the names are case sensitive: MyVideo.mp4 is not the same as myvideo.mp4 . If the filenames do not match, the LiveSYNC app running on your Oculus Go headset cannot find the file that the control device tells it to load. When this happens, a textual error notification will appear on the headset and also in the control device's live view for this particular headset. Notice that while the filenames must match, the file content can be different. Example Matt is using an old iPad as a control device for his presentation. The iPad can play FullHD resolution videos, but not 4K. Matt encodes two different versions of his video: 1920x960 for his iPad and 3840x1920 for his Oculus Go. He renames the files so that they have the same filename Christmas.mp4 on his iPad and on his Oculus Go. LiveSYNC detects them as the same video and correctly loads them on both devices. You can copy your 360-degree photos and videos directly to /Movies/LiveSYNC folder, but you can also create one level of subfolders. This helps in organizing your content. Notice that if you use subfolders on your control device, you must use them also on your viewing devices. Example /Movies/LiveSYNC/Christmas2018 is a valid location for your 360-degree Christmas photos from 2018. /Movies/LiveSYNC/2018/Christmas has two directory levels under the LiveSYNC root folder and is thus invalid. Info With the LiveSYNC tool, you can present normal 2D photos and videos, 2D and 3D 360-degree photos and videos, and use your own tag images. File naming conventions allow LiveSYNC to correctly detect files. For example, whether an image file is a 2D photo, a 2D 360 photo, a 3D 360 photo, or a tag icon. These rules are described in the User Guide and they apply also to LiveSYNC on Oculus Go.","title":"Preparing"},{"location":"tutorials/oculus_go_preparing/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_preparing/#preparing","text":"Next, we will go through basic preparations before a presentation. As a reminder, presenting with the LiveSYNC tool works as follows: The presenter controls the presentation with a separate control device. This is typically an iOS or an Android tablet. The presenter will reserve a communication channel for the presentation. A channel number consists of four digits from range 1000-9999. Each member of the audience uses a personal viewing device. This can be an iOS/Android phone or tablet, or GearVR/Oculus Go headset. The viewer devices will join the communication channel using the channel number.","title":"Preparing"},{"location":"tutorials/oculus_go_preparing/#channel-configuration","text":"Note Here we assume that you have already set up a channel in Director Mode on your control device (tablet). As an example, we will use channel number 5034 . Refer to User Guide for more information. To view a presentation, the headset must join the same channel that the control device is using. Follow these steps to configure a new presentation channel to your Oculus Go headset: On your Oculus Go device, start the LiveSYNC app, and select '+' from the Home screen. Point it with the hand remote controller and click the controller's selection button. Type in the channel number using the virtual numeric keypad, and click OK . The configured channel appears on the Home screen. Tip If you have multiple control devices (tablets), you can configure a separate channel for each by repeating the steps 1-3. This way you can easily choose which channel number to join ie. whose presentation to follow. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. When one of the representatives needs a break, they will temporarily connect all headsets to one iPad. Their iPads are using channels 2054 and 5039. Joan configures both channels to all six headsets. Now they can quickly swap a headset from one iPad to the other. Note Bluetooth connections are local and hence a channel number is reserved from a control device's own pool. Thus, you can reuse a configured channel as many times as you want. This is different when you use Finwe's GlobalSYNC connection (a cloud service). A channel number is reserved from a globally shared pool for a limited period of time. After your lease time expires, the channel will be returned to the pool. When this happens, you must reserve a new channel for your next presentation.","title":"Channel configuration"},{"location":"tutorials/oculus_go_preparing/#connection-test","text":"To test a configured channel, perform the following steps: On your control device (tablet), start the LiveSYNC app, and select a channel from the Home screen. The Mosaic tab will appear, showing My device item only (tablet's own view). On your Oculus Go device, select the same channel from the Home screen. Note If this is the first time you join a channel from this device, a permission dialog appears. It is shown also if you have removed file access permission. Select Allow to grant file access permission. Else, LiveSYNC app cannot access your own presentation files. Next, the Lobby appears. Audience members will wait here for a connection to the control device. The Lobby is a 360-degree environment where users can look around. The front wall contains a 2D screen panel, where notifications to the user are presented. The headset connects to the control device automatically as soon as it is available and within reach. User does not have to do anything . Note Usually, this takes only a few seconds. However, with Bluetooth technology and multiple devices, it can take up to tens of seconds. The devices share the same radio frequencies. Because of this, connection times become longer when the number of devices increases. Once the connection is established, live view from the Oculus Go headset appears on the control device's screen. The Mosaic view now contains a new item: Simultaneously on the headset, a notification tells that the device is ready for presentation: Tip If necessary, you can connect more viewing devices by repeating steps 2-4. They will all appear in the Mosaic view. You can freely mix all kinds of viewing devices: phones, tablets, and VR headsets. Note Your license type, used connection method, and used hardware set an upper limit for the number of simultaneous connections. You should test this in advance, before your presentation. Tip If you use a Bluetooth connection, be aware that Bluetooth implementations (chipsets and drivers) vary. Some perform better than others and have different maximum capabilities. When a Bluetooth chipset cannot handle the load, problems may occur. Usually disabling and enabling Bluetooth feature helps, but sometimes the device needs to be restarted. In case you encounter repeated connection problems, follow these steps: On your control device, disable Bluetooth, wait 10 seconds, and enable it again. All devices will reconnect. If the problem is not solved, restart the Oculus Go headset that has trouble connecting. If the problem is still not solved, restart also the control device. If the problem is still not solved, try to use a smaller amount of devices. Bluetooth 4.x compatible control devices typically allow connecting to 4-8 devices simultaneously. Warning The Oculus accompanying app uses Bluetooth for communicating with the Oculus Go headset. When the app is running, it tends to keep a Bluetooth connection open. Or, automatically opens a new connection when the headset appears within range. This can interfere with LiveSYNC when you try to connect to a channel using Bluetooth. The Oculus app does not provide a method for manually disconnecting. Also, it does not automatically disconnect when it is sent to the background. Thus, we recommend that you kill the Oculus app after use to disconnect it from the headset : Launch the recent applications menu (a.k.a task list / overview). How to do this depends on the Android version and phone brand. Often it is a simple square icon or an icon that resembles two rectangles overlapping each other. In portrait orientation, the button is located at the bottom of the screen next to the home button. Scroll through the apps until you find the Oculus app. Drag the app off the screen. In portrait orientation, swipe it to the right. When it disappears from the screen it will be closed and the Bluetooth connection will be freed.","title":"Connection test"},{"location":"tutorials/oculus_go_preparing/#presentation-test","text":"When devices are connected you can test presenting by using bundled demo content: On your control device, switch to the Player tab from the bottom bar. Drag an item from the Content tab to the Presentation area at the center of the screen, and drop it there. On the headset, the playback command will be received, requested media item loaded, and the image rendered on screen. The headset will notify the control device of success or failure. (For example, if the content file could not be found). The control device shows a live view from the headset in the Devices tab and in the Mosaic tab. This allows monitoring the headset's operation.","title":"Presentation test"},{"location":"tutorials/oculus_go_preparing/#leaving-a-presentation","text":"To leave an ongoing presentation, follow these steps: Press the Back button (arrow to left) from your Oculus Go headset's hand remote controller. A confirmation dialog will appear. After selecting OK , the headset will disconnect from the control device and return to Home . Simultaneously, the headset will disappear from the control device's Mosaic and Devices tabs. Other connected devices (if any) are not affected. Note The presenter can stop the presentation from the control device by pressing the Home icon (top left corner). A confirmation dialog will appear before the presentation is stopped. This method disconnects all connected viewing devices.","title":"Leaving a presentation"},{"location":"tutorials/oculus_go_preparing/#copying-content-files","text":"The LiveSYNC app contains sample files for testing. You can, of course, present also your own content. To do this, copy your content files (photos, videos, hotspot icons) to each device that will be used in your presentation . This means the control device and every viewing device. Read more from the User Guide . Next, we will go through how to copy content files to your Oculus Go headset.","title":"Copying content files"},{"location":"tutorials/oculus_go_preparing/#windows-pc","text":"Connect your Oculus Go headset to your PC using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Windows detects the headset and you choose to handle this device by exploring its files, it appears as a new device in Explorer . For example, on Windows 10 you'll find it under This PC -> VR-Headset . Copy your content files and folders to your Oculus Go just like you'd copy them into a USB flash drive. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your PC does not recognize the headset when you connect it with a USB cable, check that Developer Mode isn't enabled in your headset. On the Oculus companion app, navigate to Settings -> Your Oculus Go -> More Settings -> Developer Mode . Read more from Tips & Tricks . If it is enabled, the device will be detected in a different a USB mode, and it will not appear in Explorer. You can temporarily disable Developer Mode . Or, use the Android Debug Bridge (ADB) command line tool for transferring files.","title":"Windows PC"},{"location":"tutorials/oculus_go_preparing/#mac","text":"Install Android File Transfer app for transferring files: https://www.android.com/filetransfer/ Connect your Oculus Go headset to your Mac using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Android File Transfer detects the headset, a new file management window appears. Copy your content files and folders to your Oculus Go by dragging them from your Mac's Finder window. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your headset is not detected and you are using a USB hub or an extension cable, try connecting the headset's cable directly to your computer. If this doesn't help, try connecting the cable to a different USB port.","title":"Mac"},{"location":"tutorials/oculus_go_preparing/#storage-permission","text":"Check permissions in case you cannot find the LiveSYNC folder under Movies , or the LiveSYNC app cannot find your own files from that folder. /LiveSYNC folder is automatically generated when the app is run (if the folder is not found). To be able to create the folder under /Movies , the LiveSYNC app needs permission from the user to access the file system (read/write). This permission is first asked when you attempt to join a presentation. The permission remains until you manually remove it or re-install the app. You can also set the permission on your headset via Oculus Home as follows: From the bottom bar, select Navigate tab and then Library . From the left menu, select Apps . Find the LiveSYNC app from the app grid. Tap the three dots next to text LiveSYNC Oculus Go , and select Permissions from the appearing menu. Note the position of the switches. Both switches should be enabled: Location is required by Bluetooth connectivity and Storage for file access. Run the LiveSYNC app again to trigger folder creation. Note You can also use Explorer on Windows or Android File Transfer on Mac to create the folder yourself. LiveSYNC will still need the Storage permission for reading the files from this location.","title":"Storage Permission"},{"location":"tutorials/oculus_go_preparing/#file-naming-conventions","text":"The folder /Movies/LiveSYNC is the content root folder for the LiveSYNC app on all Android-based devices (Oculus Go runs on Android OS). When you copy your content files there, make sure to use exactly the same filenames that you use on your control device . For example, the names are case sensitive: MyVideo.mp4 is not the same as myvideo.mp4 . If the filenames do not match, the LiveSYNC app running on your Oculus Go headset cannot find the file that the control device tells it to load. When this happens, a textual error notification will appear on the headset and also in the control device's live view for this particular headset. Notice that while the filenames must match, the file content can be different. Example Matt is using an old iPad as a control device for his presentation. The iPad can play FullHD resolution videos, but not 4K. Matt encodes two different versions of his video: 1920x960 for his iPad and 3840x1920 for his Oculus Go. He renames the files so that they have the same filename Christmas.mp4 on his iPad and on his Oculus Go. LiveSYNC detects them as the same video and correctly loads them on both devices. You can copy your 360-degree photos and videos directly to /Movies/LiveSYNC folder, but you can also create one level of subfolders. This helps in organizing your content. Notice that if you use subfolders on your control device, you must use them also on your viewing devices. Example /Movies/LiveSYNC/Christmas2018 is a valid location for your 360-degree Christmas photos from 2018. /Movies/LiveSYNC/2018/Christmas has two directory levels under the LiveSYNC root folder and is thus invalid. Info With the LiveSYNC tool, you can present normal 2D photos and videos, 2D and 3D 360-degree photos and videos, and use your own tag images. File naming conventions allow LiveSYNC to correctly detect files. For example, whether an image file is a 2D photo, a 2D 360 photo, a 3D 360 photo, or a tag icon. These rules are described in the User Guide and they apply also to LiveSYNC on Oculus Go.","title":"File naming conventions"},{"location":"tutorials/oculus_go_presenting/","text":"LiveSYNC on Oculus Go \u00b6 Presenting \u00b6 Presenting content with Oculus Go headsets is like presenting with other viewing devices. Connecting \u00b6 The first step in presenting is to connect the devices to the same LiveSYNC channel . This allows the devices to communicate with each other ie. exchange command and status messages. Connect all viewing devices to the control device as follows: On your control device, start the LiveSYNC app. From the Home screen, select a channel that you want to use. The Mosaic view appears. The control device is now waiting for viewing devices to join the channel. On each of your viewing devices, start the LiveSYNC app. From the Home screen, select the same channel that you selected on the control device. The Lobby view appears. The viewing device is now looking for the control device. It will attempt to join the specified channel. Connections are created one by one. Live views from the viewing devices appear in the control device's Mosaic view. Wait until all connections are established and you have a live view from each viewing device. Connected devices appear in the Mosaic view. It is easy to observe what every member of the audience is looking at . Note The grid is automatically resized so that maximum amount of screen real estate is used. Playing media \u00b6 Once viewing devices are connected, play photo and video content as follows: On your control device, navigate to Player tab using the bottom bar. Drag a media item from the Content tab to the Presentation area and drop it there. The content will be loaded and rendered on the screen. All connected viewing devices will also load the same content and render it on screen. Video playback starts automatically when all connected devices are ready to play. You can pan the view by dragging with one finger inside the Presentation area . Zoom the view using the pinch gesture within the Presentation area . Notice that panning and zooming effect locally, not to other viewing devices. The audience will pan (and zoom on supported devices) by themselves. With VR headsets such as Oculus Go, panning is performed simply by turning one's head. When playing videos, a Control Panel appears at the bottom of the Presentation area . Play , Pause and Seekbar controls work just like in any other video player. It is also possible to enable Looping to continuously repeat the video. Keep an eye on the remaining playback time so that you know when it is time to change content. Change content whenever necessary. Simply drag another media item to the Presentation area . Existing photo or video will be immediately replaced with the new one. Drag a content item to the Presentation area . Control video playback via the Control Panel, which appears and disappears automatically . Marking with tags \u00b6 The presenter may want to draw the attention of the audience to a specific area of the content being played. Tags can be used for marking such points-of-interest during the presentation. You can use them as follows: Find a tag that you wish to use from the Tags tab. Drag a tag from the Tags tab to the Presentation area and drop it to the position where you want it to appear. The tag will stick to the content. It will appear also on the screens of the connected viewing devices. Drag a tag to the Presentation area to mark a point-of-interest . Observing & guiding \u00b6 Interaction and guidance are difficult when watching 360-degree content using separate viewing devices. How can you instruct if you cannot see the other guy's view? LiveSYNC solves the problem by providing live views on the control device's screen. It works as follows: Live view from each connected device appears in the Mosaic tab, and also in the Devices area of the Player tab. Make a quick glance to see if someone in the audience appears to need guidance. Or, if someone has already asked for help, find his headset based on the name given to that headset. The names appear over the live views. Assist the user. Observe from the live view coming from his headset that the problem is solved. For example, if someone in the audience cannot find a point-of-interest, the presenter can instruct him to turn to the correct direction. If necessary, switch to Mosaic view and double tap a particular users' live view to make it full screen. In full-screen mode it is easier to follow another user's view. Double tap the view again to resize it back to normal size. Observing and guiding users is convenient using the Mosaic view . Disconnecting \u00b6 When your presentation is over, stop presenting as follows: Tap the Home icon at the top left corner of the screen. A confirmation dialog appears. Select OK to quit the presentation. You will return to the Home screen. All viewing devices will be disconnected from the control device. Close the LiveSYNC app on all devices and recharge the batteries. To end your presentation, tap the home icon and select OK .","title":"Presenting"},{"location":"tutorials/oculus_go_presenting/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_presenting/#presenting","text":"Presenting content with Oculus Go headsets is like presenting with other viewing devices.","title":"Presenting"},{"location":"tutorials/oculus_go_presenting/#connecting","text":"The first step in presenting is to connect the devices to the same LiveSYNC channel . This allows the devices to communicate with each other ie. exchange command and status messages. Connect all viewing devices to the control device as follows: On your control device, start the LiveSYNC app. From the Home screen, select a channel that you want to use. The Mosaic view appears. The control device is now waiting for viewing devices to join the channel. On each of your viewing devices, start the LiveSYNC app. From the Home screen, select the same channel that you selected on the control device. The Lobby view appears. The viewing device is now looking for the control device. It will attempt to join the specified channel. Connections are created one by one. Live views from the viewing devices appear in the control device's Mosaic view. Wait until all connections are established and you have a live view from each viewing device. Connected devices appear in the Mosaic view. It is easy to observe what every member of the audience is looking at . Note The grid is automatically resized so that maximum amount of screen real estate is used.","title":"Connecting"},{"location":"tutorials/oculus_go_presenting/#playing-media","text":"Once viewing devices are connected, play photo and video content as follows: On your control device, navigate to Player tab using the bottom bar. Drag a media item from the Content tab to the Presentation area and drop it there. The content will be loaded and rendered on the screen. All connected viewing devices will also load the same content and render it on screen. Video playback starts automatically when all connected devices are ready to play. You can pan the view by dragging with one finger inside the Presentation area . Zoom the view using the pinch gesture within the Presentation area . Notice that panning and zooming effect locally, not to other viewing devices. The audience will pan (and zoom on supported devices) by themselves. With VR headsets such as Oculus Go, panning is performed simply by turning one's head. When playing videos, a Control Panel appears at the bottom of the Presentation area . Play , Pause and Seekbar controls work just like in any other video player. It is also possible to enable Looping to continuously repeat the video. Keep an eye on the remaining playback time so that you know when it is time to change content. Change content whenever necessary. Simply drag another media item to the Presentation area . Existing photo or video will be immediately replaced with the new one. Drag a content item to the Presentation area . Control video playback via the Control Panel, which appears and disappears automatically .","title":"Playing media"},{"location":"tutorials/oculus_go_presenting/#marking-with-tags","text":"The presenter may want to draw the attention of the audience to a specific area of the content being played. Tags can be used for marking such points-of-interest during the presentation. You can use them as follows: Find a tag that you wish to use from the Tags tab. Drag a tag from the Tags tab to the Presentation area and drop it to the position where you want it to appear. The tag will stick to the content. It will appear also on the screens of the connected viewing devices. Drag a tag to the Presentation area to mark a point-of-interest .","title":"Marking with tags"},{"location":"tutorials/oculus_go_presenting/#observing-guiding","text":"Interaction and guidance are difficult when watching 360-degree content using separate viewing devices. How can you instruct if you cannot see the other guy's view? LiveSYNC solves the problem by providing live views on the control device's screen. It works as follows: Live view from each connected device appears in the Mosaic tab, and also in the Devices area of the Player tab. Make a quick glance to see if someone in the audience appears to need guidance. Or, if someone has already asked for help, find his headset based on the name given to that headset. The names appear over the live views. Assist the user. Observe from the live view coming from his headset that the problem is solved. For example, if someone in the audience cannot find a point-of-interest, the presenter can instruct him to turn to the correct direction. If necessary, switch to Mosaic view and double tap a particular users' live view to make it full screen. In full-screen mode it is easier to follow another user's view. Double tap the view again to resize it back to normal size. Observing and guiding users is convenient using the Mosaic view .","title":"Observing &amp; guiding"},{"location":"tutorials/oculus_go_presenting/#disconnecting","text":"When your presentation is over, stop presenting as follows: Tap the Home icon at the top left corner of the screen. A confirmation dialog appears. Select OK to quit the presentation. You will return to the Home screen. All viewing devices will be disconnected from the control device. Close the LiveSYNC app on all devices and recharge the batteries. To end your presentation, tap the home icon and select OK .","title":"Disconnecting"},{"location":"tutorials/oculus_go_setup/","text":"LiveSYNC on Oculus Go \u00b6 Setup \u00b6 The Oculus app \u00b6 When you receive your Oculus Go headset, unpack it and start the initial setup procedure. It might be a bit surprising, but you must install an application to your phone : To set up and connect your Oculus Go, you'll need to download the Oculus app on your supported mobile phone. With the Oculus app, you can set up your headset, browse VR games and apps and customize your device settings. (Oculus Go website) This accompanying app is mandatory during the initial setup of the headset. After that you will need it very rarely, if ever. If you plan to purchase multiple headsets, they can be all set up and configured from a single phone. Download the Oculus Go app from here: https://oculus.com/app Alternatively, you can use these direct links for Android and iOS . Note This is NOT the same app that you use with a GearVR headset. Both apps have the same name Oculus . You can differentiate them from the icon: GearVR app icon has text GearVR, Oculus Go app icon does not have any text. The Oculus Go accompanying app on an Android phone. Pairing & setup \u00b6 Follow these steps to pair your phone with the headset and go through the initial setup. Download the Oculus app using one of the links above, then launch the installed app. If you don't have an Oculus account yet, sign up first, and then log in. Once logged in, navigate to Settings , and select Pair New Headset . From Choose a Headset menu, select Oculus Go , and press Start Now . Turn on your Oculus Go device. Press Continue . Plug your Oculus Go into a power source. Press Continue . After search completes, found headsets are listed. If multiple devices were found nearby, choose the one with the matching serial number. The number is printed below a QR code under the fabric of the head strap, near the USB connector. Press Continue . Select a Wifi access point that will be used for connecting the headset to the network. Press Continue . Put a battery into the controller and choose with which hand you want to use your controller. Press Continue . Select language to use in VR. Press Continue . Add a payment method (a credit card or a PayPal account), or press Skip . You will need a payment method only for making purchases from the Oculus Store. Go through safety etc. information. Press Continue . When ready, you will see a dialog saying Preparing your Oculus Go... . Once it has finished, pairing and setup has completed. For more information, see the questions and answers here . Once pairing and setup has completed your headset(s) will appear in the Settings tab. First time use \u00b6 Go to your Oculus Go device. Make sure that power is on and then put the headset on your face. There is an infrared proximity sensor inside the headset (between the lenses). The headset will wake up automatically and you will hear a greeting sound. Adjust the head strap so that you feel comfortable. Grab the hand remote controller, look straight ahead, and point with the controller to the direction you are looking at. Then, press and hold the Oculus button (the one with the ring symbol) on the controller. This procedure will calibrate the controller. Whenever the device wakes up from sleep you must calibrate the hand remote controller. Note Every time the headset wakes up from sleep it will present the calibration dialog . Calibration is necessary, because the headset and the hand remote controller are two independent devices. They are both only aware of rotation around their own center point . Moreover, the devices do not know what actual direction they are pointing at (for example, North, South-East, etc.). Hence, they need to be calibrated by making them point to the same direction and then pressing a button to signal this. Unfortunately, this needs to be repeated every time the headset wakes up. Sensors are turned off to save power during sleep, hence they will lose the tracking. After calibration and first time use tutorial you will enter Oculus Home . Here you can install new apps, start installed apps, configure the headset, and use services offered by Oculus. The Oculus Home. Note The image above is a screenshot from the headset's internal display. The device renders a heavily distorted double barrel view on the screen. You will not see the barrels nor the distortion when you wear the headset and look through the lenses. In addition, your brain will combine left and right eye images together. You will experience VR as a 3D world, although your field-of-view in VR is narrower than in real life. In this documentation, we frequently use either double barrel view or single barrel view in screenshots from a VR headset. A single barrel view is simply a double barrel view cut in half to save screen space. Tip We recommend that you spend some time to play around with the device. Get familiar with the hand remote and Oculus Home. Try a couple of pre-installed apps. Ask your boss a permission to take it home for the weekend and watch a couple of movies from Netflix or Youtube VR. Installing LiveSYNC \u00b6 Once you feel familiar with Oculus Go, it is time to install the LiveSYNC app. The installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). Info LiveSYNC has not officially launched on Oculus platform yet. However, you can start using it already by installing it from our beta channel. Follow the steps below. Check your Oculus username and email. Start the Oculus accompanying app on your phone, navigate to Settings , and find the username and email (see the image below): Installing software from Oculus beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for Oculus Go. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an invite email from Oculus to that email address, accept the invite by clicking a confirmation link in the email. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options how to install the application: Using the accompanying Oculus app on your phone: tap the magnifier class icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Install on ... button. Using the Oculus Go device, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu. Start LiveSYNC Oculus Go by selecting it from the apps grid. Note Notice that the app's version number appears in the splash screen, in the small print below the LiveSYNC logo. Here you can easily check which version you are running. The LiveSYNC app is updated frequently.","title":"Setup"},{"location":"tutorials/oculus_go_setup/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_setup/#setup","text":"","title":"Setup"},{"location":"tutorials/oculus_go_setup/#the-oculus-app","text":"When you receive your Oculus Go headset, unpack it and start the initial setup procedure. It might be a bit surprising, but you must install an application to your phone : To set up and connect your Oculus Go, you'll need to download the Oculus app on your supported mobile phone. With the Oculus app, you can set up your headset, browse VR games and apps and customize your device settings. (Oculus Go website) This accompanying app is mandatory during the initial setup of the headset. After that you will need it very rarely, if ever. If you plan to purchase multiple headsets, they can be all set up and configured from a single phone. Download the Oculus Go app from here: https://oculus.com/app Alternatively, you can use these direct links for Android and iOS . Note This is NOT the same app that you use with a GearVR headset. Both apps have the same name Oculus . You can differentiate them from the icon: GearVR app icon has text GearVR, Oculus Go app icon does not have any text. The Oculus Go accompanying app on an Android phone.","title":"The Oculus app"},{"location":"tutorials/oculus_go_setup/#pairing-setup","text":"Follow these steps to pair your phone with the headset and go through the initial setup. Download the Oculus app using one of the links above, then launch the installed app. If you don't have an Oculus account yet, sign up first, and then log in. Once logged in, navigate to Settings , and select Pair New Headset . From Choose a Headset menu, select Oculus Go , and press Start Now . Turn on your Oculus Go device. Press Continue . Plug your Oculus Go into a power source. Press Continue . After search completes, found headsets are listed. If multiple devices were found nearby, choose the one with the matching serial number. The number is printed below a QR code under the fabric of the head strap, near the USB connector. Press Continue . Select a Wifi access point that will be used for connecting the headset to the network. Press Continue . Put a battery into the controller and choose with which hand you want to use your controller. Press Continue . Select language to use in VR. Press Continue . Add a payment method (a credit card or a PayPal account), or press Skip . You will need a payment method only for making purchases from the Oculus Store. Go through safety etc. information. Press Continue . When ready, you will see a dialog saying Preparing your Oculus Go... . Once it has finished, pairing and setup has completed. For more information, see the questions and answers here . Once pairing and setup has completed your headset(s) will appear in the Settings tab.","title":"Pairing &amp; setup"},{"location":"tutorials/oculus_go_setup/#first-time-use","text":"Go to your Oculus Go device. Make sure that power is on and then put the headset on your face. There is an infrared proximity sensor inside the headset (between the lenses). The headset will wake up automatically and you will hear a greeting sound. Adjust the head strap so that you feel comfortable. Grab the hand remote controller, look straight ahead, and point with the controller to the direction you are looking at. Then, press and hold the Oculus button (the one with the ring symbol) on the controller. This procedure will calibrate the controller. Whenever the device wakes up from sleep you must calibrate the hand remote controller. Note Every time the headset wakes up from sleep it will present the calibration dialog . Calibration is necessary, because the headset and the hand remote controller are two independent devices. They are both only aware of rotation around their own center point . Moreover, the devices do not know what actual direction they are pointing at (for example, North, South-East, etc.). Hence, they need to be calibrated by making them point to the same direction and then pressing a button to signal this. Unfortunately, this needs to be repeated every time the headset wakes up. Sensors are turned off to save power during sleep, hence they will lose the tracking. After calibration and first time use tutorial you will enter Oculus Home . Here you can install new apps, start installed apps, configure the headset, and use services offered by Oculus. The Oculus Home. Note The image above is a screenshot from the headset's internal display. The device renders a heavily distorted double barrel view on the screen. You will not see the barrels nor the distortion when you wear the headset and look through the lenses. In addition, your brain will combine left and right eye images together. You will experience VR as a 3D world, although your field-of-view in VR is narrower than in real life. In this documentation, we frequently use either double barrel view or single barrel view in screenshots from a VR headset. A single barrel view is simply a double barrel view cut in half to save screen space. Tip We recommend that you spend some time to play around with the device. Get familiar with the hand remote and Oculus Home. Try a couple of pre-installed apps. Ask your boss a permission to take it home for the weekend and watch a couple of movies from Netflix or Youtube VR.","title":"First time use"},{"location":"tutorials/oculus_go_setup/#installing-livesync","text":"Once you feel familiar with Oculus Go, it is time to install the LiveSYNC app. The installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). Info LiveSYNC has not officially launched on Oculus platform yet. However, you can start using it already by installing it from our beta channel. Follow the steps below. Check your Oculus username and email. Start the Oculus accompanying app on your phone, navigate to Settings , and find the username and email (see the image below): Installing software from Oculus beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for Oculus Go. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an invite email from Oculus to that email address, accept the invite by clicking a confirmation link in the email. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options how to install the application: Using the accompanying Oculus app on your phone: tap the magnifier class icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Install on ... button. Using the Oculus Go device, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu. Start LiveSYNC Oculus Go by selecting it from the apps grid. Note Notice that the app's version number appears in the splash screen, in the small print below the LiveSYNC logo. Here you can easily check which version you are running. The LiveSYNC app is updated frequently.","title":"Installing LiveSYNC"},{"location":"tutorials/oculus_go_tips/","text":"LiveSYNC on Oculus Go \u00b6 Tips & tricks \u00b6 To conclude the tutorial, we will present a set of handy tips. Labeling \u00b6 We highly recommend that you mark your devices with labels for the following reasons: You can easily distinguish your headsets from each other, for example, to find a particular headset's live view from the Mosaic tab. You can recognize which hand remote belongs to which headset, for example when you store them together. You can get back your headsets (and content!) after a presentation elsewhere, for example when you use them in a trade show or borrow to a colleague for his presentation. You can brand them with your company/team/project logo. You can make the headsets look fancier with a bit of decoration. In general, two-three labels are placed in different positions: Oculus is using the left side of the head strap - under the fabric - for a serial number and other product info, which you should not cover. However, the right side is empty. When you place a label there, it will be hidden during normal product use. Yet, it is easy to check when you know where to look from. This is a great place to put your owner identification label. The front side of the headset has a large solid surface that is clearly visible when the headset is being used. Obviously, this is where your company logo or fancy decoration is typically placed. Warning The headset uses its front surface for head dissipation . Do not block the airflow or you will risk running your headset too hot. It will stop and you must wait until it cools down. LiveSYNC allows giving each device a name ( LiveSYNC name ). This is shown for example in the Mosaic view. It is a good idea to print a label that contains this name and put it in a place where you can easily see it when the headset is being used. If you have a small number of headsets, consider using color coding. Add something with red color to the head strap of one headset and set its name to \"Red\". This way you can find the correct headset from the Mosaic view easily. Even if it is too far for reading a textual label, or if the user is looking away from you and you cannot see the front side. Pull the hand remote's battery cover open, and put an identification label there. This way you can easily check which hand remote belongs to which headset. Labeling examples . Hand remote controllers \u00b6 Oculus Go headset is designed to be used with a hand-held remote controller. Its main purpose is to allow selecting items by pointing at them. In most presentations, users have no use for the controller. The presentation is controlled via the control device and users do not need to make any selections. In such a case, you may want to consider collecting all the remote controllers away . This way there will be much less hassle and no risk of someone forgetting the controller in his pocket. Remember that you still need to keep the remotes at hand, though. Whenever a headset is wakened up from sleep, its hand remote must be calibrated. This is a requirement of the Oculus platform. An assistant or the presenter himself can do this by pressing and holding the Oculus button on each hand remote one at a time. Notice that proper calibration by pointing at the correct direction is not necessary if the remotes will not be used for selections. To get rid of the calibration dialog after waking up the headset, all that is needed is a long press of the Oculus button. Note This is a known issue for Oculus. They have said that they are working on a solution to allow using the headset without performing hand remote calibration each time. However, a fix has not been released yet. Tip The Developer Mode has an interesting shortcut: to skip hand remote calibration, the user can press one of the volume keys (+/-) when the dialog appears. It is not a perfect solution as action is still required for each headset, but at least you don't need to keep the remote controllers around. Read more about the Developer Mode below. Oculus Go headset has three buttons. From left to right, Power, Volume Up, Volume Down. In Developer Mode, hand remote calibration can be skipped by pressing Volume Up/Down button . Power management \u00b6 We recommend that you increase the headset's timeout for dropping into sleep : Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Power Settings . Select Auto Sleep and set it to 5 minutes or Never . Reduce the frequency of hand remote calibration by adjusting the headset's sleep timeout . Tip You can also cover the proximity sensor (between the lenses) with a piece of tape. However, configuring Auto Sleep time works very well and we recommend that you use that option. Tip If you carry your headset in a bag in a sleep mode, you may also want to consider disabling Auto Wake-Up feature. The head strap tends to find its way in front of the proximity sensor, turn the headset on every now and then inside your bag, and drain the battery. Of course, you can also turn off the headset when you are not using it. Warning The headset uses its front surface for head dissipation . When you place the headset on a table, try to avoid placing its front side towards the table surface. It is tempting because in this way the headset is in a good balance. However, head dissipation suffers because there is no air flow around the front surface. Developer Mode \u00b6 You can easily activate and deactivate the Developer Mode for each headset from the Oculus accompanying app on your phone. To do this you must first create a developer organization (real or fake) on the Oculus Dashboard . After that, follow these steps: Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Developer Mode . Disable or enable the feature. Read more from here . Enable/disable Developer Mode by turning the switch from the Oculus accompanying app . When the Developer Mode is enabled, you can: Skip hand remote calibration by pressing volume +/- key from the headset . Copy content files back and forth using Android Debug Bridge (ADB) tool from the command line. This can be handy if you're into automating things by writing scripts. Sideload apps using the Android Debug Bridge (ADB) tool from the command line. Note When the Developer Mode is enabled, the device connects via USB cable in a different mode. It will NOT appear in the Windows Explorer. Automation \u00b6 The LiveSYNC app on Oculus Go supports a configuration file . The filename must be settings.ini and it must be copied to \\Movies\\LiveSYNC folder on the headset. (This is the same location where you will copy your own content files.) A configuration file is written using Windows INI file format. Here is an example of contents for a configuration file: [defaults] ; automatically opens given channel on startup, true/false default_channel_on_startup=true ; channel number to open, between 1000-9999 default_channel=1000 ; connection type to use (bluetooth or globalsync) default_connection_type=bluetooth ; LiveSYNC username to use, can be left blank default_livesync_name=Joan's This particular configuration would work as follows: When the LiveSYNC app is started, it will automatically move from Splash screen to Lobby . The user does not need to select a channel number from the Home screen. The app will attempt to connect using Bluetooth on channel number 1000. Once connected to the control device, the headset will appear as Joan's in the control device's Mosaic and Devices views. Note The configuration file must be saved as plain text. Do not save it in a format that adds formatting characters. For example, you can use the Notepad application on Windows.","title":"Tips"},{"location":"tutorials/oculus_go_tips/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_tips/#tips-tricks","text":"To conclude the tutorial, we will present a set of handy tips.","title":"Tips &amp; tricks"},{"location":"tutorials/oculus_go_tips/#labeling","text":"We highly recommend that you mark your devices with labels for the following reasons: You can easily distinguish your headsets from each other, for example, to find a particular headset's live view from the Mosaic tab. You can recognize which hand remote belongs to which headset, for example when you store them together. You can get back your headsets (and content!) after a presentation elsewhere, for example when you use them in a trade show or borrow to a colleague for his presentation. You can brand them with your company/team/project logo. You can make the headsets look fancier with a bit of decoration. In general, two-three labels are placed in different positions: Oculus is using the left side of the head strap - under the fabric - for a serial number and other product info, which you should not cover. However, the right side is empty. When you place a label there, it will be hidden during normal product use. Yet, it is easy to check when you know where to look from. This is a great place to put your owner identification label. The front side of the headset has a large solid surface that is clearly visible when the headset is being used. Obviously, this is where your company logo or fancy decoration is typically placed. Warning The headset uses its front surface for head dissipation . Do not block the airflow or you will risk running your headset too hot. It will stop and you must wait until it cools down. LiveSYNC allows giving each device a name ( LiveSYNC name ). This is shown for example in the Mosaic view. It is a good idea to print a label that contains this name and put it in a place where you can easily see it when the headset is being used. If you have a small number of headsets, consider using color coding. Add something with red color to the head strap of one headset and set its name to \"Red\". This way you can find the correct headset from the Mosaic view easily. Even if it is too far for reading a textual label, or if the user is looking away from you and you cannot see the front side. Pull the hand remote's battery cover open, and put an identification label there. This way you can easily check which hand remote belongs to which headset. Labeling examples .","title":"Labeling"},{"location":"tutorials/oculus_go_tips/#hand-remote-controllers","text":"Oculus Go headset is designed to be used with a hand-held remote controller. Its main purpose is to allow selecting items by pointing at them. In most presentations, users have no use for the controller. The presentation is controlled via the control device and users do not need to make any selections. In such a case, you may want to consider collecting all the remote controllers away . This way there will be much less hassle and no risk of someone forgetting the controller in his pocket. Remember that you still need to keep the remotes at hand, though. Whenever a headset is wakened up from sleep, its hand remote must be calibrated. This is a requirement of the Oculus platform. An assistant or the presenter himself can do this by pressing and holding the Oculus button on each hand remote one at a time. Notice that proper calibration by pointing at the correct direction is not necessary if the remotes will not be used for selections. To get rid of the calibration dialog after waking up the headset, all that is needed is a long press of the Oculus button. Note This is a known issue for Oculus. They have said that they are working on a solution to allow using the headset without performing hand remote calibration each time. However, a fix has not been released yet. Tip The Developer Mode has an interesting shortcut: to skip hand remote calibration, the user can press one of the volume keys (+/-) when the dialog appears. It is not a perfect solution as action is still required for each headset, but at least you don't need to keep the remote controllers around. Read more about the Developer Mode below. Oculus Go headset has three buttons. From left to right, Power, Volume Up, Volume Down. In Developer Mode, hand remote calibration can be skipped by pressing Volume Up/Down button .","title":"Hand remote controllers"},{"location":"tutorials/oculus_go_tips/#power-management","text":"We recommend that you increase the headset's timeout for dropping into sleep : Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Power Settings . Select Auto Sleep and set it to 5 minutes or Never . Reduce the frequency of hand remote calibration by adjusting the headset's sleep timeout . Tip You can also cover the proximity sensor (between the lenses) with a piece of tape. However, configuring Auto Sleep time works very well and we recommend that you use that option. Tip If you carry your headset in a bag in a sleep mode, you may also want to consider disabling Auto Wake-Up feature. The head strap tends to find its way in front of the proximity sensor, turn the headset on every now and then inside your bag, and drain the battery. Of course, you can also turn off the headset when you are not using it. Warning The headset uses its front surface for head dissipation . When you place the headset on a table, try to avoid placing its front side towards the table surface. It is tempting because in this way the headset is in a good balance. However, head dissipation suffers because there is no air flow around the front surface.","title":"Power management"},{"location":"tutorials/oculus_go_tips/#developer-mode","text":"You can easily activate and deactivate the Developer Mode for each headset from the Oculus accompanying app on your phone. To do this you must first create a developer organization (real or fake) on the Oculus Dashboard . After that, follow these steps: Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Developer Mode . Disable or enable the feature. Read more from here . Enable/disable Developer Mode by turning the switch from the Oculus accompanying app . When the Developer Mode is enabled, you can: Skip hand remote calibration by pressing volume +/- key from the headset . Copy content files back and forth using Android Debug Bridge (ADB) tool from the command line. This can be handy if you're into automating things by writing scripts. Sideload apps using the Android Debug Bridge (ADB) tool from the command line. Note When the Developer Mode is enabled, the device connects via USB cable in a different mode. It will NOT appear in the Windows Explorer.","title":"Developer Mode"},{"location":"tutorials/oculus_go_tips/#automation","text":"The LiveSYNC app on Oculus Go supports a configuration file . The filename must be settings.ini and it must be copied to \\Movies\\LiveSYNC folder on the headset. (This is the same location where you will copy your own content files.) A configuration file is written using Windows INI file format. Here is an example of contents for a configuration file: [defaults] ; automatically opens given channel on startup, true/false default_channel_on_startup=true ; channel number to open, between 1000-9999 default_channel=1000 ; connection type to use (bluetooth or globalsync) default_connection_type=bluetooth ; LiveSYNC username to use, can be left blank default_livesync_name=Joan's This particular configuration would work as follows: When the LiveSYNC app is started, it will automatically move from Splash screen to Lobby . The user does not need to select a channel number from the Home screen. The app will attempt to connect using Bluetooth on channel number 1000. Once connected to the control device, the headset will appear as Joan's in the control device's Mosaic and Devices views. Note The configuration file must be saved as plain text. Do not save it in a format that adds formatting characters. For example, you can use the Notepad application on Windows.","title":"Automation"},{"location":"tutorials/tutorials/","text":"Tutorials \u00b6 LiveSYNC on Oculus Go \u00b6 This tutorial focuses on using LiveSYNC on Oculus Go standalone VR headset.","title":"Index"},{"location":"tutorials/tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/tutorials/#livesync-on-oculus-go","text":"This tutorial focuses on using LiveSYNC on Oculus Go standalone VR headset.","title":"LiveSYNC on Oculus Go"},{"location":"user_guide/asset_management/","text":"4. Managing Assets \u00b6 Basics \u00b6 With LiveSYNC, you can present various kinds of content to other people. Before your presentation, the content files must be manually copied to specific locations. This way LiveSYNC can find and access them during the presentation. The content files are called assets . In this chapter, we will go through various methods and strategies for managing your assets. Tip LiveSYNC contains free content samples. These are bundled with the app. You can study and evaluate the app using the demo content. Return here when you want to learn how to present your own files. Assets \u00b6 In short, a presentation's assets are all the files that you will need in your presentation . They include for example: 360-degree photos 360-degree videos Ordinary 2D photos Ordinary 2D videos Slide images Custom tag and hotspot icons Video stream configuration files Project files 2D map images 3D map models Camera path files You can combine different kinds of assets in your presentation. They can be partially the same as other presentations and partially different. Big Screen \u00b6 With LiveSYNC, you can use a common big screen device for showing content to an audience. This can be for example a TV or a projector. When you present content via the big screen, copy your presentation's assets to the control device. There is no need to copy them anywhere else. The view from your control device is mirrored to the big screen as a video stream, not as command messages. For example, you can connect an iPad to a TV using a standard HDMI cable (you will also need an adapter from Apple). Note Some modern TVs and projectors support playing photos and videos from a USB memory stick. You may wonder if that approach can be used for showing also 360-degree content. Technically, the photos and videos can be played. However, built-in players do not understand special projections used in 360-degree content. In addition, they do not provide interactivity ie. ways for panning and zooming 360-degree content. Hence, this approach does not really work. Viewing Devices \u00b6 LiveSYNC supports presenting content also via personal viewing devices. For example, phones, tablets, and VR headsets. You may wonder where the content comes from in this case. Is it streamed from the control device to all the audience devices? Is it streamed from a local server or a cloud service? Or is it played from local file copies on each device? LiveSYNC supports more than one option. However, in most cases, the recommended solution is to use local file copies. Streaming high-resolution video to many devices tends to congest local network connection. With local file copies, the user experience is great. And you don't need to struggle with network bandwidth issues. When you present content via viewing devices, copy your presentation's assets to the control device AND all viewing devices . Tip This chapter focuses on local file playback approach. If you want to learn how to use streaming with LiveSYNC, you can read about it from here . File Storage Strategy \u00b6 An app that supports embedding images, videos, and sounds needs a strategy for handling multimedia files. This decision is fundamental. The chosen strategy defines how easy it is to manage your project with that app. For example, to share a finished work, store multiple versions of it, or to make some last-minute modifications. You have probably created presentations using PowerPoint or Keynote application. These tools make adding images easy. Have you ever thought about what happens in the background when you drag an image onto a slide? When you save the presentation, the output will be a single file. This file contains everything that belongs to your presentation, including all the images. A single file is easy to manage on disk and share via email. However, as everything is embedded file size can easily grow to several megabytes. Sometimes, tens of megabytes. What would happen if you added a couple of high-resolution videos? Single-file strategy becomes unpractical when the file size becomes too large. Hence, applications that make extensive use of video content choose a different strategy. The project is not stored as a single file that contains everything. Instead, all multimedia files are stored as individual files. A light-weight project file references to them - it does not embed them. This approach has many benefits, but also one drawback. A project becomes partially broken if any of the referenced files are not present. Hence, moving the files around or renaming them frequently should be avoided. Also, LiveSYNC uses this approach. A presentation consists of multiple smaller files that you need to copy in place. Not just one huge file . Central Media Repository \u00b6 Previously, we explained why LiveSYNC presentations consist of multiple files. A logical follow-up question is this: should a presentation's files be kept in a single location, or scattered around the file system? Many platforms have built-in locations for photos, videos, and music. Should we copy presentation's photos to the device's Photos folder, videos to Videos folder, and so on? The short answer is: no. If a presentation's assets were scattered around the file system, managing them would become a nightmare. Especially with a large number of devices. 360-degree content would show up in other photo and video applications that do not know how to play them. All ordinary 2D photos and videos would be mixed with 360-degree content in LiveSYNC. To avoid confusion, LiveSYNC uses its own central media repository. It is a single location for storing all kinds of assets. This is not the same path where content captured with the device's camera appears. Sometimes, we do want to include ordinary photos and videos into our presentations. LiveSYNC allows this. For example, on iPad, the contents of the device's Camera Roll appear as a subdirectory. It is not a real subdirectory in LiveSYNC's media repository. Consider it as a virtual link to the Camera Roll directory. It is physically located elsewhere in the file system. File Access Permissions \u00b6 On personal computers, almost the whole file system is accessible for applications. A file manager application is an essential, built-in part of the desktop environment. On Windows, every user is experienced in managing files via Explorer . On Mac, Finder is your best friend. This is different on mobile platforms such as iOS, Android and Oculus, where applications are sandboxed . Sandboxing means that the platform enforces security by isolating apps from each other. The same approach is applied to the file system: apps see only their own files by default. This has consequences to media applications such as LiveSYNC. On Android and Oculus, a user can give * permission* for a particular app for accessing files in the common files area. This is useful in many ways. For instance, one can use different apps for playing a video file that is located in the common files area. The user can also use a file manager app for moving, copying, and renaming files. On Android and Oculus, LiveSYNC uses a directory in the common files area as the root of its media repository. Permission for accessing the file system is asked from the user. This permission also allows reading the device's photos and videos folders. Hence, ordinary 2D content captured with the device's camera can be used in presentations. iOS does not have a similar concept. The apps can access only their own files. You cannot open the gates by giving permission, not even if you wanted to. On iOS, LiveSYNC uses its own file sharing directory as the root of its media repository . iOS provides an interface for 3rd party apps for accessing the photos folder. LiveSYNC supports this feature if you give it permission. Hence, ordinary 2D content captured with the device's camera can be used in presentations also on iOS. File System Limitations \u00b6 There are some common pitfalls when dealing with large video files in presentations. To avoid them, we need to understand technical and practical limitations. Some of them come from the file system . All the assets that belong to a presentation must be stored as files on a mass storage device. This might be the hard drive of the computer where you edit the files. Or, the internal mass memory of the tablet you use for presentations. Or, a USB memory stick that you use for quickly copying files to multiple VR headsets. Or, an SD card that you use for expanding your Android tablet's storage capacity. All these devices use a file system for managing the files. It defines how the total size of the mass memory is divided into smaller blocks, and how these blocks are allocated for individual files. The type of file system sets some important hard limits. One of them is the total number of bytes that a drive can store ie. max volume size in bytes . This is rarely a bottleneck nowadays. Theoretical limits are far higher than current storage capacities. Another limitation is the maximum size that a single file can be ie. max file size in bytes . This is important when you deal with large video files. None of your files can exceed the file system's max file size limit. Often you encounter problems with files that are over 4 GB in size. Another common problem is that a device does not have enough free space for the presentation's files. Example Jean has purchased a cheap Android tablet. It has 16 GB internal storage capacity. Android operating system and bundled factory apps (which cannot be removed) consume 6 GB. Other apps and Jean's photos, music, video clips, and web browser cache consume another 4 GB. Jean has only 6 GB left. Thus, the total size of all the files she needs in her presentation must not exceed 6 GB. Else, she will need to clean up her tablet to release some memory. Jean regrets not buying a 32 GB model. Based on a tip from her friend, Jean buys a 16 GB SD card to expand the storage capacity of her Android tablet. The SD card is formatted with the FAT32 file system. FAT32 is often used on older hard disks, USB flash drives, and memory cards. It is fairly well supported on different devices. The theoretical size of a FAT32 formatted drive could be as large as 8 TB (eight terabytes). Enough for 300 hours of 4K video captured with an iPhone. However, the size of each individual file may not exceed 4 GB (four gigabytes). That is only 10 minutes of 4K video captured with an iPhone. After adding the SD card, Jean has 16 GB more storage space. Each of her video files must stay below the 4 GB limit. But, one of Jean's videos is 6.5 GB. It is a high-resolution video that she wishes to present using an Oculus Go headset. She plans to control the presentation with her Android tablet. Jean has several options for overcoming her SD card's 4 GB max file size limitation: a) She can try to format her SD card with the exFAT file system. If her tablet recognizes the card, the problem is solved. exFAT is a newer file system and has much larger limits. 6.5 GB file will work easily. b) She can make space on her tablet's internal memory and copy the file there instead. That drive is very likely formatted with a different file system and does not have the 4 GB limit. c) She can encode a smaller version (low resolution/bitrate) version of the video and use that in her tablet. She can still use the high-resolution version on her Oculus Go headset. The filenames must match so that LiveSYNC can identify them as the same video. d) She can use a video editor application to split her video to shorter clips, each below the 4GB limit. During the presentation, she will simply start the next clip when the previous one ends. Tip On iOS devices, you cannot expand the storage capacity with an SD card. There are memory sticks intended for iOS devices, but they only work with the manufacturer's own app - not LiveSYNC. When purchasing a new iPhone or iPad, invest in a model that has enough built-in storage space for your needs. On some Android devices, you can use an SD card for expanding the storage capacity. When purchasing a new Android phone or tablet, check if it supports an SD card. If not, choose another model or invest in a model that has enough built-in storage capacity. We recommend purchasing a device that has at least 32 GB of internal storage space. Tip If you have videos that are larger than 4 GB, you may encounter problems in storing or transferring them. Usually, such problems can be solved. You might be able to simply reformat the storage device with another file system type (such as exFAT). Beware that formatting destroys all the data! Using a different device for storing or transferring the files often helps. You can also try transferring the files via cable or via a network connection. If nothing else helps, consider splitting a large file to multiple smaller ones (multiple video clips). Or, encode your video again to lower resolution and lower bitrate to create a smaller file. Dealing With Redundancy \u00b6 Video files consume a lot of storage space. Thus, it is important to learn to fight against unintended redundancy . Don't get this wrong: creating a backup is always a good idea. That is intended redundancy. However, when you are busy preparing for a presentation, it is easy to forget that you have one version of a video clip here and another there. Files that consume hundreds of megabytes or even multiple gigabytes quickly add up. And cleaning is laborious! One strategy is to create a new folder on your computer. This folder will be the master directory of your presentation. It should contain all the necessary assets and nothing else. Keep it tidy: delete old and extra versions immediately. Create backups by copying the whole folder e.g. to an external drive. Delete old content on your presentation devices. Copy the contents of your master directory there as-is. The idea is to focus on this one presentation and keep its assets in one directory. This approach works well if you use shared devices that must be cleared frequently. The approach described above is not suitable for all cases. Some users have presentations often and reuse material from previous presentations. Creating a new presentation folder every time would mean redundant copies of the same files. Another strategy is to create one master directory and many subfolders. Folder titles can be for example dates or topics, whatever suits your situation. You will quickly get familiar with your own content archive. During a presentation, you can adapt to the audience as you go and present content directly from your content archive. Maintain a backup of the whole archive on an external drive. Copy new files to your presentation devices incrementally. This approach works well if you have a set of devices reserved for your own use. A third option is a hybrid one. Build a content archive as explained above. Create a subfolder for a particular presentation by copying the necessary assets there. This means that there will be multiple copies of some of the files, but only for a while. After the presentation, simply delete its folder. All its assets are already in your archive folders. Note To keep the user interface simple, LiveSYNC supports only one level of subdirectories . We encourage you to make use of them. Choose one strategy for organizing your content, and stick to it. Transfer Times \u00b6 Sometimes, a large number of devices is used in a presentation. The time required to copy the files grows exponentially along the total file size and the number of devices . Unless you can do it in parallel using multiple copy stations. Even so, you should have the final version ready early enough so that there is enough time to copy the assets in place. Notice that last-minute changes can be problematic. Changing even one small icon requires going through all the devices again. Moreover, on iOS devices, you may have to copy the whole folder again even if you need to change a single file inside it. This is because Apple's iTunes application. It is used for copying files for iOS devices, but it has very limited file management capabilities. Example Lisa is a teacher in elementary school. They have a rack of 20 iPads, which any of the teachers can reserve for their class. Lisa is preparing a presentation for the parent's night. Her students have made a 360-degree video of their day at school. She plans to use the iPads to show the video to the parents. Lisa has a single presentation folder that contains one 3 GB video file and a few 360-degree photographs. Her students already copied the files to all iPads during the afternoon. Suddenly Liza realizes that their 360-degree group photo is missing. If Lisa adds the missing photo to her presentation folder, she must copy the whole folder again. This is because of limitations in Apple's iTunes application. She quickly calculates that copying 3 GB * 20 devices means 60 GB in total. That will take hours! Lisa decides to create another folder that contains only this one image. Since the image is just 5 MB in size, Lisa needs to copy 5 MB * 20 devices = 100 MB. She will be ready in 15 minutes. Which Files Must Be Copied? \u00b6 In principle, all files that you need in your presentation must be copied to the control device and to each viewing device (except the big screen). We recommend that you create a master directory on your computer. Collect all the assets there. Then, copy the contents of this directory to your control device and viewing devices. 360-Degree Photos \u00b6 360-degree photos in JPG or PNG format can be copied either to the root of the LiveSYNC media repository or in a subdirectory . You can freely choose the name for the subdirectory (with a couple exceptions shown below). Remember that LiveSYNC supports only one level of subdirectories. Examples of valid paths: LiveSYNC/my_image.jpg LiveSYNC/my_image.png LiveSYNC/Another Image.jpg LiveSYNC/January/Flowers.png LiveSYNC/Summer Holiday/At the beach.jpg Examples of invalid paths: LiveSYNC/my_image.tiff TIFF image format is not supported LiveSYNC/January/16/Flowers.png Only one subdirectory level is allowed LiveSYNC/slides_tuesday/my_image.jpg Do not put 360-degree content in a subdirectory that starts with \"slides_\" LiveSYNC/hotspots_tuesday/my_image.jpg Do not put 360-degree content in a subdirectory that starts with \"hotspots_\" LiveSYNC/documents/my_image.jpg Do not put 360-degree content in a subdirectory whose name is \"documents\" 360-Degree Videos \u00b6 360-degree videos in MP4 container can be copied either to the root of the LiveSYNC media repository or in a subdirectory . You can freely choose the name for the subdirectory (with a couple exceptions shown below). Remember that LiveSYNC supports only one level of subdirectories. Live 360-degree video streams can be configured as special .videostream config files. These files can be placed to same locations where local 360-degree video files are placed. Examples of valid paths: LiveSYNC/my_video.mp4 LiveSYNC/live.videostream LiveSYNC/Another Video.mp4 LiveSYNC/January/Skiing.mp4 LiveSYNC/Summer Holiday/At the beach.mp4 Examples of invalid paths: LiveSYNC/my_image.avi AVI video container is not supported LiveSYNC/January/16/Skiing.mp4 Only one subdirectory level is allowed LiveSYNC/slides_tuesday/my_video.mp4 Do not put 360-degree content in a subdirectory that starts with \"slides_\" LiveSYNC/hotspots_tuesday/my_video.mp4 Do not put 360-degree content in a subdirectory that starts with \"hotspots_\" LiveSYNC/documents/my_video.mp4 Do not put 360-degree content in a subdirectory whose name is \"documents\" Slides & 2D Content \u00b6 Slides exported as images can be shown with LiveSYNC. Also, other 2D images and videos can be shown. Currently, content files are detected as ordinary 2D files based on filename convention. All images and videos in folders that begin with \"slides_\" are played as ordinary 2D content. The prefix \"slides_\" is automatically stripped from the filename when the directory is shown. For example, \"slides_Tuesday\" appears as a folder \"Tuesday\" in LiveSYNC. Live 2D video streams can be configured as special .videostream config files. These files can be placed to same locations where local 2D video files are placed. Examples of valid paths: LiveSYNC/slides_Tuesday/my_2D_image.jpg LiveSYNC/slides_Holiday/my_2D_video.mp4 LiveSYNC/slides_Meeting/Slide 1.png Examples of invalid paths: LiveSYNC/slides_/Skiing.mp4 Do not use the prefix alone LiveSYNC/slides_Holiday/Tuesday/Skiing.mp4 Only one subdirectory is allowed. LiveSYNC/Meeting/Slide 1.png Always put 2D content in a subdirectory that starts with \"slides_\" Icons \u00b6 Custom tag and hotspot icons are small 2D images that appear as items that you can drag & drop onto 360-degree content. All images in folders that begin with \"hotspots_\" are treated as custom tag or hotspot icons. They appear in the Tags tab (instead of the Content tab). We recommend using PNG images with transparent background (alpha). Examples of valid paths: LiveSYNC/hotspots_signs/staff_only.png LiveSYNC/hotspots_prohibition/Do not enter.png Examples of invalid paths: LiveSYNC/hotspots_/No running.png Do not use the prefix alone LiveSYNC/hotspots_signs/Traffic/Stop.png Only one subdirectory is allowed. LiveSYNC/Meeting/Reserved.png Always put tag & hotspot icons in a subdirectory that starts with \"hotspots_\" Where Should My Files Go? \u00b6 iOS \u00b6 On iOS, LiveSYNC uses the app's own file sharing directory as the root of its media repository . The screen capture shows a view to it from the iTunes application. On this iPad, there are many folders that contain icons for tags and hotspots. These are the folders that start with hotspot_ prefix. There is also a 360-degree content folder OfficeTour2018 . And, a 2D content folder for the same tour: slides_OfficeTour2018 . Android \u00b6 On Android, LiveSYNC uses directory /Movies/LiveSYNC in the common files area as the root of its media repository . The screen capture shows many 360-degree content folders: an archive of 360-degree photos. There is also a license file ( license.key.lic ). This file must not be removed. Note In case you have a device that supports a memory card (SD card), the media files can be copied there, too. The logical path will be the same: /Movies/LiveSYNC is searched also from the SD card. In case you have the same filename in both directories the internal memory takes precedence. Oculus \u00b6 On Oculus, LiveSYNC uses directory /Movies/LiveSYNC in the common files area as the root of its media repository . This is exactly the same as on Android. The screen capture shows many 360-degree content folders: an archive of 360-degree photos. There is also a configuration file ( settings.ini ). This file is used for customizing the operation of the app. Note In case you have a GearVR compatible phone that supports a memory card (SD card), the media files can be copied there, too. The logical path will be the same: /Movies/LiveSYNC is searched also from the SD card. In case you have the same filename in both directories the internal memory takes precedence. How To Copy Files? \u00b6 Copying files back and forth between a computer and a mobile device is easy. The method depends on the platforms beings used: your computer's operating system and your mobile device's operating system. Common combinations are covered below. iOS \u00b6 Copying files from a computer to an iOS device requires using Apple's iTunes application. iTunes is available for Mac and Windows operating systems. It is not available for Linux. Option A: Watch a video guide Video not working? Click here to open it in Vimeo. Note Since the video was made, Apple has slightly changed the iTunes application. File Sharing is more easily accessible now. Instead of selecting Apps from the left side menu and scrolling down the page, simply choose File Sharing from that same menu. Option B: Follow the steps described below On your computer, open a web browser and go here to download the iTunes application: https://www.apple.com/itunes/download/ Based on your operating system, choose a download link either for macOS or Windows. Once the download has completed, install the application on your computer just like any other app. Once the installation has completed, start the iTunes application. Your iOS device came with a bundled USB to Lightning cable . It is the same one you use for charging your iOS device. Connect your iOS device to your computer using this cable. On your computer, take a look at the iTunes application. It begins to synchronize content between your iOS device and computer. Assets for the LiveSYNC app are not automatically synchronized. Notice the small device icon that appears in the top bar. Click it. Tip It may take a moment after connecting the cable until the device icon appears. If it doesn't appear, restart iTunes application. The Summary page of your device appears. From the left side menu, click File Sharing . A list of apps that use the file sharing feature appears. Select LiveSYNC from the list. A list of the LiveSYNC app's files appears. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC Documents area in the iTunes application, and drop it there. It will be copied to your iOS device. The progress will be shown in the top area of the iTunes application. Tip You may have noticed that there is Add... button at the bottom of the LiveSYNC Documents area. This is an alternative method for copying the files. However, we recommend using drag'n drop. Tip If you copy more content to your iOS device during a presentation , there is no need to restart the LiveSYNC app. Use pull-to-refresh gesture in the Content tab or in the Tags tab. The contents will be refreshed. Android \u00b6 Copying files from a computer to an Android device requires using Explorer on Windows or Android File Transfer on Mac. Option A: Watch a video guide Video not working? Click here to open it in Vimeo. Note The Android part starts at 3m 13sec. On some Android devices, micro USB has been replaced with USB-C. Use the cable that came with the device. Option B: Follow the steps described below Windows Your Android device came with a bundled USB to micro USB or USB to USB-C cable. It is the same one you use for charging your Android device. Connect your Android device to your computer using this cable. When you connect the cable, your Android device asks your permission for file access. Tap Allow to grant permission. On your computer, Windows has detected a new device. It may ask what action to perform; you want to explore its files. When Explorer opens, find your Android device from the list. Expand the folder structure until you see the contents of /Movies/LiveSYNC directory. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC directory in Explorer , and drop it there. It will be copied to your Android device. The progress will be shown in a separate dialog. Mac Unfortunately, macOS does not have built-in support for transferring files to Android devices. The solution is to install Android File Transfer application. It is made by Google and available for free. On your computer, open a web browser and go here to download the Android File Transfer application: https://www.android.com/filetransfer/ Once the download has completed, install the application on your computer just like any other app. Your Android device came with a bundled USB to micro USB or USB to USB-C cable. It is the same one you use for charging your Android device. Connect your Android device to your computer using this cable. When you connect the cable, your Android device asks your permission for file access. Tap Allow to grant permission. On your computer, the Android File Transfer application should start automatically. Expand the folder structure until you see the contents of /Movies/LiveSYNC directory. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. Note Android File Transfer often shows a warning dialog similar to the one below. It means that you have not granted permission for file access on your Android device. Do it now, then click OK. The dialog disappears and a moment later a working file manager window opens. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC directory in Android File Transfer , and drop it there. It will be copied to your Android device. The progress will be shown in a separate dialog. Note Reportedly, Android File Manager has trouble in transferring files over 4 GB in size. The easiest solution is to use a Windows computer. You can also transfer large files between Mac and Android using Android development tools. Android Debug Bridge (ADB) allows copying files back and forth from the command line. Oculus \u00b6 Supported Oculus devices (Samsung GearVR and Oculus Go) are both based on Android operating system. Hence, copying files to Oculus devices is the same as copying to other Android devices. Follow the instructions that were given for Android. Note In Step 3., you need to grant file access permission on your Android device. This also applies to Oculus Go. You must put the headset on your face and use the hand remote to select Allow . See the image below. Alternative Solutions \u00b6 Copying assets from your computer to your mobile devices is usually done via USB cable. Cable connection is fast and reliable. However, it is not the only way. Here we discuss some alternative file transfer methods. USB Memory Stick \u00b6 In this method, the idea is to copy the files from your computer to your mobile devices in two phases: In the first phase, plug the USB stick to your computer and copy all the assets there. In the second phase, plug the USB stick to your mobile device and copy all the assets from the stick to /Movies/LiveSYNC on the mobile device. Note This method works only with Android and Oculus (GearVR) devices. It does not work with iOS. Special USB memory sticks that are compatible with iOS devices do exist. But, they only work with the manufacturer's own app - not 3rd party apps such as LiveSYNC. This method is usually slower than using a USB cable, not faster. Yet, there is one important exception: it scales up much cheaper. When you need to copy files to a large number of viewing devices, a single computer becomes a bottleneck. But, USB memory sticks are cheap. If you buy ten memory sticks and copy your files there, then a group of ten assistants can continue copying the files simultaneously . This way files can be quickly updated to, say, one hundred devices. Another benefit is that the files can be copied from the USB stick to the mobile device without a computer. For example, a teacher can copy the files to a memory stick in advance. Then, she circulates the stick among her students. Everyone can copy the files to their own device at their own turn. This is much easier to manage than using a laptop for the same task. In USB standard, one device acts as a host and another one as a peripheral. Usually, USB is used for connecting phones and tablets to a computer. In this arrangement, the computer is the USB host and the mobile is peripheral. But, when you connect a USB stick directly to a mobile, it needs to change to the host mode. The solution is a standard called USB On-The-Go (OTG) . Thus, to connect a USB stick into your mobile, you will need to use a USB OTG adapter between. This is illustrated in the image below. A USB OTG adapter allows connecting ordinary USB sticks. Some phone manufacturers include a tiny USB OTG adapter as an accessory - you may already have one. Another alternative is to buy a dual connector USB memory stick, which has integrated USB OTG adapter. These handy devices allow plugging the other end to a computer for phase 1. Then, unplug it and plug to your mobile for phase 2. Separate OTG adapter is not needed. For copying files from the memory stick to /Movies/LiveSYNC on the mobile device, you will need a file manager application. Many phones and tablets have one built-in. You can also install many file managers for free from Google Play store (just search for file manager ). With a file manager app, you can perform the usual file management tasks such as copying, moving, renaming and deleting. Once you connect a USB memory stick to your phone, it appears next to the device's internal memory and SD card in the file manager. Notice that some file managers show USB sticks as SD cards, as in the image below. Windows Shares \u00b6 Some file manager apps are able to use network services. For example, Astro File Explorer has an option to connect to Windows shares. Connect all devices to the same WiFi network and share the project's master directory on your Windows computer. Then, copy the files using a file manager app on each device. https://play.google.com/store/apps/details?id=com.metago.astro DropBox, Google Drive, etc. \u00b6 Some file manager apps are able to use cloud services. For example, Astro File Explorer has an option to connect with five different file sharing solutions. Many cloud services also provide their own app.","title":"4. Managing Assets"},{"location":"user_guide/asset_management/#4-managing-assets","text":"","title":"4. Managing Assets"},{"location":"user_guide/asset_management/#basics","text":"With LiveSYNC, you can present various kinds of content to other people. Before your presentation, the content files must be manually copied to specific locations. This way LiveSYNC can find and access them during the presentation. The content files are called assets . In this chapter, we will go through various methods and strategies for managing your assets. Tip LiveSYNC contains free content samples. These are bundled with the app. You can study and evaluate the app using the demo content. Return here when you want to learn how to present your own files.","title":"Basics"},{"location":"user_guide/asset_management/#assets","text":"In short, a presentation's assets are all the files that you will need in your presentation . They include for example: 360-degree photos 360-degree videos Ordinary 2D photos Ordinary 2D videos Slide images Custom tag and hotspot icons Video stream configuration files Project files 2D map images 3D map models Camera path files You can combine different kinds of assets in your presentation. They can be partially the same as other presentations and partially different.","title":"Assets"},{"location":"user_guide/asset_management/#big-screen","text":"With LiveSYNC, you can use a common big screen device for showing content to an audience. This can be for example a TV or a projector. When you present content via the big screen, copy your presentation's assets to the control device. There is no need to copy them anywhere else. The view from your control device is mirrored to the big screen as a video stream, not as command messages. For example, you can connect an iPad to a TV using a standard HDMI cable (you will also need an adapter from Apple). Note Some modern TVs and projectors support playing photos and videos from a USB memory stick. You may wonder if that approach can be used for showing also 360-degree content. Technically, the photos and videos can be played. However, built-in players do not understand special projections used in 360-degree content. In addition, they do not provide interactivity ie. ways for panning and zooming 360-degree content. Hence, this approach does not really work.","title":"Big Screen"},{"location":"user_guide/asset_management/#viewing-devices","text":"LiveSYNC supports presenting content also via personal viewing devices. For example, phones, tablets, and VR headsets. You may wonder where the content comes from in this case. Is it streamed from the control device to all the audience devices? Is it streamed from a local server or a cloud service? Or is it played from local file copies on each device? LiveSYNC supports more than one option. However, in most cases, the recommended solution is to use local file copies. Streaming high-resolution video to many devices tends to congest local network connection. With local file copies, the user experience is great. And you don't need to struggle with network bandwidth issues. When you present content via viewing devices, copy your presentation's assets to the control device AND all viewing devices . Tip This chapter focuses on local file playback approach. If you want to learn how to use streaming with LiveSYNC, you can read about it from here .","title":"Viewing Devices"},{"location":"user_guide/asset_management/#file-storage-strategy","text":"An app that supports embedding images, videos, and sounds needs a strategy for handling multimedia files. This decision is fundamental. The chosen strategy defines how easy it is to manage your project with that app. For example, to share a finished work, store multiple versions of it, or to make some last-minute modifications. You have probably created presentations using PowerPoint or Keynote application. These tools make adding images easy. Have you ever thought about what happens in the background when you drag an image onto a slide? When you save the presentation, the output will be a single file. This file contains everything that belongs to your presentation, including all the images. A single file is easy to manage on disk and share via email. However, as everything is embedded file size can easily grow to several megabytes. Sometimes, tens of megabytes. What would happen if you added a couple of high-resolution videos? Single-file strategy becomes unpractical when the file size becomes too large. Hence, applications that make extensive use of video content choose a different strategy. The project is not stored as a single file that contains everything. Instead, all multimedia files are stored as individual files. A light-weight project file references to them - it does not embed them. This approach has many benefits, but also one drawback. A project becomes partially broken if any of the referenced files are not present. Hence, moving the files around or renaming them frequently should be avoided. Also, LiveSYNC uses this approach. A presentation consists of multiple smaller files that you need to copy in place. Not just one huge file .","title":"File Storage Strategy"},{"location":"user_guide/asset_management/#central-media-repository","text":"Previously, we explained why LiveSYNC presentations consist of multiple files. A logical follow-up question is this: should a presentation's files be kept in a single location, or scattered around the file system? Many platforms have built-in locations for photos, videos, and music. Should we copy presentation's photos to the device's Photos folder, videos to Videos folder, and so on? The short answer is: no. If a presentation's assets were scattered around the file system, managing them would become a nightmare. Especially with a large number of devices. 360-degree content would show up in other photo and video applications that do not know how to play them. All ordinary 2D photos and videos would be mixed with 360-degree content in LiveSYNC. To avoid confusion, LiveSYNC uses its own central media repository. It is a single location for storing all kinds of assets. This is not the same path where content captured with the device's camera appears. Sometimes, we do want to include ordinary photos and videos into our presentations. LiveSYNC allows this. For example, on iPad, the contents of the device's Camera Roll appear as a subdirectory. It is not a real subdirectory in LiveSYNC's media repository. Consider it as a virtual link to the Camera Roll directory. It is physically located elsewhere in the file system.","title":"Central Media Repository"},{"location":"user_guide/asset_management/#file-access-permissions","text":"On personal computers, almost the whole file system is accessible for applications. A file manager application is an essential, built-in part of the desktop environment. On Windows, every user is experienced in managing files via Explorer . On Mac, Finder is your best friend. This is different on mobile platforms such as iOS, Android and Oculus, where applications are sandboxed . Sandboxing means that the platform enforces security by isolating apps from each other. The same approach is applied to the file system: apps see only their own files by default. This has consequences to media applications such as LiveSYNC. On Android and Oculus, a user can give * permission* for a particular app for accessing files in the common files area. This is useful in many ways. For instance, one can use different apps for playing a video file that is located in the common files area. The user can also use a file manager app for moving, copying, and renaming files. On Android and Oculus, LiveSYNC uses a directory in the common files area as the root of its media repository. Permission for accessing the file system is asked from the user. This permission also allows reading the device's photos and videos folders. Hence, ordinary 2D content captured with the device's camera can be used in presentations. iOS does not have a similar concept. The apps can access only their own files. You cannot open the gates by giving permission, not even if you wanted to. On iOS, LiveSYNC uses its own file sharing directory as the root of its media repository . iOS provides an interface for 3rd party apps for accessing the photos folder. LiveSYNC supports this feature if you give it permission. Hence, ordinary 2D content captured with the device's camera can be used in presentations also on iOS.","title":"File Access Permissions"},{"location":"user_guide/asset_management/#file-system-limitations","text":"There are some common pitfalls when dealing with large video files in presentations. To avoid them, we need to understand technical and practical limitations. Some of them come from the file system . All the assets that belong to a presentation must be stored as files on a mass storage device. This might be the hard drive of the computer where you edit the files. Or, the internal mass memory of the tablet you use for presentations. Or, a USB memory stick that you use for quickly copying files to multiple VR headsets. Or, an SD card that you use for expanding your Android tablet's storage capacity. All these devices use a file system for managing the files. It defines how the total size of the mass memory is divided into smaller blocks, and how these blocks are allocated for individual files. The type of file system sets some important hard limits. One of them is the total number of bytes that a drive can store ie. max volume size in bytes . This is rarely a bottleneck nowadays. Theoretical limits are far higher than current storage capacities. Another limitation is the maximum size that a single file can be ie. max file size in bytes . This is important when you deal with large video files. None of your files can exceed the file system's max file size limit. Often you encounter problems with files that are over 4 GB in size. Another common problem is that a device does not have enough free space for the presentation's files. Example Jean has purchased a cheap Android tablet. It has 16 GB internal storage capacity. Android operating system and bundled factory apps (which cannot be removed) consume 6 GB. Other apps and Jean's photos, music, video clips, and web browser cache consume another 4 GB. Jean has only 6 GB left. Thus, the total size of all the files she needs in her presentation must not exceed 6 GB. Else, she will need to clean up her tablet to release some memory. Jean regrets not buying a 32 GB model. Based on a tip from her friend, Jean buys a 16 GB SD card to expand the storage capacity of her Android tablet. The SD card is formatted with the FAT32 file system. FAT32 is often used on older hard disks, USB flash drives, and memory cards. It is fairly well supported on different devices. The theoretical size of a FAT32 formatted drive could be as large as 8 TB (eight terabytes). Enough for 300 hours of 4K video captured with an iPhone. However, the size of each individual file may not exceed 4 GB (four gigabytes). That is only 10 minutes of 4K video captured with an iPhone. After adding the SD card, Jean has 16 GB more storage space. Each of her video files must stay below the 4 GB limit. But, one of Jean's videos is 6.5 GB. It is a high-resolution video that she wishes to present using an Oculus Go headset. She plans to control the presentation with her Android tablet. Jean has several options for overcoming her SD card's 4 GB max file size limitation: a) She can try to format her SD card with the exFAT file system. If her tablet recognizes the card, the problem is solved. exFAT is a newer file system and has much larger limits. 6.5 GB file will work easily. b) She can make space on her tablet's internal memory and copy the file there instead. That drive is very likely formatted with a different file system and does not have the 4 GB limit. c) She can encode a smaller version (low resolution/bitrate) version of the video and use that in her tablet. She can still use the high-resolution version on her Oculus Go headset. The filenames must match so that LiveSYNC can identify them as the same video. d) She can use a video editor application to split her video to shorter clips, each below the 4GB limit. During the presentation, she will simply start the next clip when the previous one ends. Tip On iOS devices, you cannot expand the storage capacity with an SD card. There are memory sticks intended for iOS devices, but they only work with the manufacturer's own app - not LiveSYNC. When purchasing a new iPhone or iPad, invest in a model that has enough built-in storage space for your needs. On some Android devices, you can use an SD card for expanding the storage capacity. When purchasing a new Android phone or tablet, check if it supports an SD card. If not, choose another model or invest in a model that has enough built-in storage capacity. We recommend purchasing a device that has at least 32 GB of internal storage space. Tip If you have videos that are larger than 4 GB, you may encounter problems in storing or transferring them. Usually, such problems can be solved. You might be able to simply reformat the storage device with another file system type (such as exFAT). Beware that formatting destroys all the data! Using a different device for storing or transferring the files often helps. You can also try transferring the files via cable or via a network connection. If nothing else helps, consider splitting a large file to multiple smaller ones (multiple video clips). Or, encode your video again to lower resolution and lower bitrate to create a smaller file.","title":"File System Limitations"},{"location":"user_guide/asset_management/#dealing-with-redundancy","text":"Video files consume a lot of storage space. Thus, it is important to learn to fight against unintended redundancy . Don't get this wrong: creating a backup is always a good idea. That is intended redundancy. However, when you are busy preparing for a presentation, it is easy to forget that you have one version of a video clip here and another there. Files that consume hundreds of megabytes or even multiple gigabytes quickly add up. And cleaning is laborious! One strategy is to create a new folder on your computer. This folder will be the master directory of your presentation. It should contain all the necessary assets and nothing else. Keep it tidy: delete old and extra versions immediately. Create backups by copying the whole folder e.g. to an external drive. Delete old content on your presentation devices. Copy the contents of your master directory there as-is. The idea is to focus on this one presentation and keep its assets in one directory. This approach works well if you use shared devices that must be cleared frequently. The approach described above is not suitable for all cases. Some users have presentations often and reuse material from previous presentations. Creating a new presentation folder every time would mean redundant copies of the same files. Another strategy is to create one master directory and many subfolders. Folder titles can be for example dates or topics, whatever suits your situation. You will quickly get familiar with your own content archive. During a presentation, you can adapt to the audience as you go and present content directly from your content archive. Maintain a backup of the whole archive on an external drive. Copy new files to your presentation devices incrementally. This approach works well if you have a set of devices reserved for your own use. A third option is a hybrid one. Build a content archive as explained above. Create a subfolder for a particular presentation by copying the necessary assets there. This means that there will be multiple copies of some of the files, but only for a while. After the presentation, simply delete its folder. All its assets are already in your archive folders. Note To keep the user interface simple, LiveSYNC supports only one level of subdirectories . We encourage you to make use of them. Choose one strategy for organizing your content, and stick to it.","title":"Dealing With Redundancy"},{"location":"user_guide/asset_management/#transfer-times","text":"Sometimes, a large number of devices is used in a presentation. The time required to copy the files grows exponentially along the total file size and the number of devices . Unless you can do it in parallel using multiple copy stations. Even so, you should have the final version ready early enough so that there is enough time to copy the assets in place. Notice that last-minute changes can be problematic. Changing even one small icon requires going through all the devices again. Moreover, on iOS devices, you may have to copy the whole folder again even if you need to change a single file inside it. This is because Apple's iTunes application. It is used for copying files for iOS devices, but it has very limited file management capabilities. Example Lisa is a teacher in elementary school. They have a rack of 20 iPads, which any of the teachers can reserve for their class. Lisa is preparing a presentation for the parent's night. Her students have made a 360-degree video of their day at school. She plans to use the iPads to show the video to the parents. Lisa has a single presentation folder that contains one 3 GB video file and a few 360-degree photographs. Her students already copied the files to all iPads during the afternoon. Suddenly Liza realizes that their 360-degree group photo is missing. If Lisa adds the missing photo to her presentation folder, she must copy the whole folder again. This is because of limitations in Apple's iTunes application. She quickly calculates that copying 3 GB * 20 devices means 60 GB in total. That will take hours! Lisa decides to create another folder that contains only this one image. Since the image is just 5 MB in size, Lisa needs to copy 5 MB * 20 devices = 100 MB. She will be ready in 15 minutes.","title":"Transfer Times"},{"location":"user_guide/asset_management/#which-files-must-be-copied","text":"In principle, all files that you need in your presentation must be copied to the control device and to each viewing device (except the big screen). We recommend that you create a master directory on your computer. Collect all the assets there. Then, copy the contents of this directory to your control device and viewing devices.","title":"Which Files Must Be Copied?"},{"location":"user_guide/asset_management/#360-degree-photos","text":"360-degree photos in JPG or PNG format can be copied either to the root of the LiveSYNC media repository or in a subdirectory . You can freely choose the name for the subdirectory (with a couple exceptions shown below). Remember that LiveSYNC supports only one level of subdirectories. Examples of valid paths: LiveSYNC/my_image.jpg LiveSYNC/my_image.png LiveSYNC/Another Image.jpg LiveSYNC/January/Flowers.png LiveSYNC/Summer Holiday/At the beach.jpg Examples of invalid paths: LiveSYNC/my_image.tiff TIFF image format is not supported LiveSYNC/January/16/Flowers.png Only one subdirectory level is allowed LiveSYNC/slides_tuesday/my_image.jpg Do not put 360-degree content in a subdirectory that starts with \"slides_\" LiveSYNC/hotspots_tuesday/my_image.jpg Do not put 360-degree content in a subdirectory that starts with \"hotspots_\" LiveSYNC/documents/my_image.jpg Do not put 360-degree content in a subdirectory whose name is \"documents\"","title":"360-Degree Photos"},{"location":"user_guide/asset_management/#360-degree-videos","text":"360-degree videos in MP4 container can be copied either to the root of the LiveSYNC media repository or in a subdirectory . You can freely choose the name for the subdirectory (with a couple exceptions shown below). Remember that LiveSYNC supports only one level of subdirectories. Live 360-degree video streams can be configured as special .videostream config files. These files can be placed to same locations where local 360-degree video files are placed. Examples of valid paths: LiveSYNC/my_video.mp4 LiveSYNC/live.videostream LiveSYNC/Another Video.mp4 LiveSYNC/January/Skiing.mp4 LiveSYNC/Summer Holiday/At the beach.mp4 Examples of invalid paths: LiveSYNC/my_image.avi AVI video container is not supported LiveSYNC/January/16/Skiing.mp4 Only one subdirectory level is allowed LiveSYNC/slides_tuesday/my_video.mp4 Do not put 360-degree content in a subdirectory that starts with \"slides_\" LiveSYNC/hotspots_tuesday/my_video.mp4 Do not put 360-degree content in a subdirectory that starts with \"hotspots_\" LiveSYNC/documents/my_video.mp4 Do not put 360-degree content in a subdirectory whose name is \"documents\"","title":"360-Degree Videos"},{"location":"user_guide/asset_management/#slides-2d-content","text":"Slides exported as images can be shown with LiveSYNC. Also, other 2D images and videos can be shown. Currently, content files are detected as ordinary 2D files based on filename convention. All images and videos in folders that begin with \"slides_\" are played as ordinary 2D content. The prefix \"slides_\" is automatically stripped from the filename when the directory is shown. For example, \"slides_Tuesday\" appears as a folder \"Tuesday\" in LiveSYNC. Live 2D video streams can be configured as special .videostream config files. These files can be placed to same locations where local 2D video files are placed. Examples of valid paths: LiveSYNC/slides_Tuesday/my_2D_image.jpg LiveSYNC/slides_Holiday/my_2D_video.mp4 LiveSYNC/slides_Meeting/Slide 1.png Examples of invalid paths: LiveSYNC/slides_/Skiing.mp4 Do not use the prefix alone LiveSYNC/slides_Holiday/Tuesday/Skiing.mp4 Only one subdirectory is allowed. LiveSYNC/Meeting/Slide 1.png Always put 2D content in a subdirectory that starts with \"slides_\"","title":"Slides &amp; 2D Content"},{"location":"user_guide/asset_management/#icons","text":"Custom tag and hotspot icons are small 2D images that appear as items that you can drag & drop onto 360-degree content. All images in folders that begin with \"hotspots_\" are treated as custom tag or hotspot icons. They appear in the Tags tab (instead of the Content tab). We recommend using PNG images with transparent background (alpha). Examples of valid paths: LiveSYNC/hotspots_signs/staff_only.png LiveSYNC/hotspots_prohibition/Do not enter.png Examples of invalid paths: LiveSYNC/hotspots_/No running.png Do not use the prefix alone LiveSYNC/hotspots_signs/Traffic/Stop.png Only one subdirectory is allowed. LiveSYNC/Meeting/Reserved.png Always put tag & hotspot icons in a subdirectory that starts with \"hotspots_\"","title":"Icons"},{"location":"user_guide/asset_management/#where-should-my-files-go","text":"","title":"Where Should My Files Go?"},{"location":"user_guide/asset_management/#ios","text":"On iOS, LiveSYNC uses the app's own file sharing directory as the root of its media repository . The screen capture shows a view to it from the iTunes application. On this iPad, there are many folders that contain icons for tags and hotspots. These are the folders that start with hotspot_ prefix. There is also a 360-degree content folder OfficeTour2018 . And, a 2D content folder for the same tour: slides_OfficeTour2018 .","title":"iOS"},{"location":"user_guide/asset_management/#android","text":"On Android, LiveSYNC uses directory /Movies/LiveSYNC in the common files area as the root of its media repository . The screen capture shows many 360-degree content folders: an archive of 360-degree photos. There is also a license file ( license.key.lic ). This file must not be removed. Note In case you have a device that supports a memory card (SD card), the media files can be copied there, too. The logical path will be the same: /Movies/LiveSYNC is searched also from the SD card. In case you have the same filename in both directories the internal memory takes precedence.","title":"Android"},{"location":"user_guide/asset_management/#oculus","text":"On Oculus, LiveSYNC uses directory /Movies/LiveSYNC in the common files area as the root of its media repository . This is exactly the same as on Android. The screen capture shows many 360-degree content folders: an archive of 360-degree photos. There is also a configuration file ( settings.ini ). This file is used for customizing the operation of the app. Note In case you have a GearVR compatible phone that supports a memory card (SD card), the media files can be copied there, too. The logical path will be the same: /Movies/LiveSYNC is searched also from the SD card. In case you have the same filename in both directories the internal memory takes precedence.","title":"Oculus"},{"location":"user_guide/asset_management/#how-to-copy-files","text":"Copying files back and forth between a computer and a mobile device is easy. The method depends on the platforms beings used: your computer's operating system and your mobile device's operating system. Common combinations are covered below.","title":"How To Copy Files?"},{"location":"user_guide/asset_management/#ios_1","text":"Copying files from a computer to an iOS device requires using Apple's iTunes application. iTunes is available for Mac and Windows operating systems. It is not available for Linux. Option A: Watch a video guide Video not working? Click here to open it in Vimeo. Note Since the video was made, Apple has slightly changed the iTunes application. File Sharing is more easily accessible now. Instead of selecting Apps from the left side menu and scrolling down the page, simply choose File Sharing from that same menu. Option B: Follow the steps described below On your computer, open a web browser and go here to download the iTunes application: https://www.apple.com/itunes/download/ Based on your operating system, choose a download link either for macOS or Windows. Once the download has completed, install the application on your computer just like any other app. Once the installation has completed, start the iTunes application. Your iOS device came with a bundled USB to Lightning cable . It is the same one you use for charging your iOS device. Connect your iOS device to your computer using this cable. On your computer, take a look at the iTunes application. It begins to synchronize content between your iOS device and computer. Assets for the LiveSYNC app are not automatically synchronized. Notice the small device icon that appears in the top bar. Click it. Tip It may take a moment after connecting the cable until the device icon appears. If it doesn't appear, restart iTunes application. The Summary page of your device appears. From the left side menu, click File Sharing . A list of apps that use the file sharing feature appears. Select LiveSYNC from the list. A list of the LiveSYNC app's files appears. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC Documents area in the iTunes application, and drop it there. It will be copied to your iOS device. The progress will be shown in the top area of the iTunes application. Tip You may have noticed that there is Add... button at the bottom of the LiveSYNC Documents area. This is an alternative method for copying the files. However, we recommend using drag'n drop. Tip If you copy more content to your iOS device during a presentation , there is no need to restart the LiveSYNC app. Use pull-to-refresh gesture in the Content tab or in the Tags tab. The contents will be refreshed.","title":"iOS"},{"location":"user_guide/asset_management/#android_1","text":"Copying files from a computer to an Android device requires using Explorer on Windows or Android File Transfer on Mac. Option A: Watch a video guide Video not working? Click here to open it in Vimeo. Note The Android part starts at 3m 13sec. On some Android devices, micro USB has been replaced with USB-C. Use the cable that came with the device. Option B: Follow the steps described below Windows Your Android device came with a bundled USB to micro USB or USB to USB-C cable. It is the same one you use for charging your Android device. Connect your Android device to your computer using this cable. When you connect the cable, your Android device asks your permission for file access. Tap Allow to grant permission. On your computer, Windows has detected a new device. It may ask what action to perform; you want to explore its files. When Explorer opens, find your Android device from the list. Expand the folder structure until you see the contents of /Movies/LiveSYNC directory. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC directory in Explorer , and drop it there. It will be copied to your Android device. The progress will be shown in a separate dialog. Mac Unfortunately, macOS does not have built-in support for transferring files to Android devices. The solution is to install Android File Transfer application. It is made by Google and available for free. On your computer, open a web browser and go here to download the Android File Transfer application: https://www.android.com/filetransfer/ Once the download has completed, install the application on your computer just like any other app. Your Android device came with a bundled USB to micro USB or USB to USB-C cable. It is the same one you use for charging your Android device. Connect your Android device to your computer using this cable. When you connect the cable, your Android device asks your permission for file access. Tap Allow to grant permission. On your computer, the Android File Transfer application should start automatically. Expand the folder structure until you see the contents of /Movies/LiveSYNC directory. From LiveSYNC's point of view, this is the root of its media repository. It is the location where you copy all your presentation's assets. Note Android File Transfer often shows a warning dialog similar to the one below. It means that you have not granted permission for file access on your Android device. Do it now, then click OK. The dialog disappears and a moment later a working file manager window opens. On your computer, find the folder that contains the assets for your presentation. Then drag it over LiveSYNC directory in Android File Transfer , and drop it there. It will be copied to your Android device. The progress will be shown in a separate dialog. Note Reportedly, Android File Manager has trouble in transferring files over 4 GB in size. The easiest solution is to use a Windows computer. You can also transfer large files between Mac and Android using Android development tools. Android Debug Bridge (ADB) allows copying files back and forth from the command line.","title":"Android"},{"location":"user_guide/asset_management/#oculus_1","text":"Supported Oculus devices (Samsung GearVR and Oculus Go) are both based on Android operating system. Hence, copying files to Oculus devices is the same as copying to other Android devices. Follow the instructions that were given for Android. Note In Step 3., you need to grant file access permission on your Android device. This also applies to Oculus Go. You must put the headset on your face and use the hand remote to select Allow . See the image below.","title":"Oculus"},{"location":"user_guide/asset_management/#alternative-solutions","text":"Copying assets from your computer to your mobile devices is usually done via USB cable. Cable connection is fast and reliable. However, it is not the only way. Here we discuss some alternative file transfer methods.","title":"Alternative Solutions"},{"location":"user_guide/asset_management/#usb-memory-stick","text":"In this method, the idea is to copy the files from your computer to your mobile devices in two phases: In the first phase, plug the USB stick to your computer and copy all the assets there. In the second phase, plug the USB stick to your mobile device and copy all the assets from the stick to /Movies/LiveSYNC on the mobile device. Note This method works only with Android and Oculus (GearVR) devices. It does not work with iOS. Special USB memory sticks that are compatible with iOS devices do exist. But, they only work with the manufacturer's own app - not 3rd party apps such as LiveSYNC. This method is usually slower than using a USB cable, not faster. Yet, there is one important exception: it scales up much cheaper. When you need to copy files to a large number of viewing devices, a single computer becomes a bottleneck. But, USB memory sticks are cheap. If you buy ten memory sticks and copy your files there, then a group of ten assistants can continue copying the files simultaneously . This way files can be quickly updated to, say, one hundred devices. Another benefit is that the files can be copied from the USB stick to the mobile device without a computer. For example, a teacher can copy the files to a memory stick in advance. Then, she circulates the stick among her students. Everyone can copy the files to their own device at their own turn. This is much easier to manage than using a laptop for the same task. In USB standard, one device acts as a host and another one as a peripheral. Usually, USB is used for connecting phones and tablets to a computer. In this arrangement, the computer is the USB host and the mobile is peripheral. But, when you connect a USB stick directly to a mobile, it needs to change to the host mode. The solution is a standard called USB On-The-Go (OTG) . Thus, to connect a USB stick into your mobile, you will need to use a USB OTG adapter between. This is illustrated in the image below. A USB OTG adapter allows connecting ordinary USB sticks. Some phone manufacturers include a tiny USB OTG adapter as an accessory - you may already have one. Another alternative is to buy a dual connector USB memory stick, which has integrated USB OTG adapter. These handy devices allow plugging the other end to a computer for phase 1. Then, unplug it and plug to your mobile for phase 2. Separate OTG adapter is not needed. For copying files from the memory stick to /Movies/LiveSYNC on the mobile device, you will need a file manager application. Many phones and tablets have one built-in. You can also install many file managers for free from Google Play store (just search for file manager ). With a file manager app, you can perform the usual file management tasks such as copying, moving, renaming and deleting. Once you connect a USB memory stick to your phone, it appears next to the device's internal memory and SD card in the file manager. Notice that some file managers show USB sticks as SD cards, as in the image below.","title":"USB Memory Stick"},{"location":"user_guide/asset_management/#windows-shares","text":"Some file manager apps are able to use network services. For example, Astro File Explorer has an option to connect to Windows shares. Connect all devices to the same WiFi network and share the project's master directory on your Windows computer. Then, copy the files using a file manager app on each device. https://play.google.com/store/apps/details?id=com.metago.astro","title":"Windows Shares"},{"location":"user_guide/asset_management/#dropbox-google-drive-etc","text":"Some file manager apps are able to use cloud services. For example, Astro File Explorer has an option to connect with five different file sharing solutions. Many cloud services also provide their own app.","title":"DropBox, Google Drive, etc."},{"location":"user_guide/configuration/","text":"3. Configuration \u00b6 Overview \u00b6 With LiveSYNC, you can present 360-degree and ordinary 2D content to other people. The content can be shown using a common big screen device and/or a set of viewing devices shared to the audience. The viewing devices must be able to communicate with the presenter's control device. For example, the control device sends a command to change the slide. The viewing devices respond that the slide was found and is now loaded on screen. This communication takes place via a wireless connection . It is opened between the control device and each viewing device. Before the connection can be opened, it must be configured to each device. For example, a device must know which role to adopt in the presentation. It also needs to know which connection type to use, and which channel number to join. In LiveSYNC, this is called a channel configuration . A set of devices that use the same connection type and the same channel number form a device group . They can exchange messages with each other. A device group is controlled by a single control device. This is the device that is configured to the director role. All other devices in the group must be configured to the audience role. Multiple device groups can operate in parallel even in the same room. Each group has its own control device. Creating a channel configuration is a one-time operation. Whenever you start a presentation, you can re-use the same configuration. However, it is possible to create multiple channel configurations for each device. This is useful in dynamic environments ie. if multiple presentations take place simultaneously and device group sizes vary. On each device, only one connection can be open at a time. Channel configuration is slightly different depending on the device's role (director or audience). Next, we will go through both cases. Director Role \u00b6 iOS & Android \u00b6 In a device group, one device must be configured to the director role. This device is typically an iPad or an Android tablet. The configuration is very similar on both platforms. Follow the steps below to configure your device to the director role: Start the LiveSYNC app. The Home screen appears. Initially, channel configurations do not exist. To create a new channel configuration, tap the big (+) button. The Setup screen appears. First, tap the Director button to select the director role. Your selection is marked by underlining it, as in the image below. The selected role also appears in the setup progress menu on the left. Tap Next to proceed. Note Current setup page is underlined in the setup progress menu on the left. You can navigate through the setup pages by tapping the page titles in the progress menu. You can also swipe the pages towards left or right, or tap the Next button on the bottom right corner. On Android, you can also use the device's Back button to return to the previous page. Your input is validated on each page. You cannot proceed to the next page before your input is accepted. If you return to the first page and change the role, your input to the other pages will be cleared. If you want to cancel channel configuration, tap the Home button on the top left corner. Then answer OK to the confirmation dialog that appears. Other users can identify you by your LiveSYNC name. This can be for example your own name, a name that you have given to your device, or your device's model name. The name cannot be left empty. Its maximum length is 32 characters. Tap the text field to type a name. Tap Next to proceed. Communication between the control device and viewing devices uses a wireless connection. The currently available connection types are listed. Choose a connection type that you wish to use. The director device and the audience devices must use the same connection type . Tap Next to proceed. Note A connection type may be temporarily unavailable. If so, it is marked as Not available in the list. Tap the info button (i) next to it to learn why it is not available. As an example, you may need to enable Wifi or Bluetooth feature from the device's settings. Use pull-to-refresh gesture to update the list after fixing the issue. Devices using the same connection type and the same channel can exchange messages. Devices on different channels will not see them. A random free channel number is given. If you want to get another one, tap the dice button next to the number. The channel numbers are in the range 1000-9999. Tap Next to proceed. Note You may wonder why the channel number is given randomly instead of asking it from the user. This is to prevent abuse. Consider a lesson in elementary school. Ms. Williams is about to begin a presentation and asks her students to join channel number 1234. However, young Mr. Smart Guy in the back seat configures his tablet as the director on that same channel. Now, there are two competing control devices. Other students' devices will form a connection randomly to one of them. Not being able to input the channel number prevents mischief. Android only: a dialog appears to ask your permission to access the files on your device. Without permission the app cannot create the directory where your assets will go. Also, it cannot read from this directory even if you create it manually. Tap Allow to give file access permission. The channel configuration is almost complete. This setup page reminds you that the presentation's assets must be copied to the device. This task will be covered in detail in the next chapter Managing assets . You can copy the assets in place now, but you can also do that after completing the setup phase. Tap Next to proceed. The channel configuration is now created and the setup phase has been completed. Tap OK to return to the Home screen. Note Channel configurations are stored to your device's permanent memory. Closing the app or powering off the device will not make them disappear. They will also survive updating the app to the latest version. Not so if you reinstall the app. For example, to change between the store version and the beta version. Then, the channel configurations will be lost and you need to re-create them. Notice that you will not get the same channel numbers, as they are given randomly. Your new channel configuration now appears on the Home screen. If you create many channel configurations, swipe them to left or right to find the one you wish to use. If you wish to edit an existing channel configuration, long tap it. This will bring its editing controls in the view. Tap the cog wheel to return to the Setup screen where you can edit the configuration. Tap the cross to delete the channel configuration. Tap anywhere else to hide the editing controls. If you long-tap a channel configuration and then tap the cross button, a confirmation dialog is shown. Select OK to permanently delete the channel configuration. Warning This action cannot be undone. Once you confirm removing a channel configuration, it is gone. You have to re-create it from scratch to get it back. Notice that you will not get the same channel number, as they are given randomly. Oculus \u00b6 Currently, the director mode is not supported on VR-only devices such as GearVR and Oculus Go. You cannot use these devices for controlling a presentation. However, you can configure them to the audience role to view a presentation. This is explained later in this chapter . Audience Role \u00b6 iOS & Android \u00b6 In a device group, all but one device must be configured to the audience role. These devices are often iOS or Android phones and tablets. The configuration is very similar on both platforms. Follow the steps below to configure your device to the audience role: Start the LiveSYNC app. The Home screen appears. Initially, channel configurations do not exist. To create a new channel configuration, tap the big (+) button. The Setup screen appears. First, tap the Audience button to select the audience role. Your selection is marked by underlining it, as in the image below. The selected role also appears in the setup progress menu on the left. If you are using a phone, a new page View mode appears at the end of the progress menu (it is related to VR mode). Tap Next to proceed. Note Current setup page is underlined in the setup progress menu on the left. You can navigate through the setup pages by tapping the page titles in the progress menu. You can also swipe the pages towards left or right, or tap the Next button on the bottom right corner. On Android, you can also use the device's Back button to return to the previous page. Your input is validated on each page. You cannot proceed to the next page before your input is accepted. If you return to the first page and change the role, your input to the other pages will be cleared. If you want to cancel channel configuration, tap the Home button on the top left corner. Then answer OK to the confirmation dialog that appears. Other users can identify you by your LiveSYNC name. This can be for example your own name, a name that you have given to your device, or your device's model name. The name cannot be left empty. Its maximum length is 32 characters. Tap the text field to type a name. Tap Next to proceed. Communication between the control device and viewing devices uses a wireless connection. The currently available connection types are listed. Choose a connection type that you wish to use. The director device and the audience devices must use the same connection type . Tap Next to proceed. Note A connection type may be temporarily unavailable. If so, it is marked as Not available in the list. Tap the info button (i) next to it to learn why it is not available. As an example, you may need to enable Wifi or Bluetooth feature from the device's settings. Use pull-to-refresh gesture to update the list after fixing the issue. Devices using the same connection type and the same channel can exchange messages. Devices on a different channel will not see them. Tap the text field to type a channel number. Use the same channel number that you configured to your control device. The channel numbers are in the range 1000-9999. Tap Next to proceed. If you are configuring a phone, select the view mode. If you plan to use Google Cardboard or similar passive VR headset, select VR . Else, select Normal . Tap Next to proceed. Tip You can change the view mode also during a presentation by long tapping the screen. Note This page does not appear on tablets. Tablets are physically too big to fit inside a VR headset. Android only: a dialog appears to ask your permission to access the files on your device. Without permission, the app cannot create the directory where your assets will go. Also, it cannot read from this directory even if you create it manually. Tap Allow to give file access permission. The channel configuration is almost complete. This setup page reminds you that the presentation's assets must be copied to the device. This task will be covered in detail in the next chapter Managing assets . You can copy the assets in place now, but you can also do that after completing the setup phase. Tap Next to proceed. The channel configuration is now created and the setup phase has been completed. Tap OK to return to the Home screen. Note Channel configurations are stored to your device's permanent memory. Closing the app or powering off the device will not make them disappear. They will also survive updating the app to the latest version. Not so if you reinstall the app. For example, to change between the store version and the beta version. Then, the channel configurations will be lost and you need to re-create them. Your new channel configuration now appears on the Home screen. If you create many channel configurations, swipe them to left or right to find the one you wish to use. If you wish to edit an existing channel configuration, long tap it. This will bring its editing controls in the view. Tap the cog wheel to return to the Setup screen where you can edit the configuration. Tap the cross to delete the channel configuration. Tap anywhere else to hide the editing controls. If you long-tap a channel configuration and then tap the cross button, a confirmation dialog is shown. Select OK to permanently delete the channel configuration. Warning This action cannot be undone. Once you confirm removing a channel configuration, it is gone. You have to re-create it from scratch to get it back. Oculus \u00b6 TODO Configuration Tips \u00b6 Device Role \u00b6 You should always start configuring from the device that will act in the director role . This is because you need to know the channel number to be able to configure the audience devices. You will get that number during the director role configuration. Identification \u00b6 During a presentation, you may notice that one of the users needs help. Or, a particular device has problems. For example, you forgot to copy the assets to one of the devices. You will see the devices' LiveSYNC names on your control device's screen. To find which device to go to, you need a way to match the LiveSYNC name to a particular device. Hence, you should always use descriptive names. If your audience is using their own personal devices, it is a good idea to ask them to use their own first name as the LiveSYNC name. Then, you can recognize each user right away from the control device's screen. Isabella, you seem to be lost, do you need help? If you provide the viewing devices for the audience, it is a good idea to mark the LiveSYNC name to the device . For example, use a label printer. Another good alternative is to use color coding. Attach labels of different colors to the devices and use colors as LiveSYNC names. When Green headset has problems, you immediately know where to go. If you have lots of devices, use numbers. Everybody, listen up! Who has headset number 56? Please raise your hand and someone will come to assist you. Connection Type \u00b6 The control device and viewing devices need to be able to communicate. There are multiple connection types that can be used. Which one should I use, you may wonder. This table will help you choose. Connection Type Network Scalability Channel Reservation Bluetooth Standalone 1 Bluetooth LE 4.0 2 1-8 clients 3 Permanent 4 GlobalSYNC Cloud service 5 Wifi or mobile data 1000+ clients Short-time lease 6 Example Sophie is a tourist guide in Rome. Every night she will take a small group of tourists to a tour. Sophie will share VR headsets before they start. Every once in a while, they will stop in front of a building or a market square. Sophie will show them photos and videos via the VR headsets. The tourists love the way how this will bring the history of the city alive in front of their eyes. Sophie uses a Bluetooth connection. She doesn't need to pay for mobile data access. They can stop anywhere because Wifi access is not needed either. Example Tina works in the manufacturing industry. Clients often come to visit the factory. They are taken to a tour in groups of five. For safety reasons, some parts of the factory are closed from visitors. Tina shows 360-degree video clips from the closed areas. She uses a Bluetooth connection. Wifi signal reception in the factory is poor. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. Joan uses a Bluetooth connection. In most trade shows the Wifi network is congested. Example Mark is a teacher in elementary school. They have a rack of 20 Android tablets, which any of the teachers can reserve for their class. Mark uses GlobalSYNC connection. The school has Wifi access and he needs to control more devices than Bluetooth can handle. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. They are building in several cities. Anthony wants to keep himself up-to-date without spending his days traveling. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. Sometimes, Anthony needs to intervene. They will have a short teleconference. During the meeting, Anthony wants to show other site managers how one of them cleverly handled a difficult case. Anthony uses GlobalSYNC over mobile data. He and the site managers are all in different cities and the construction sites do not have Wifi access. Channel Number \u00b6 When configuring a device to the director role you must choose a channel number. This cannot be selected freely. Instead, you are given a randomly selected number. However, you can press a button to ask a different one. When should you do that? If you select a standalone connection type, such as Bluetooth, channel numbers are reserved from a local pool. There is a small chance that a channel number, which is already being used in another control device nearby, is offered. In such a case, get another channel number. If you use multiple channel configurations on a single device, it is easier to remember and distinguish them if the numbers are not too similar. View Mode \u00b6 On phones that are being configured to the audience role, you can select a view mode. This is used for making either the VR mode or the normal mode as the default for a particular channel. If you plan to use a passive VR headset with your audience phones, select the VR mode. It is easier for the users if the app starts in the correct mode. If not, always select the normal mode. The VR mode can look very confusing without a VR headset! Standalone means that the connection type does not require any infrastructure . You could use this connection type even on a deserted island. \u21a9 Bluetooth 4.0 is the minimum requirement. You can, of course, use devices that have newer versions. \u21a9 The maximum number depends on the license type and the features of the control device's Bluetooth chipset and operating system. We encourage you to test your hardware in advance. \u21a9 Channels are reserved from a local pool and you can use them as long as you want. \u21a9 Cloud service requires an active Internet connection to work throughout the presentation. \u21a9 Channels are reserved from one common pool. After 7 days of inactivity, a reserved channel will be returned to the pool. The next time you will need to reserve a new channel. Hence, your channel number will change. \u21a9","title":"3. Configuration"},{"location":"user_guide/configuration/#3-configuration","text":"","title":"3. Configuration"},{"location":"user_guide/configuration/#overview","text":"With LiveSYNC, you can present 360-degree and ordinary 2D content to other people. The content can be shown using a common big screen device and/or a set of viewing devices shared to the audience. The viewing devices must be able to communicate with the presenter's control device. For example, the control device sends a command to change the slide. The viewing devices respond that the slide was found and is now loaded on screen. This communication takes place via a wireless connection . It is opened between the control device and each viewing device. Before the connection can be opened, it must be configured to each device. For example, a device must know which role to adopt in the presentation. It also needs to know which connection type to use, and which channel number to join. In LiveSYNC, this is called a channel configuration . A set of devices that use the same connection type and the same channel number form a device group . They can exchange messages with each other. A device group is controlled by a single control device. This is the device that is configured to the director role. All other devices in the group must be configured to the audience role. Multiple device groups can operate in parallel even in the same room. Each group has its own control device. Creating a channel configuration is a one-time operation. Whenever you start a presentation, you can re-use the same configuration. However, it is possible to create multiple channel configurations for each device. This is useful in dynamic environments ie. if multiple presentations take place simultaneously and device group sizes vary. On each device, only one connection can be open at a time. Channel configuration is slightly different depending on the device's role (director or audience). Next, we will go through both cases.","title":"Overview"},{"location":"user_guide/configuration/#director-role","text":"","title":"Director Role"},{"location":"user_guide/configuration/#ios-android","text":"In a device group, one device must be configured to the director role. This device is typically an iPad or an Android tablet. The configuration is very similar on both platforms. Follow the steps below to configure your device to the director role: Start the LiveSYNC app. The Home screen appears. Initially, channel configurations do not exist. To create a new channel configuration, tap the big (+) button. The Setup screen appears. First, tap the Director button to select the director role. Your selection is marked by underlining it, as in the image below. The selected role also appears in the setup progress menu on the left. Tap Next to proceed. Note Current setup page is underlined in the setup progress menu on the left. You can navigate through the setup pages by tapping the page titles in the progress menu. You can also swipe the pages towards left or right, or tap the Next button on the bottom right corner. On Android, you can also use the device's Back button to return to the previous page. Your input is validated on each page. You cannot proceed to the next page before your input is accepted. If you return to the first page and change the role, your input to the other pages will be cleared. If you want to cancel channel configuration, tap the Home button on the top left corner. Then answer OK to the confirmation dialog that appears. Other users can identify you by your LiveSYNC name. This can be for example your own name, a name that you have given to your device, or your device's model name. The name cannot be left empty. Its maximum length is 32 characters. Tap the text field to type a name. Tap Next to proceed. Communication between the control device and viewing devices uses a wireless connection. The currently available connection types are listed. Choose a connection type that you wish to use. The director device and the audience devices must use the same connection type . Tap Next to proceed. Note A connection type may be temporarily unavailable. If so, it is marked as Not available in the list. Tap the info button (i) next to it to learn why it is not available. As an example, you may need to enable Wifi or Bluetooth feature from the device's settings. Use pull-to-refresh gesture to update the list after fixing the issue. Devices using the same connection type and the same channel can exchange messages. Devices on different channels will not see them. A random free channel number is given. If you want to get another one, tap the dice button next to the number. The channel numbers are in the range 1000-9999. Tap Next to proceed. Note You may wonder why the channel number is given randomly instead of asking it from the user. This is to prevent abuse. Consider a lesson in elementary school. Ms. Williams is about to begin a presentation and asks her students to join channel number 1234. However, young Mr. Smart Guy in the back seat configures his tablet as the director on that same channel. Now, there are two competing control devices. Other students' devices will form a connection randomly to one of them. Not being able to input the channel number prevents mischief. Android only: a dialog appears to ask your permission to access the files on your device. Without permission the app cannot create the directory where your assets will go. Also, it cannot read from this directory even if you create it manually. Tap Allow to give file access permission. The channel configuration is almost complete. This setup page reminds you that the presentation's assets must be copied to the device. This task will be covered in detail in the next chapter Managing assets . You can copy the assets in place now, but you can also do that after completing the setup phase. Tap Next to proceed. The channel configuration is now created and the setup phase has been completed. Tap OK to return to the Home screen. Note Channel configurations are stored to your device's permanent memory. Closing the app or powering off the device will not make them disappear. They will also survive updating the app to the latest version. Not so if you reinstall the app. For example, to change between the store version and the beta version. Then, the channel configurations will be lost and you need to re-create them. Notice that you will not get the same channel numbers, as they are given randomly. Your new channel configuration now appears on the Home screen. If you create many channel configurations, swipe them to left or right to find the one you wish to use. If you wish to edit an existing channel configuration, long tap it. This will bring its editing controls in the view. Tap the cog wheel to return to the Setup screen where you can edit the configuration. Tap the cross to delete the channel configuration. Tap anywhere else to hide the editing controls. If you long-tap a channel configuration and then tap the cross button, a confirmation dialog is shown. Select OK to permanently delete the channel configuration. Warning This action cannot be undone. Once you confirm removing a channel configuration, it is gone. You have to re-create it from scratch to get it back. Notice that you will not get the same channel number, as they are given randomly.","title":"iOS &amp; Android"},{"location":"user_guide/configuration/#oculus","text":"Currently, the director mode is not supported on VR-only devices such as GearVR and Oculus Go. You cannot use these devices for controlling a presentation. However, you can configure them to the audience role to view a presentation. This is explained later in this chapter .","title":"Oculus"},{"location":"user_guide/configuration/#audience-role","text":"","title":"Audience Role"},{"location":"user_guide/configuration/#ios-android_1","text":"In a device group, all but one device must be configured to the audience role. These devices are often iOS or Android phones and tablets. The configuration is very similar on both platforms. Follow the steps below to configure your device to the audience role: Start the LiveSYNC app. The Home screen appears. Initially, channel configurations do not exist. To create a new channel configuration, tap the big (+) button. The Setup screen appears. First, tap the Audience button to select the audience role. Your selection is marked by underlining it, as in the image below. The selected role also appears in the setup progress menu on the left. If you are using a phone, a new page View mode appears at the end of the progress menu (it is related to VR mode). Tap Next to proceed. Note Current setup page is underlined in the setup progress menu on the left. You can navigate through the setup pages by tapping the page titles in the progress menu. You can also swipe the pages towards left or right, or tap the Next button on the bottom right corner. On Android, you can also use the device's Back button to return to the previous page. Your input is validated on each page. You cannot proceed to the next page before your input is accepted. If you return to the first page and change the role, your input to the other pages will be cleared. If you want to cancel channel configuration, tap the Home button on the top left corner. Then answer OK to the confirmation dialog that appears. Other users can identify you by your LiveSYNC name. This can be for example your own name, a name that you have given to your device, or your device's model name. The name cannot be left empty. Its maximum length is 32 characters. Tap the text field to type a name. Tap Next to proceed. Communication between the control device and viewing devices uses a wireless connection. The currently available connection types are listed. Choose a connection type that you wish to use. The director device and the audience devices must use the same connection type . Tap Next to proceed. Note A connection type may be temporarily unavailable. If so, it is marked as Not available in the list. Tap the info button (i) next to it to learn why it is not available. As an example, you may need to enable Wifi or Bluetooth feature from the device's settings. Use pull-to-refresh gesture to update the list after fixing the issue. Devices using the same connection type and the same channel can exchange messages. Devices on a different channel will not see them. Tap the text field to type a channel number. Use the same channel number that you configured to your control device. The channel numbers are in the range 1000-9999. Tap Next to proceed. If you are configuring a phone, select the view mode. If you plan to use Google Cardboard or similar passive VR headset, select VR . Else, select Normal . Tap Next to proceed. Tip You can change the view mode also during a presentation by long tapping the screen. Note This page does not appear on tablets. Tablets are physically too big to fit inside a VR headset. Android only: a dialog appears to ask your permission to access the files on your device. Without permission, the app cannot create the directory where your assets will go. Also, it cannot read from this directory even if you create it manually. Tap Allow to give file access permission. The channel configuration is almost complete. This setup page reminds you that the presentation's assets must be copied to the device. This task will be covered in detail in the next chapter Managing assets . You can copy the assets in place now, but you can also do that after completing the setup phase. Tap Next to proceed. The channel configuration is now created and the setup phase has been completed. Tap OK to return to the Home screen. Note Channel configurations are stored to your device's permanent memory. Closing the app or powering off the device will not make them disappear. They will also survive updating the app to the latest version. Not so if you reinstall the app. For example, to change between the store version and the beta version. Then, the channel configurations will be lost and you need to re-create them. Your new channel configuration now appears on the Home screen. If you create many channel configurations, swipe them to left or right to find the one you wish to use. If you wish to edit an existing channel configuration, long tap it. This will bring its editing controls in the view. Tap the cog wheel to return to the Setup screen where you can edit the configuration. Tap the cross to delete the channel configuration. Tap anywhere else to hide the editing controls. If you long-tap a channel configuration and then tap the cross button, a confirmation dialog is shown. Select OK to permanently delete the channel configuration. Warning This action cannot be undone. Once you confirm removing a channel configuration, it is gone. You have to re-create it from scratch to get it back.","title":"iOS &amp; Android"},{"location":"user_guide/configuration/#oculus_1","text":"TODO","title":"Oculus"},{"location":"user_guide/configuration/#configuration-tips","text":"","title":"Configuration Tips"},{"location":"user_guide/configuration/#device-role","text":"You should always start configuring from the device that will act in the director role . This is because you need to know the channel number to be able to configure the audience devices. You will get that number during the director role configuration.","title":"Device Role"},{"location":"user_guide/configuration/#identification","text":"During a presentation, you may notice that one of the users needs help. Or, a particular device has problems. For example, you forgot to copy the assets to one of the devices. You will see the devices' LiveSYNC names on your control device's screen. To find which device to go to, you need a way to match the LiveSYNC name to a particular device. Hence, you should always use descriptive names. If your audience is using their own personal devices, it is a good idea to ask them to use their own first name as the LiveSYNC name. Then, you can recognize each user right away from the control device's screen. Isabella, you seem to be lost, do you need help? If you provide the viewing devices for the audience, it is a good idea to mark the LiveSYNC name to the device . For example, use a label printer. Another good alternative is to use color coding. Attach labels of different colors to the devices and use colors as LiveSYNC names. When Green headset has problems, you immediately know where to go. If you have lots of devices, use numbers. Everybody, listen up! Who has headset number 56? Please raise your hand and someone will come to assist you.","title":"Identification"},{"location":"user_guide/configuration/#connection-type","text":"The control device and viewing devices need to be able to communicate. There are multiple connection types that can be used. Which one should I use, you may wonder. This table will help you choose. Connection Type Network Scalability Channel Reservation Bluetooth Standalone 1 Bluetooth LE 4.0 2 1-8 clients 3 Permanent 4 GlobalSYNC Cloud service 5 Wifi or mobile data 1000+ clients Short-time lease 6 Example Sophie is a tourist guide in Rome. Every night she will take a small group of tourists to a tour. Sophie will share VR headsets before they start. Every once in a while, they will stop in front of a building or a market square. Sophie will show them photos and videos via the VR headsets. The tourists love the way how this will bring the history of the city alive in front of their eyes. Sophie uses a Bluetooth connection. She doesn't need to pay for mobile data access. They can stop anywhere because Wifi access is not needed either. Example Tina works in the manufacturing industry. Clients often come to visit the factory. They are taken to a tour in groups of five. For safety reasons, some parts of the factory are closed from visitors. Tina shows 360-degree video clips from the closed areas. She uses a Bluetooth connection. Wifi signal reception in the factory is poor. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. Joan uses a Bluetooth connection. In most trade shows the Wifi network is congested. Example Mark is a teacher in elementary school. They have a rack of 20 Android tablets, which any of the teachers can reserve for their class. Mark uses GlobalSYNC connection. The school has Wifi access and he needs to control more devices than Bluetooth can handle. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. They are building in several cities. Anthony wants to keep himself up-to-date without spending his days traveling. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. Sometimes, Anthony needs to intervene. They will have a short teleconference. During the meeting, Anthony wants to show other site managers how one of them cleverly handled a difficult case. Anthony uses GlobalSYNC over mobile data. He and the site managers are all in different cities and the construction sites do not have Wifi access.","title":"Connection Type"},{"location":"user_guide/configuration/#channel-number","text":"When configuring a device to the director role you must choose a channel number. This cannot be selected freely. Instead, you are given a randomly selected number. However, you can press a button to ask a different one. When should you do that? If you select a standalone connection type, such as Bluetooth, channel numbers are reserved from a local pool. There is a small chance that a channel number, which is already being used in another control device nearby, is offered. In such a case, get another channel number. If you use multiple channel configurations on a single device, it is easier to remember and distinguish them if the numbers are not too similar.","title":"Channel Number"},{"location":"user_guide/configuration/#view-mode","text":"On phones that are being configured to the audience role, you can select a view mode. This is used for making either the VR mode or the normal mode as the default for a particular channel. If you plan to use a passive VR headset with your audience phones, select the VR mode. It is easier for the users if the app starts in the correct mode. If not, always select the normal mode. The VR mode can look very confusing without a VR headset! Standalone means that the connection type does not require any infrastructure . You could use this connection type even on a deserted island. \u21a9 Bluetooth 4.0 is the minimum requirement. You can, of course, use devices that have newer versions. \u21a9 The maximum number depends on the license type and the features of the control device's Bluetooth chipset and operating system. We encourage you to test your hardware in advance. \u21a9 Channels are reserved from a local pool and you can use them as long as you want. \u21a9 Cloud service requires an active Internet connection to work throughout the presentation. \u21a9 Channels are reserved from one common pool. After 7 days of inactivity, a reserved channel will be returned to the pool. The next time you will need to reserve a new channel. Hence, your channel number will change. \u21a9","title":"View Mode"},{"location":"user_guide/customizing/","text":"Customizing \u00b6 TODO","title":"13. Customizing"},{"location":"user_guide/customizing/#customizing","text":"TODO","title":"Customizing"},{"location":"user_guide/editor/","text":"The Editor tab is a feature in the Enterprise version of the LiveSYNC app. Users who don't have the Enterprise version do not have the Editor tab and should skip this chapter. The Editor is a tab in the control device, which provides persistency for tags. You can mark points-of-interest with simple passive tags or add interactive (selectable) hotspots. Such configurations can be saved as projects and used in presentations. They can also be opened again for editing, or exported as a report and sent to a colleague. Select the Editor tab and drag a content to the presentation area. Select Create New Project if you don't have previously saved projects. Tap Tags tab and select the tag you want to add to a presentation type by long press and release on presentation content. You can later long press on the added tag and choose the right position you want to add. The moment you add a new tag, a tag settings window will be shown on the right side of the screen. From the settings, you can choose the type of Tag, give title and description, you can even add attachments and adjust the size of tag using the slider indicated as Hotspot size. After adding tags, remarks and notes make sure you press the Save button by providing the project name to reuse the project and export it later. If you wish to delete the added tag, select the tag by long press and tap Delete text from the window seen on the right side. Exporting presentation content \u00b6 After adding hotspot make sure you press save icon and switch to Editor tab if tags (hotspots) are added on player mode. Drag the saved content to center labeled with \u201cDrag content here\u201d on Editor tab. You can add tags and also add note, title and description and even adjust size of tag on editor mode by simply tapping the added tag on the content. An editor window will appear on the right side of the screen. If you want to export the content with all remarks, notes and tags simply press Export from top menu and Document details dialog view will appear, give document title as file name and other optional fields if needed then press OK . LiveSYNC will take snapshots and convert that pdf and will display the pdf preview for you with an option to print, save, mail and share on social media. Note LiveSYNC projects are saved in JSON format . You have access to them directly by using iTunes or you can send them through e-mail by using the export feature in LiveSYNC.","title":"11. Editor"},{"location":"user_guide/editor/#exporting-presentation-content","text":"After adding hotspot make sure you press save icon and switch to Editor tab if tags (hotspots) are added on player mode. Drag the saved content to center labeled with \u201cDrag content here\u201d on Editor tab. You can add tags and also add note, title and description and even adjust size of tag on editor mode by simply tapping the added tag on the content. An editor window will appear on the right side of the screen. If you want to export the content with all remarks, notes and tags simply press Export from top menu and Document details dialog view will appear, give document title as file name and other optional fields if needed then press OK . LiveSYNC will take snapshots and convert that pdf and will display the pdf preview for you with an option to print, save, mail and share on social media. Note LiveSYNC projects are saved in JSON format . You have access to them directly by using iTunes or you can send them through e-mail by using the export feature in LiveSYNC.","title":"Exporting presentation content"},{"location":"user_guide/hotspots/","text":"Marking with tags Is it possible to add own tags? \u00b6 In LiveSYNC it is possible to add own tags. Make sure the icons are in .png format and all your icons are in a folder. Add \"hotspot_\" before folder name e.g. hotspot_Brands then copy the folder to LiveSYNC directory. Pull Tags section from the director device to refresh its content. After refreshing the tags section you will see your tags added to the list by the name you provided after \"hotspot_\", in this case, icons will be seen in Brands folder with a drop-down arrow icon. The enterprise version of LiveSYNC has a fourth tab called Editor . The Editor tab is designed for clients and industries who require an interactive annotation using hotspots and tags. This enterprise version comes with a feature to save projects, edit or reuse saved projects, add tags with title and remarks, and later export presentations as pdf format.","title":"9. Tags & Hotspots"},{"location":"user_guide/hotspots/#is-it-possible-to-add-own-tags","text":"In LiveSYNC it is possible to add own tags. Make sure the icons are in .png format and all your icons are in a folder. Add \"hotspot_\" before folder name e.g. hotspot_Brands then copy the folder to LiveSYNC directory. Pull Tags section from the director device to refresh its content. After refreshing the tags section you will see your tags added to the list by the name you provided after \"hotspot_\", in this case, icons will be seen in Brands folder with a drop-down arrow icon. The enterprise version of LiveSYNC has a fourth tab called Editor . The Editor tab is designed for clients and industries who require an interactive annotation using hotspots and tags. This enterprise version comes with a feature to save projects, edit or reuse saved projects, add tags with title and remarks, and later export presentations as pdf format.","title":"Is it possible to add own tags?"},{"location":"user_guide/installing/","text":"2. Installing \u00b6 Versions \u00b6 The LiveSYNC app is available as different versions . Read this section first to understand which version is best for you and what hardware you need for using it. Platforms \u00b6 The LiveSYNC app can be installed and run on multiple platforms . Currently, the supported platforms include iOS , Android , and Oculus . Each platform has its own version of the app. They are compatible with each other, but not exactly the same. The app works on multiple device types . Currently, the supported device types include phones , tablets , and VR headsets . All these are great for viewing content. A tablet is recommended for creating, editing, and presenting content. Some device types allow viewing content in VR mode via a passive or an active VR headset. The availability of VR mode varies. Tablets are not physically compatible with VR headsets. Hence, the VR mode is not available on tablets. On phones, the VR mode is selectable . You can view content via the touchscreen but you can also put the phone inside a passive VR headset. For example, Google Cardboard or compatible. On devices that were made for VR, such as GearVR and Oculus Go, the VR mode is always active. The supported device types and the availability of the VR mode are summarized below. Platform Device Type VR Mode iOS iPhone, and iPad Selectable on iPhone, N/A on iPad Android Phones, and tablets Selectable on phones, N/A on tablets Oculus VR headset Always on Example Tina works in the manufacturing industry. Clients often come to visit the factory. They are taken to a tour in groups of five. For safety reasons, some parts of the factory are closed from visitors. Tina wants to show 360-degree video clips from the closed areas. Hence, she chooses to buy an iPad and 5 Oculus Go headsets. At the beginning of the tour, she shares the headsets to the visitors and keeps the iPad herself. During the tour around the factory, they often stop in front of a closed area to watch a video clip via the headsets. Example Mark is a teacher in elementary school. They have a rack of 20 Android tablets, which any of the teachers can reserve for their class. Mark wants to use 360-degree video clips around the World in his geography lessons. He shares 19 tablets to his students and uses one of them for controlling his presentation. Example Kate is participating in a voluntary project. They are documenting an old building that used to be a boarding school. Now it is about to get demolished. To preserve precious memories, they have shot a 360-degree image of each of the rooms of the building. The plan is to present them in a class reunion. The team has only a small budget, hence they have ordered 30 pieces of Google Cardboards. During the reunion, they will circulate among the former students and copy the photos to everyone's mobiles. Kate uses her own phone for controlling the presentation. Store vs. Beta \u00b6 The LiveSYNC app is installed to a device via the platform's own app store. On iOS it is Apple iTunes AppStore , on Android, it is Google Play Store , and on Oculus it is Oculus Store . The app version that is publicly available for everyone is called the store version . This is the recommended version for most users. LiveSYNC is continuously being developed further. New features are added and previous ones improved. Feedback from users is essential. Thus, new features are made first available via beta channels. Users who wish to try out new things and provide feedback can choose to install the beta version instead. Installing the beta version is only slightly different than installing the store version. Instructions are given separately for both later in this chapter. Note The store version and the beta version cannot exist simultaneously on a single device. For each device, you can install either but not both at the same time. Of course, it is possible to reinstall the app to get a different version. Note On some platforms, the maximum number of beta testers is limited by the platform. Hence, we may not be able to accept all requests to become a beta tester. The beta tester status may be given for a specific need such as testing a particular new feature. We may later need to remove the user from the beta channel (switch to the public store version). Example Jerome has a business where he sells 360-degree kits to retirement homes. They are used for relaxation and entertainment. Most popular are videos where old-style working methods are documented. Each of Jerome's kits consists of an Android tablet and an Oculus Go headset. The nurse helps in placing the headset comfortably to the senior's face. Then she selects from the tablet a video he wishes to see. Jerome uses the store version on kits that have been delivered. He uses the beta version on his own devices, just to stay on top of what's coming. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. They are building in several cities. Anthony wants to keep himself up-to-date without spending his days traveling. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. They email a PDF report to Anthony straight from LiveSYNC. They use the beta version because it has some new annotation features they wish to test. Roles \u00b6 When you are presenting content with LiveSYNC, each device adopts a role. A device acts either as the viewing device of an audience member or as the control device of the presenter. You can read more about this here . The device's role ( audience or director ) can be selected in the setup phase. When you create a channel configuration, one of the choices you make is the role that the device assumes. Since the role is configurable, the same app version can be used for both roles. However, the director role is not available in all versions of the app. For example, app versions made for VR headsets support only the audience role. In addition, the store version and the beta version may differ at times. The table below summarizes the current situation ie. which roles are available in which version. Platform Store version Beta version iOS Audience & Director Audience & Director Android Audience Audience & Director Oculus N/A Audience Note In a presentation, you can freely mix all kinds of devices. For example, an iOS tablet can control a mix of iOS phones & tablets, Android phones & tablets, GearVR headsets, and Oculus Go headsets. Some of the phones can be in VR mode and some not. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. Hence, the iPads act in the director role and the headsets act in the audience role. Joan configures two channels. The two groups will work in parallel without disturbing each other. Version Number \u00b6 New features and fixes are implemented and tested frequently. Then, a new software build is pushed to the release channels and release notes are published. Each build has a version number. To learn what has changed, take note of the version number from your app installation. Then compare it to the release notes document. To check which version (build) you have, start the LiveSYNC app and notice the version number from the splash screen. On iOS and Android, you can find the version number and other build details also in another way. Start the LiveSYNC app, navigate to Settings , then About , and scroll down the page near the end until you find Version section. When you send a support request from the LiveSYNC app, version information is pre-filled. The support email link appears just below version details. System Requirements \u00b6 Your hardware must meet the minimum technical specifications outlined below to run and use the LiveSYNC app. If you plan to purchase new hardware, consider our recommendations. iOS \u00b6 Feature Minimum requirement Recommendation Form factor iPhone or iPad 1 iPad for the director mode, either for the audience mode Model iPhone 6 2 or iPad Air 3 iPad 5th/6th gen. or any iPad Pro Operating system iOS 8.0 or later iOS 11.0 or later Accessories - AV adapter 4 Tip All iPad Pro models are much more expensive than the standard iPad. When using the LiveSYNC app, they are usually not worth the extra cost. Unless you prefer to have a larger screen. You cannot extend the storage size. LiveSYNC does not support memory sticks on iOS. Not even those that are sold as iOS compatible (they only work with the manufacturer's own app). Hence, we recommend that you invest in a model with large storage capacity. In most cases, the Wifi-only model is enough. Mobile LTE network can be useful if you plan to control other devices via GlobalSYNC and do not rely on Wifi availability. Android \u00b6 Feature Minimum requirement Recommendation Form factor Phone or Tablet 5 Tablet for the director mode, either for the audience mode Model n/a 6 Samsung Galaxy Tab S2/S3/S4 are known to work well Operating system Android 5.0 (API 21) or later Android 6.0 (API 23) or later Accessories - Micro SD card, Micro USB memory stick 7 Tip There is a huge selection of devices to choose from. Many low-end Android devices have poor display resolution, no support for 4K video playback, and no gyro sensor. Check at least these features before purchasing a new Android tablet. We recommend ~10-inch screen size for the director mode. You can extend the storage size if your device has an SD card slot. LiveSYNC supports SD card for media files. In most cases, the Wifi-only model is enough. Mobile LTE network can be useful if you plan to control other devices via GlobalSYNC and do not rely on Wifi availability. Warning LiveSYNC's director mode is more advanced on iOS than Android. If you plan to purchase a new director's device, we currently recommend iOS over Android. If you prefer to use Android, please check that the Android version of the app contains features that are relevant to you. It will take time before the Android version catches the iOS version in features. Oculus \u00b6 Feature Minimum requirement Recommendation Form factor GearVR or Oculus Go 8 Oculus Go 9 Model Galaxy S7/S7 Edge 10 Galaxy S8 & GearVR SM-324 (or newer), or Oculus Go Operating system Android 7.0 (API 24) or later Android 7.0 (API 24) or later Accessories - Micro SD card, Micro USB memory stick 11 Note You can try installing the LiveSYNC app also on models that are not officially supported. If the app is not visible in the store for a particular device, then that device is not supported (not even unofficially). If you use an unsupported device, performance may suffer and some features may not be available. Installing the Store Version \u00b6 The store version is the official release version of the LiveSYNC app. It is available via each platform's own app store. This is the recommended version for most users. iOS \u00b6 Tip If you are familiar with installing apps on iOS, you can use this direct link: https://itunes.apple.com/us/app/livesync-presentation-solution/id1202200449?mt=8 Option A: Watch a video installation guide Video not working? Click here to open it in Vimeo. Option B: Follow the steps described below Go to your iOS device and turn it on. When it starts, unlock it and enter the home screen. Look for App Store icon and tap it. From the bottom bar, select Search tab. Type livesync to the search box. Tap Search button. Select LiveSYNC Presentation Solution from the search results. Select Install button (cloud symbol with an arrow). Wait a moment. Once downloading and installation has completed, select Open . The app starts. It is now listed in your device's application menu. The next time you can start it from there. Android \u00b6 Tip If you are familiar with installing apps on Android, you can use this direct link: https://play.google.com/store/apps/details?id=fi.finwe.livesync.player.android&hl=en Option A: Watch a video installation guide Video not working? Click here to open it in Vimeo. Option B: Follow the steps described below Go to your Android device and turn it on. When it starts, unlock it and enter the home screen. Look for Play Store icon and tap it. Type livesync to the search box and tap Enter . (On some software keyboards it appears as the search button. You can recognize it from a magnifier icon.) Select LiveSYNC Presentation Solution from the search results. Select Install . Wait a moment. Once downloading and installation has completed, select Open . The app starts. It is now listed in your device's application menu. The next time you can start it from there. Oculus \u00b6 Info LiveSYNC has not officially launched on the Oculus platform yet. However, you can start using it already by installing it from our beta channel . Installing the Beta Version \u00b6 The beta version is the official test version of the LiveSYNC app. It is available via each platform's own beta distribution method. Beta version is recommended for users who are eager to try out new features and provide feedback for developers. Warning We test all releases internally before pushing them to beta channels. Be aware that beta versions are more likely to contain bugs. Feature and user interface changes occur frequently based on user feedback. What you see on a beta release may not end up to store release. Releases are published more often via beta channels. Also, documentation for new features may not be available at the time of release. iOS \u00b6 Follow the instructions below to install the beta version of the LiveSYNC app on your iOS device. Note Currently, we accept only customers to iOS beta channel. You must have a valid license to become a tester. Installing software from iOS beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for iOS. We need your email address, first name, and last name to be able to add you . Once you receive an invite email from Apple, accept the invite by clicking View in TestFlight . Notice that we must send each invite manually, so it can take a while before the email arrives. When you click the button in the email, a web page opens in the browser. Read the instructions and write down the code shown in step 3. Go to your iOS device and turn it on. When it starts, unlock it and enter the home screen. Look for App Store icon and tap it. From the bottom bar, select Search tab. Type testflight to the search box. Select Search . Select TestFlight from the search results. Select Install button (cloud symbol with an arrow). Wait a moment. Once downloading and installation has completed, select Open . TestFlight starts. Select Continue . Accept Apple's Terms of Service. Initially, you don't have any test apps. Select Redeem from the top right corner. Input here the code that you got in Step 3. LiveSYNC appears in your TestFlight apps list. Select INSTALL . Wait a moment. After downloading and installing has completed, select OPEN . The app starts. It is now listed in your device's application menu. The next time you can start it from there. Note When a new release is pushed to TestFlight, you will get a notification to your iPad and to your email. Unless you have disabled these notifications from TestFlight app. If you want to update to the new version, open TestFlight, select LiveSYNC, and select UPDATE . This is similar to Steps 14-16 above. Android \u00b6 Note Currently, there is no need to request access to the beta version on Android. It is open to everyone. Follow the instructions below to install the beta version of the LiveSYNC app on your Android device. Go to your Android device and turn it on. When it starts, unlock it and enter the home screen. Start the web browser ( Chrome or Internet ) and open this link: https://play.google.com/apps/testing/fi.finwe.livesync.player.android Once the following page appears, click the button that says BECOME A TESTER . You are now a beta tester. Read the instructions on the page. If you haven't installed the app yet, click the link that has been framed in the screen capture. Then follow normal Android install instructions from here , continuing from Step 4 onwards. Note If you want to switch back to the store version, uninstall the beta version and install the app again from the store. You are still part of the beta program and can switch back to the beta version at any time. If you want to leave the beta program completely, repeat steps 1 and 2. Then click the button that says LEAVE THE PROGRAM . You are not a beta tester anymore. Oculus \u00b6 Tip Read our full step-by-step tutorial about Oculus Go headset and learn how to use it with LiveSYNC. Follow the instructions below to install the beta version of the LiveSYNC app on your Oculus Go headset or GearVR compatible phone. The instructions are written for Oculus Go. When something differs for GearVR, it is mentioned. Notice that the installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). We will cover both methods. Installing software from Oculus beta channel requires an invite . First, check your Oculus username and email. Start the Oculus app on your phone, navigate to Settings (GearVR: More ), and find the username and email. See the image below for reference: Contact us and tell that you want to join LiveSYNC beta channel for Oculus. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an email from Oculus, accept the invite by clicking a confirmation link. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options on how to install the application: Using the Oculus app on your phone: tap the magnifier glass icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list (GearVR: check Not Installed tab). Select it from the list and then click Install on ... button. Using the Oculus Go / GearVR headset, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu (GearVR phone: My Apps tab from the top menu). Start LiveSYNC Oculus Go by selecting it from the apps grid. (On GearVR, you can start the app from the Oculus app on your phone, or you can first enter VR and then start it from the Oculus Home.) License \u00b6 Warning LiveSYNC is a commercial product. You can install it from the stores free of charge to try it out. Continuing using the product requires purchasing a license. Trial Period \u00b6 After installation, the app starts in the trial mode . This means that some features are not available. Watermarks are overlaid over your own content. We encourage you to test the app with your own devices. Take your time. For your convenience, the trial period is not limited. Of course, this does not mean that actively using the product without a license is allowed. After a reasonable trial period, you must purchase a license to continue using the product. Licensing Model \u00b6 A license needs to be purchased for a device that is being used for creating, editing or presenting content (the director role). Devices that are used only for viewing presentations do not need a license (the audience role). Example Kate is participating in a voluntary project. They have ordered 30 pieces of Google Cardboards. During the presentation, they will use everyone's own mobiles. Kate uses her personal phone for controlling the presentation. Kate is the only one who needs a license. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. They email a PDF report to Anthony straight from LiveSYNC. Each site manager is creating content and needs a license. Anthony does not, since he only receives PDF reports via email. The cost of the license depends on the selected features and subscription length. Check available license options from the app. You can also ask from your local distributor or Finwe (the makers of LiveSYNC). A license is valid for a limited period of time. Typically a subscription is purchased for 12 months at a time. Subscriptions purchased via in-app payment (from the app stores) are auto-renewing. You can manage these subscriptions via the app stores. Once a paid subscription ends the license becomes invalid and the app returns to the trial mode. Watermarks will appear, etc. Watermarks \u00b6 When you are using the product without a license, watermarks are overlaid over your own content. The watermarks disappear from all connected devices when the controlling device (the one running in the director role) has a valid license. Buying \u00b6 You can get a license directly via the app. It is easy, you can purchase a subscription using an in-app payment as follows: Start the LiveSYNC app. You will enter the Home screen. Notice the top bar. On the right side, you will find text that indicates the current license status. Player means the trial mode. Tap the cogwheel next to it. The Settings screen opens. Select Subscription page from the left side menu. Available license options will be shown. Tap the option that suits you best. The usual Apple AppStore / Google Play Store in-app payment process begins. Follow the instructions on the screen to complete the purchase. Once the purchase has completed, the license will be applied. The text in the top bar changes to indicate your new license type. Your subscription type is marked active. Its expiration date appears at the bottom of the license button. Note If you cannot use the in-app payment mechanism, ask for assistance from your local distributor or Finwe (the makers of LiveSYNC). The license can be activated also with a prepaid code. Note If you received the LiveSYNC app pre-installed with a kit such as the 360 Video Starter Kit, the software is probably already licensed. However, the license may need to be activated when you take the kit in use the very first time. Follow the instructions in the kit. If in doubt, please check from your reseller. License Code \u00b6 If you have received your license as a code, follow the steps below. A license code looks like this: FAZBB-PWYXP-XYSND-RACKR-JGGFX Start the LiveSYNC app. You will enter the Home screen. Notice the top bar. On the right side, you will find text that indicates the current license status. Player means the trial mode. Tap the cog wheel next to the text. The Settings view opens. Select About page from the left side menu. Scroll down the page until you see License Code section. Tap the link Enter license code . Type the code into the text field. Make sure to type it exactly as given, including the dashes '-'. Notice that the code is case sensitive. Select OK when ready. Note You need a working network connection to complete this step. In a few seconds, a pop-up asks your permission to take the new license in use. Select OK . The license will be applied and the text in the top bar changes to indicate your license type. Select Subscription page from the left side menu. Your subscription type is marked active. Its expiration date appears at the bottom of the license button. Store vs. Beta \u00b6 On iOS, the string that uniquely identifies an app installation is different between the store and the beta version. This is a platform feature. As a consequence, the same license cannot work on both installations. We recommend that you activate the license when the store version is installed . If you wish to use the beta version for a while, we can provide a temporary license code that will work with the beta version. Locked Features \u00b6 Some features can be locked from use based on the active license type. For example, a feature may be reserved for a customer who has funded developing that feature (for a limited period of time). In another example, a customer may request that a certain feature is disabled for security reasons. An example could be insecure sharing via email. To avoid confusion, locked features are usually hidden from the app. Enterprise Features \u00b6 Enterprise features are not included in the standard license that you can purchase from the app stores. They are always enabled with a license code or a license file. 360 Video Starter Kit \u00b6 The kit contains a tablet where the LiveSYNC app is pre-installed. The bundled license only needs to be activated. Follow the instructions that are included in the kit. iPhone and iPad are supported. iPod Touch and Apple TV are iOS devices but not supported. Apple Watch and Apple MacBook use different operating systems and are not supported. \u21a9 iPhone 6/6+/6s/6s+/7/7+/8/8+/X/XR/XS/XS Max are supported. iPhone 2007/3G/3GS/4/4s/5/5c/5s/SE are not supported. See iPhone models for reference. \u21a9 iPad Air/Air 2/5th gen./6th gen./Pro (all) are supported. iPad 2010/2/3rd gen./4th gen./mini/mini 2/mini 3/mini 4 are not supported. See iPhone models for reference. \u21a9 You can use the AV adapter for sharing your iPad's screen to a big screen via an HDMI cable. See product info for reference. \u21a9 Android phones and tablets are supported. Android TV, Android Auto, and Android Wear devices are not supported. Chromebook uses a different operating system and is not supported. \u21a9 There are thousands of supported models. It is not possible to list them all here. \u21a9 If your device has a micro SD card slot, you can expand storage size (LiveSYNC supports SD card for media files). A micro USB to a USB memory stick (with OTG feature) is handy for copying media files from PC/Mac to tablet. \u21a9 Oculus Quest will likely be supported later. Oculus Rift and HTC Vive are not supported. \u21a9 GearVR is also OK, but we prefer Oculus Go as it is a standalone low-cost device. \u21a9 GearVR compatible phones older than Galaxy S7 tend to overheat quickly. \u21a9 If your device has a micro SD card slot, you can expand storage size (LiveSYNC supports SD card for media files). A micro USB to a USB memory stick (with OTG feature) is handy for copying media files from PC/Mac to tablet. \u21a9","title":"2. Installing"},{"location":"user_guide/installing/#2-installing","text":"","title":"2. Installing"},{"location":"user_guide/installing/#versions","text":"The LiveSYNC app is available as different versions . Read this section first to understand which version is best for you and what hardware you need for using it.","title":"Versions"},{"location":"user_guide/installing/#platforms","text":"The LiveSYNC app can be installed and run on multiple platforms . Currently, the supported platforms include iOS , Android , and Oculus . Each platform has its own version of the app. They are compatible with each other, but not exactly the same. The app works on multiple device types . Currently, the supported device types include phones , tablets , and VR headsets . All these are great for viewing content. A tablet is recommended for creating, editing, and presenting content. Some device types allow viewing content in VR mode via a passive or an active VR headset. The availability of VR mode varies. Tablets are not physically compatible with VR headsets. Hence, the VR mode is not available on tablets. On phones, the VR mode is selectable . You can view content via the touchscreen but you can also put the phone inside a passive VR headset. For example, Google Cardboard or compatible. On devices that were made for VR, such as GearVR and Oculus Go, the VR mode is always active. The supported device types and the availability of the VR mode are summarized below. Platform Device Type VR Mode iOS iPhone, and iPad Selectable on iPhone, N/A on iPad Android Phones, and tablets Selectable on phones, N/A on tablets Oculus VR headset Always on Example Tina works in the manufacturing industry. Clients often come to visit the factory. They are taken to a tour in groups of five. For safety reasons, some parts of the factory are closed from visitors. Tina wants to show 360-degree video clips from the closed areas. Hence, she chooses to buy an iPad and 5 Oculus Go headsets. At the beginning of the tour, she shares the headsets to the visitors and keeps the iPad herself. During the tour around the factory, they often stop in front of a closed area to watch a video clip via the headsets. Example Mark is a teacher in elementary school. They have a rack of 20 Android tablets, which any of the teachers can reserve for their class. Mark wants to use 360-degree video clips around the World in his geography lessons. He shares 19 tablets to his students and uses one of them for controlling his presentation. Example Kate is participating in a voluntary project. They are documenting an old building that used to be a boarding school. Now it is about to get demolished. To preserve precious memories, they have shot a 360-degree image of each of the rooms of the building. The plan is to present them in a class reunion. The team has only a small budget, hence they have ordered 30 pieces of Google Cardboards. During the reunion, they will circulate among the former students and copy the photos to everyone's mobiles. Kate uses her own phone for controlling the presentation.","title":"Platforms"},{"location":"user_guide/installing/#store-vs-beta","text":"The LiveSYNC app is installed to a device via the platform's own app store. On iOS it is Apple iTunes AppStore , on Android, it is Google Play Store , and on Oculus it is Oculus Store . The app version that is publicly available for everyone is called the store version . This is the recommended version for most users. LiveSYNC is continuously being developed further. New features are added and previous ones improved. Feedback from users is essential. Thus, new features are made first available via beta channels. Users who wish to try out new things and provide feedback can choose to install the beta version instead. Installing the beta version is only slightly different than installing the store version. Instructions are given separately for both later in this chapter. Note The store version and the beta version cannot exist simultaneously on a single device. For each device, you can install either but not both at the same time. Of course, it is possible to reinstall the app to get a different version. Note On some platforms, the maximum number of beta testers is limited by the platform. Hence, we may not be able to accept all requests to become a beta tester. The beta tester status may be given for a specific need such as testing a particular new feature. We may later need to remove the user from the beta channel (switch to the public store version). Example Jerome has a business where he sells 360-degree kits to retirement homes. They are used for relaxation and entertainment. Most popular are videos where old-style working methods are documented. Each of Jerome's kits consists of an Android tablet and an Oculus Go headset. The nurse helps in placing the headset comfortably to the senior's face. Then she selects from the tablet a video he wishes to see. Jerome uses the store version on kits that have been delivered. He uses the beta version on his own devices, just to stay on top of what's coming. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. They are building in several cities. Anthony wants to keep himself up-to-date without spending his days traveling. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. They email a PDF report to Anthony straight from LiveSYNC. They use the beta version because it has some new annotation features they wish to test.","title":"Store vs. Beta"},{"location":"user_guide/installing/#roles","text":"When you are presenting content with LiveSYNC, each device adopts a role. A device acts either as the viewing device of an audience member or as the control device of the presenter. You can read more about this here . The device's role ( audience or director ) can be selected in the setup phase. When you create a channel configuration, one of the choices you make is the role that the device assumes. Since the role is configurable, the same app version can be used for both roles. However, the director role is not available in all versions of the app. For example, app versions made for VR headsets support only the audience role. In addition, the store version and the beta version may differ at times. The table below summarizes the current situation ie. which roles are available in which version. Platform Store version Beta version iOS Audience & Director Audience & Director Android Audience Audience & Director Oculus N/A Audience Note In a presentation, you can freely mix all kinds of devices. For example, an iOS tablet can control a mix of iOS phones & tablets, Android phones & tablets, GearVR headsets, and Oculus Go headsets. Some of the phones can be in VR mode and some not. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. Hence, the iPads act in the director role and the headsets act in the audience role. Joan configures two channels. The two groups will work in parallel without disturbing each other.","title":"Roles"},{"location":"user_guide/installing/#version-number","text":"New features and fixes are implemented and tested frequently. Then, a new software build is pushed to the release channels and release notes are published. Each build has a version number. To learn what has changed, take note of the version number from your app installation. Then compare it to the release notes document. To check which version (build) you have, start the LiveSYNC app and notice the version number from the splash screen. On iOS and Android, you can find the version number and other build details also in another way. Start the LiveSYNC app, navigate to Settings , then About , and scroll down the page near the end until you find Version section. When you send a support request from the LiveSYNC app, version information is pre-filled. The support email link appears just below version details.","title":"Version Number"},{"location":"user_guide/installing/#system-requirements","text":"Your hardware must meet the minimum technical specifications outlined below to run and use the LiveSYNC app. If you plan to purchase new hardware, consider our recommendations.","title":"System Requirements"},{"location":"user_guide/installing/#ios","text":"Feature Minimum requirement Recommendation Form factor iPhone or iPad 1 iPad for the director mode, either for the audience mode Model iPhone 6 2 or iPad Air 3 iPad 5th/6th gen. or any iPad Pro Operating system iOS 8.0 or later iOS 11.0 or later Accessories - AV adapter 4 Tip All iPad Pro models are much more expensive than the standard iPad. When using the LiveSYNC app, they are usually not worth the extra cost. Unless you prefer to have a larger screen. You cannot extend the storage size. LiveSYNC does not support memory sticks on iOS. Not even those that are sold as iOS compatible (they only work with the manufacturer's own app). Hence, we recommend that you invest in a model with large storage capacity. In most cases, the Wifi-only model is enough. Mobile LTE network can be useful if you plan to control other devices via GlobalSYNC and do not rely on Wifi availability.","title":"iOS"},{"location":"user_guide/installing/#android","text":"Feature Minimum requirement Recommendation Form factor Phone or Tablet 5 Tablet for the director mode, either for the audience mode Model n/a 6 Samsung Galaxy Tab S2/S3/S4 are known to work well Operating system Android 5.0 (API 21) or later Android 6.0 (API 23) or later Accessories - Micro SD card, Micro USB memory stick 7 Tip There is a huge selection of devices to choose from. Many low-end Android devices have poor display resolution, no support for 4K video playback, and no gyro sensor. Check at least these features before purchasing a new Android tablet. We recommend ~10-inch screen size for the director mode. You can extend the storage size if your device has an SD card slot. LiveSYNC supports SD card for media files. In most cases, the Wifi-only model is enough. Mobile LTE network can be useful if you plan to control other devices via GlobalSYNC and do not rely on Wifi availability. Warning LiveSYNC's director mode is more advanced on iOS than Android. If you plan to purchase a new director's device, we currently recommend iOS over Android. If you prefer to use Android, please check that the Android version of the app contains features that are relevant to you. It will take time before the Android version catches the iOS version in features.","title":"Android"},{"location":"user_guide/installing/#oculus","text":"Feature Minimum requirement Recommendation Form factor GearVR or Oculus Go 8 Oculus Go 9 Model Galaxy S7/S7 Edge 10 Galaxy S8 & GearVR SM-324 (or newer), or Oculus Go Operating system Android 7.0 (API 24) or later Android 7.0 (API 24) or later Accessories - Micro SD card, Micro USB memory stick 11 Note You can try installing the LiveSYNC app also on models that are not officially supported. If the app is not visible in the store for a particular device, then that device is not supported (not even unofficially). If you use an unsupported device, performance may suffer and some features may not be available.","title":"Oculus"},{"location":"user_guide/installing/#installing-the-store-version","text":"The store version is the official release version of the LiveSYNC app. It is available via each platform's own app store. This is the recommended version for most users.","title":"Installing the Store Version"},{"location":"user_guide/installing/#ios_1","text":"Tip If you are familiar with installing apps on iOS, you can use this direct link: https://itunes.apple.com/us/app/livesync-presentation-solution/id1202200449?mt=8 Option A: Watch a video installation guide Video not working? Click here to open it in Vimeo. Option B: Follow the steps described below Go to your iOS device and turn it on. When it starts, unlock it and enter the home screen. Look for App Store icon and tap it. From the bottom bar, select Search tab. Type livesync to the search box. Tap Search button. Select LiveSYNC Presentation Solution from the search results. Select Install button (cloud symbol with an arrow). Wait a moment. Once downloading and installation has completed, select Open . The app starts. It is now listed in your device's application menu. The next time you can start it from there.","title":"iOS"},{"location":"user_guide/installing/#android_1","text":"Tip If you are familiar with installing apps on Android, you can use this direct link: https://play.google.com/store/apps/details?id=fi.finwe.livesync.player.android&hl=en Option A: Watch a video installation guide Video not working? Click here to open it in Vimeo. Option B: Follow the steps described below Go to your Android device and turn it on. When it starts, unlock it and enter the home screen. Look for Play Store icon and tap it. Type livesync to the search box and tap Enter . (On some software keyboards it appears as the search button. You can recognize it from a magnifier icon.) Select LiveSYNC Presentation Solution from the search results. Select Install . Wait a moment. Once downloading and installation has completed, select Open . The app starts. It is now listed in your device's application menu. The next time you can start it from there.","title":"Android"},{"location":"user_guide/installing/#oculus_1","text":"Info LiveSYNC has not officially launched on the Oculus platform yet. However, you can start using it already by installing it from our beta channel .","title":"Oculus"},{"location":"user_guide/installing/#installing-the-beta-version","text":"The beta version is the official test version of the LiveSYNC app. It is available via each platform's own beta distribution method. Beta version is recommended for users who are eager to try out new features and provide feedback for developers. Warning We test all releases internally before pushing them to beta channels. Be aware that beta versions are more likely to contain bugs. Feature and user interface changes occur frequently based on user feedback. What you see on a beta release may not end up to store release. Releases are published more often via beta channels. Also, documentation for new features may not be available at the time of release.","title":"Installing the Beta Version"},{"location":"user_guide/installing/#ios_2","text":"Follow the instructions below to install the beta version of the LiveSYNC app on your iOS device. Note Currently, we accept only customers to iOS beta channel. You must have a valid license to become a tester. Installing software from iOS beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for iOS. We need your email address, first name, and last name to be able to add you . Once you receive an invite email from Apple, accept the invite by clicking View in TestFlight . Notice that we must send each invite manually, so it can take a while before the email arrives. When you click the button in the email, a web page opens in the browser. Read the instructions and write down the code shown in step 3. Go to your iOS device and turn it on. When it starts, unlock it and enter the home screen. Look for App Store icon and tap it. From the bottom bar, select Search tab. Type testflight to the search box. Select Search . Select TestFlight from the search results. Select Install button (cloud symbol with an arrow). Wait a moment. Once downloading and installation has completed, select Open . TestFlight starts. Select Continue . Accept Apple's Terms of Service. Initially, you don't have any test apps. Select Redeem from the top right corner. Input here the code that you got in Step 3. LiveSYNC appears in your TestFlight apps list. Select INSTALL . Wait a moment. After downloading and installing has completed, select OPEN . The app starts. It is now listed in your device's application menu. The next time you can start it from there. Note When a new release is pushed to TestFlight, you will get a notification to your iPad and to your email. Unless you have disabled these notifications from TestFlight app. If you want to update to the new version, open TestFlight, select LiveSYNC, and select UPDATE . This is similar to Steps 14-16 above.","title":"iOS"},{"location":"user_guide/installing/#android_2","text":"Note Currently, there is no need to request access to the beta version on Android. It is open to everyone. Follow the instructions below to install the beta version of the LiveSYNC app on your Android device. Go to your Android device and turn it on. When it starts, unlock it and enter the home screen. Start the web browser ( Chrome or Internet ) and open this link: https://play.google.com/apps/testing/fi.finwe.livesync.player.android Once the following page appears, click the button that says BECOME A TESTER . You are now a beta tester. Read the instructions on the page. If you haven't installed the app yet, click the link that has been framed in the screen capture. Then follow normal Android install instructions from here , continuing from Step 4 onwards. Note If you want to switch back to the store version, uninstall the beta version and install the app again from the store. You are still part of the beta program and can switch back to the beta version at any time. If you want to leave the beta program completely, repeat steps 1 and 2. Then click the button that says LEAVE THE PROGRAM . You are not a beta tester anymore.","title":"Android"},{"location":"user_guide/installing/#oculus_2","text":"Tip Read our full step-by-step tutorial about Oculus Go headset and learn how to use it with LiveSYNC. Follow the instructions below to install the beta version of the LiveSYNC app on your Oculus Go headset or GearVR compatible phone. The instructions are written for Oculus Go. When something differs for GearVR, it is mentioned. Notice that the installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). We will cover both methods. Installing software from Oculus beta channel requires an invite . First, check your Oculus username and email. Start the Oculus app on your phone, navigate to Settings (GearVR: More ), and find the username and email. See the image below for reference: Contact us and tell that you want to join LiveSYNC beta channel for Oculus. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an email from Oculus, accept the invite by clicking a confirmation link. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options on how to install the application: Using the Oculus app on your phone: tap the magnifier glass icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list (GearVR: check Not Installed tab). Select it from the list and then click Install on ... button. Using the Oculus Go / GearVR headset, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu (GearVR phone: My Apps tab from the top menu). Start LiveSYNC Oculus Go by selecting it from the apps grid. (On GearVR, you can start the app from the Oculus app on your phone, or you can first enter VR and then start it from the Oculus Home.)","title":"Oculus"},{"location":"user_guide/installing/#license","text":"Warning LiveSYNC is a commercial product. You can install it from the stores free of charge to try it out. Continuing using the product requires purchasing a license.","title":"License"},{"location":"user_guide/installing/#trial-period","text":"After installation, the app starts in the trial mode . This means that some features are not available. Watermarks are overlaid over your own content. We encourage you to test the app with your own devices. Take your time. For your convenience, the trial period is not limited. Of course, this does not mean that actively using the product without a license is allowed. After a reasonable trial period, you must purchase a license to continue using the product.","title":"Trial Period"},{"location":"user_guide/installing/#licensing-model","text":"A license needs to be purchased for a device that is being used for creating, editing or presenting content (the director role). Devices that are used only for viewing presentations do not need a license (the audience role). Example Kate is participating in a voluntary project. They have ordered 30 pieces of Google Cardboards. During the presentation, they will use everyone's own mobiles. Kate uses her personal phone for controlling the presentation. Kate is the only one who needs a license. Example Anthony has purchased a set of 360 Video Starter kits for his construction company. Twice per week his site managers take a set of 360-degree photos and annotate the progress and next tasks. They email a PDF report to Anthony straight from LiveSYNC. Each site manager is creating content and needs a license. Anthony does not, since he only receives PDF reports via email. The cost of the license depends on the selected features and subscription length. Check available license options from the app. You can also ask from your local distributor or Finwe (the makers of LiveSYNC). A license is valid for a limited period of time. Typically a subscription is purchased for 12 months at a time. Subscriptions purchased via in-app payment (from the app stores) are auto-renewing. You can manage these subscriptions via the app stores. Once a paid subscription ends the license becomes invalid and the app returns to the trial mode. Watermarks will appear, etc.","title":"Licensing Model"},{"location":"user_guide/installing/#watermarks","text":"When you are using the product without a license, watermarks are overlaid over your own content. The watermarks disappear from all connected devices when the controlling device (the one running in the director role) has a valid license.","title":"Watermarks"},{"location":"user_guide/installing/#buying","text":"You can get a license directly via the app. It is easy, you can purchase a subscription using an in-app payment as follows: Start the LiveSYNC app. You will enter the Home screen. Notice the top bar. On the right side, you will find text that indicates the current license status. Player means the trial mode. Tap the cogwheel next to it. The Settings screen opens. Select Subscription page from the left side menu. Available license options will be shown. Tap the option that suits you best. The usual Apple AppStore / Google Play Store in-app payment process begins. Follow the instructions on the screen to complete the purchase. Once the purchase has completed, the license will be applied. The text in the top bar changes to indicate your new license type. Your subscription type is marked active. Its expiration date appears at the bottom of the license button. Note If you cannot use the in-app payment mechanism, ask for assistance from your local distributor or Finwe (the makers of LiveSYNC). The license can be activated also with a prepaid code. Note If you received the LiveSYNC app pre-installed with a kit such as the 360 Video Starter Kit, the software is probably already licensed. However, the license may need to be activated when you take the kit in use the very first time. Follow the instructions in the kit. If in doubt, please check from your reseller.","title":"Buying"},{"location":"user_guide/installing/#license-code","text":"If you have received your license as a code, follow the steps below. A license code looks like this: FAZBB-PWYXP-XYSND-RACKR-JGGFX Start the LiveSYNC app. You will enter the Home screen. Notice the top bar. On the right side, you will find text that indicates the current license status. Player means the trial mode. Tap the cog wheel next to the text. The Settings view opens. Select About page from the left side menu. Scroll down the page until you see License Code section. Tap the link Enter license code . Type the code into the text field. Make sure to type it exactly as given, including the dashes '-'. Notice that the code is case sensitive. Select OK when ready. Note You need a working network connection to complete this step. In a few seconds, a pop-up asks your permission to take the new license in use. Select OK . The license will be applied and the text in the top bar changes to indicate your license type. Select Subscription page from the left side menu. Your subscription type is marked active. Its expiration date appears at the bottom of the license button.","title":"License Code"},{"location":"user_guide/installing/#store-vs-beta_1","text":"On iOS, the string that uniquely identifies an app installation is different between the store and the beta version. This is a platform feature. As a consequence, the same license cannot work on both installations. We recommend that you activate the license when the store version is installed . If you wish to use the beta version for a while, we can provide a temporary license code that will work with the beta version.","title":"Store vs. Beta"},{"location":"user_guide/installing/#locked-features","text":"Some features can be locked from use based on the active license type. For example, a feature may be reserved for a customer who has funded developing that feature (for a limited period of time). In another example, a customer may request that a certain feature is disabled for security reasons. An example could be insecure sharing via email. To avoid confusion, locked features are usually hidden from the app.","title":"Locked Features"},{"location":"user_guide/installing/#enterprise-features","text":"Enterprise features are not included in the standard license that you can purchase from the app stores. They are always enabled with a license code or a license file.","title":"Enterprise Features"},{"location":"user_guide/installing/#360-video-starter-kit","text":"The kit contains a tablet where the LiveSYNC app is pre-installed. The bundled license only needs to be activated. Follow the instructions that are included in the kit. iPhone and iPad are supported. iPod Touch and Apple TV are iOS devices but not supported. Apple Watch and Apple MacBook use different operating systems and are not supported. \u21a9 iPhone 6/6+/6s/6s+/7/7+/8/8+/X/XR/XS/XS Max are supported. iPhone 2007/3G/3GS/4/4s/5/5c/5s/SE are not supported. See iPhone models for reference. \u21a9 iPad Air/Air 2/5th gen./6th gen./Pro (all) are supported. iPad 2010/2/3rd gen./4th gen./mini/mini 2/mini 3/mini 4 are not supported. See iPhone models for reference. \u21a9 You can use the AV adapter for sharing your iPad's screen to a big screen via an HDMI cable. See product info for reference. \u21a9 Android phones and tablets are supported. Android TV, Android Auto, and Android Wear devices are not supported. Chromebook uses a different operating system and is not supported. \u21a9 There are thousands of supported models. It is not possible to list them all here. \u21a9 If your device has a micro SD card slot, you can expand storage size (LiveSYNC supports SD card for media files). A micro USB to a USB memory stick (with OTG feature) is handy for copying media files from PC/Mac to tablet. \u21a9 Oculus Quest will likely be supported later. Oculus Rift and HTC Vive are not supported. \u21a9 GearVR is also OK, but we prefer Oculus Go as it is a standalone low-cost device. \u21a9 GearVR compatible phones older than Galaxy S7 tend to overheat quickly. \u21a9 If your device has a micro SD card slot, you can expand storage size (LiveSYNC supports SD card for media files). A micro USB to a USB memory stick (with OTG feature) is handy for copying media files from PC/Mac to tablet. \u21a9","title":"360 Video Starter Kit"},{"location":"user_guide/introduction/","text":"1. Introduction \u00b6 Welcome to LiveSYNC \u00b6 Since you are reading this user guide, you must have decided to learn more about 360-degree photos, videos, and a tool called LiveSYNC. That is awesome! We, too, believe it is time to finally get rid of the limitations of the image frame. It is time to start capturing the World as we humans experience it. In a way that a viewer can look around and feel like being there . In other words, in 360-degrees. To make most out of 360-degree content you need proper tools. Not only cameras and software for creating content. It is even more important what comes after that: how you will utilize your content. This means carrying out tasks such as annotating, presenting and sharing. This is also where LiveSYNC as a product comes to picture. You will be surprised how much you can achieve with it. We will start with the big picture to learn why this is the right time to start using 360-degree content. Then, we will discuss how LiveSYNC was born and what needs it was built to satisfy. We will conclude this chapter by going through some key concepts in LiveSYNC. Tip If you can't wait to start using the app, begin reading from Chapter 2. Installing . Continuing from there provides answers to how -questions. Return here when you are ready to find answers to why -questions. The Big Picture \u00b6 Let's begin by taking a few steps back to see the big picture. We will discuss trends in human communication and photography, and how 360-degree imaging fits in. Next, we go through recent advancements in camera and network technologies, and how they make 360-degree content creation and delivery feasible. We will paint a picture of where we seem to be going, and learn what role 360-degree imaging has in the long run. Communication Becomes Visual \u00b6 During the past few decades, the way humans communicate with each other has changed. First, we learned to connect computers together locally, then globally. We made a giant web of wires where messages can travel from one corner of the planet to another in a fraction of a second. Building Internet was a major breakthrough in communication. In its early days, bandwidth was low and it was practical only for transmitting text . You had to pay for usage by the minute, so it was wise to write an email first and go online only to send it. How odd that must sound to the children of the current age! Along the years, many technological advancements made the web faster and faster. We started to use it for other forms of communication that need higher bandwidth. Images and GIF animations started to appear on web pages. Kids learned to share music as downloadable MP3s, then full-length movies. Low-latency streaming changed the landscape even more. Voice calls via Skype started to become popular. On-demand video rentals began, and Youtube showed up. Eventually, even traditional media gave up. They brought newspapers, radio shows, and live TV online. All forms of digital communication became available for a single low-cost data plan. Fixed monthly price made it possible to stay online all day long. As a consequence, we saw a paradigm shift in human communication. Our messaging began to change from textual to visual representation . Young people didn't bother to write long, well thought out emails. They chose to send lots of short messages that were read and replied almost instantly. Tiny images called smileys were added by combining special characters. They were used to communicate common feelings by mimicking facial expressions. Then more complicated, graphical emojis arrived and teens began to skip words. They communicated with cryptic abbreviations and sequences of emojis. Kind of a modern version of the Egyptian hieroglyphs! Later, textual part in many messages had reduced to a caption for a photograph. Now the image had become the message itself. Then we moved on to video. The age of Youtube stars had begun: anyone could start their own TV show with a press of a button. The world now is more visual that it has ever been in human history. 93% of all human communication is visual. (1) Why are we so fond of photos and videos? What is wrong with plain old books and letters, you may ask. There is room for all kinds of communication methods. The new ones do not replace the old ones; they add to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of the visual kind. We can even argue that humans are visual by nature: Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. (1) To simplify it, the more visual a communication method is the faster we understand the message it carries . Consider an ordinary video. If that small window is so effective, imagine the impact of widening the view to full 360 degrees. Then go further and show it via a virtual reality headset. It is telling that afterward people use words like when I was there , not when I watched that . This is a significant difference. The viewer does not see himself as an outsider, a spectator, but part of the story. Immersion is so powerful that it can even create false memories (2) . Another research shows that people recall information better through virtual reality (3) . That seems logical: our brains are much better in remembering events that happen to us than stories that are told to us . It is clear that 360-degree video is a very effective communication method. Interactivity Arouses Interest \u00b6 The advances in imaging technology are pushing another paradigm shift. Traditionally, most features of a photograph had to be decided in advance ie. before the camera shutter closed. Now more and more of these can be changed after taking the shot. Careful planning, expensive camera gear, and interest to technical details still have value but are not mandatory anymore. Ordinary people succeed fairly well even in difficult circumstances. And they enjoy the current interactive nature of photography, where many features are not designed in advance but selected based on their effect to the image. As a consequence, photography is more popular than ever before. Since the beginning of digital imaging cameras have developed in fast forward. We have reached a point where we have to ask again: what exactly is a camera? Or, who exactly is the photographer? Consider new phones that contain many physical camera modules. When you press the trigger, a set of images is captured. The camera modules trigger simultaneously and take many shots in sequence. Then, artificial intelligence is used for composing one perfect shot. Some of the frames are retrieved from a memory buffer and were taken before you pressed the trigger. Such a camera is a complex combination of hardware, software, and mathematical algorithms. The software is adopting the role of the photographer. It has become a decision maker in the creation process. So, can we still justify calling the person that pressed the trigger the photographer? All he did was point the camera to a certain direction! It is not surprising that also framing the shot is becoming history. Or, more specifically, we can leave that decision to the viewer of the image. In the 360-degree video, every direction is captured. There are no traces of a camera, a cameraman, or even a tripod. It is a kind of a miracle. Now there is no need to decide even where to point the camera, as everything around the camera can be captured. This has some profound effects, as we will soon see. Let's take a closer look at this technology. Traditionally, a photographer or a videographer has decided how to frame the shot. What is necessary, what can be left out. This decision is locked at the time of capture. It becomes a permanent limitation. At viewing time, it is not possible to see beyond the frame. In fact, the only possibility to re-frame the shot is to limit the viewing angle even more by cropping the image (ie. digital zoom in). The benefit is that the photographer has the power to make his audience focus on exactly what he wants them to see. But that can also become a lie. Beautiful places you see in photos and movies often look very different when you visit them in real life. For example, the pyramids of Giza are always photographed so that they appear to stand in the middle of a desert. If you turn around, there is Cairo with its 20 million inhabitants! Capturing in 360-degrees tends to show a much more realistic view of the World. But how exactly are those images made? Imagine a perfect zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what will happen if you keep zooming out? You will see more and more of the surrounding area become visible in the image. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. A 360-degree camera is essentially a camera with a very wide angle lens. When you are zooming in, there is no natural limit where to stop. But when you are zooming out, such a limit does exist: it is where the opposing sides of the image meet. In that sense, a 360-degree camera has a perfect wide-angle lens: we cannot expand the view any further. We may never be able to manufacture a single lens that could capture everything at once. Yet, 360-degree cameras do exist. How is it possible, then? Again, digital image processing comes to the rescue! Almost seamless representation can be achieved by stitching together many images. These are captured with two or more lenses. Current 360-degree photo and video panoramas capture the world in every direction. In that sense, they are perfect: everything around the camera is in the image. In reality, there is a small distance around the camera where an object can be hidden between the lenses. This has one useful side effect: it allows hiding a small tripod or a selfie stick that is holding the camera. What is the main difference in 360-degree imaging compared to old school photos? During playback, the viewer is in control of the viewing direction . He can turn around and even look up to the sky or down to his feet. It feels like magic when you experience this freedom the very first time. But soon you will get used to it and begin to expect that all images work this way. So much, that you feel annoyed when you attempt to pan an image and it won't budge; it is a plain old 2D image. You've probably seen how today's toddlers expect that every screen is a touch screen. Likewise, you begin to assume that all images can be panned. This interactivity is what makes consuming 360-degree media so interesting to us . The ability to explore. 360 Cameras for All \u00b6 360-degree photography has come a long way in a short time. Only a few years ago it was unheard of to the general audience. Yet, we can hardly say that 360-degree photographs are anything new . Devoted photographers have been shooting in 360 degrees for quite some time. For a long time, you had to love this form of art to go through all the trouble. Capturing 360-degree photographs required purchasing a lot of camera gear. A DSLR camera, an expensive fisheye lens, and a tripod with a special panoramic head. To create a single image, you had to take multiple shots. Then move the files from your camera to a PC and run them through a special stitching software. You could spend half an hour adding control points, masking the tripod, and many other steps. It took several minutes from a powerful PC to render the image into a file. After months of trial-and-error, you learned to tune your camera gear and shooting process. Quite a learning curve! This is how professionals still work. Of course, everything has become faster, easier, and better looking. Yet, the real revolution was the arrival of the integrated consumer and prosumer level 360-degree cameras. With a few hundred dollars you can now buy a point-and-shoot camera and control it with your phone. The phone also stitches the image and does a pretty good job. The result is not comparable to a professionally made panorama. But, it is quick, cheap, and easy enough for anyone to make. Moreover, it is good enough to be useful . That is the recipe for general adoption. We cannot talk about 360-degree photography without also discussing 360-degree videos. It is, after all, a natural step forward. If you can create a single 360-degree photograph, you can also create a video. Just capture lots of photos in sequence, right? It is not that easy in practice. For video, you need many cameras which need to be synchronized. Because of their physical size, the cameras cannot be in the optimal pivoting point. You must deal with parallax and hence more difficult stitching issues. GoPro action cameras used to be very popular for 360-degree video. They were small, affordable, reliable, and produced good image quality. Popular layouts required using six cameras. Not one of them could fail without ruining the whole shot. Today, integrated 360-degree cameras capture video, too. All you need to do is press the REC button. After transferring the file to your phone, let it stitch the video together. You can even do live streams, straight from your phone to the tubes. 360-degree camera technology definitely has matured. Monoscopic 360-degree video can be considered a solved problem, and it is ready to be used by anyone. Stereoscopic (3D) 360-degree video is only a little behind. Note In the monoscopic 360-degree video there is no depth information. When viewed through a virtual reality headset both eyes see the same image. In the stereoscopic 360-degree video the left and the right eye see a different image. The scene appears to contain depth ie. it is three dimensional. The difference is the same than in movie theaters, where you choose between 2D or 3D screening. Bandwidth Brings Presence \u00b6 Being able to look around is not all that it takes to make us feel being there . Not even if the image is stereoscopic. Another key to immersion is the amount of detail . It is another aspect of the amount of information an image represents. A 360-degree image covers the whole field-of-view. But we must also preserve as much detail per degree as possible to make the image look realistic. Or, at least as much as a human eye can distinguish! This is more easily said than done since a human eye is an amazing camera and a lens in a very compact form. It is able to capture an enormous amount of information in a fraction of a second. This brings us back to bandwidth and the Internet. Whenever transfer rates take a step forward, we find a richer communication method. Something that provides a better way to capture and share what is happening around us. Often it is a way that communicates presence better. First, we had to describe everything in text, then we could add images and photos, then sound and music, then video... Can you imagine what will be the next step on this path? One way to understand it all is to see the changes as stepping stones towards telepresence . Fancy word, but it is only a combination of technologies that allow a person to feel as if they were present in another place. And maybe also in another time. That other place can be real or imaginary. This is where we are heading. And step by step, that feeling of being there is becoming stronger and stronger. When we are reading books, we see the story unfold in our own imagination. When we are looking at photos, we see snapshots and imagine the rest. When we are watching movies, we see what someone else has already imagined. When we are wearing a VR headset, we become eyewitnesses to the story. Note Notice that we are not talking about a solution where a human would not be able to recognize if the world is real or virtual. Telepresence is good enough already when we allow ourselves to forget the fact that it isn't real. Tracing the path backward from the goal to the present, we are not there yet but not that far off either. During the past few years, technology has matured enough to solve one profound roadblock. This is the image frame that limits our field of view. Think about it: how could you ever feel being there if you weren't even able to turn your head to look around? Least, you should be able to observe the surroundings by looking in any direction you want. And experience the view and soundscape change corresponding to your head movements. Virtual reality headsets are one way to make this workable. Computer games and movies are an endless source for imaginary worlds. They are the driving force for virtual reality. However, the real world surrounding us must not be forgotten: it is, after all, where we live in! It can be documented with 360-degree photography and 360-degree videography. Accompanied by 360-degree audio, perhaps. Of course, 360-degree cameras and VR headsets are not enough. They are solutions for content creation and consumption. We must also be able to transmit the content from the camera to the headsets. But, moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate. It is not coincident that 360-degree content is becoming popular now . Internet is just becoming fast enough to carry the amount of data 360-degree video requires. Traditional photos are great for saving memories. Videos are perfect for telling stories and documenting events. What is the main reason for shooting something in 360-degrees? It is the feeling of presence . And it is much stronger when the video is combined with 360-degree audio and viewed through a VR headset. The memories are brought alive as space and time where you can step inside. You'll feel like being there . This feeling of (tele)presence is what we want to achieve. We need enough bandwidth to make it look so good that our brains allow us to forget it isn't real. And we are gaining this capability now. Fiber and 5G networks are exactly what 360-degree content needs to thrive. Example Consider a single frame of 4K video that contains 3840 x 2160 = 8.3M pixels. A wide-angle shot could contain 90-degree field-of-view (horizontal). This yields 3840 / 90 = ~43 pixels per degree. To produce a 360-degree view (horizontal), we need four similar frames side-by-side. This yields 4 x 3840 = 15360 pixels. A full spherical image is 360 degrees horizontally but only 180 degrees vertically. Thus, we need only half of that number vertically. To experience \"4K quality in 360-degrees\", our image frame would have to be 15360 x 7680 = 118M pixels. This is over 14 times the amount of pixels in a 4K video frame! What if we settle with \"FullHD quality in 360-degrees\" or 1920 / 90 = 21.3 pixels per degree? Then our image frame size becomes 7680 x 3840 = 29.5M pixels, which is \"only\" 3.5 times the amount of pixels in a 4K video frame. 360-degree cameras that capture video in the 7680x3840 resolution are becoming commonplace. This means that we are now entering the \"FullHD\" time of the 360-degree video. \"4K\" time of the 360-degree video is still a few years away. We can also calculate it in another way. High quality compressed 4K video requires roughly 40 Mbit/s bitrate. Scaling up with factor 3.5 yields 140 Mbit/s data rate for \"FullHD quality in 360-degrees\". This could pass through the best current mobile LTE networks that offer a 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for \"4K quality in 360-degrees\". We will need one gigabit Internet connection to view the 360-degree video in a quality that we know as \"4K\". Upcoming mobile 5G networks promise such speeds. In reality, many clever solutions have and will be developed to reduce the need for bandwidth. The capability to transmit high-quality 360-degree video is coming. And sooner than it appears when only bandwidth is considered. What's Next? \u00b6 This is the big picture: we've gone through text, image, sound, and video already, and now 360-degree imaging is here. It is easy and cost-effective enough to be used in daily activities. It is time to take it in use. We can also see the fog disappearing and revealing the next stepping stone. Monoscopic 360-degree video allows the viewer to turn around. The stereoscopic video provides depth cues, but you still cannot move. Volumetric video changes this. It will allow moving your head and even taking a step inside the 360-degree view. This will make us feel about being ever more present. That technology is still in its infancy and needs a few more years to begin to work. And many more before the cameras and networks catch up. There is no reason to wait, though. The future will always bring better, more advanced technology. We can also argue that moving from monoscopic to stereoscopic to volumetric are evolutionary steps, but moving from square video to 360-degree video is a revolution. Improving depth perception is great as the feeling of presence becomes stronger. But widening the field-of-view is one remarkable leap that opens so many possibilities at once. So, let us consider what it means that we can now let go of the limitations of the image frame. After 200 years of photography, that is already a miracle of its own! Making LiveSYNC \u00b6 Now we have covered the big picture and understand the significance of 360-degree imaging. It is time to move on to the LiveSYNC application and its role in this playing field. First, we will take a look at the market situation of 360-degree software in general. Next, a brief summary of the history of the LiveSYNC product follows. We will also discuss use cases that had a great impact on designing the user interface and original features of the app. Market Situation \u00b6 The rise of 360-degree content production did not go unnoticed. In the early days, special software was needed for stitching and editing 360-degree content. And then again for rendering it on your website. Now, traditional video editing tools have been extended to support 360-degree video. You can stitch and edit a 360-degree photo with the de facto standard Photoshop application. The outcome can be embedded in your Wordpress blog. Previously, you needed a custom app if you wanted to play 360-degree video in a phone or a tablet. Today, most social media services support at least basic monoscopic 360-degree content. Some also support viewing via a VR headset. This is great if you shoot and share content for your friends or potential customers. In other words, if it is for fun or part of your mass-marketing strategy. But what about content that is not intended to be available in public? Or, not allowed to be uploaded to 3rd party services, to be shown next to your competitor's ad? How about controlled situations, such as private product presentations or employee training? What about using 360-degree media as part of speeches and lectures? Or, using 360-degree content as a canvas for annotating points-of-interest? And then sharing this information as visual memos or work instructions? Images and videos are very versatile, and 360-degree variants even more so. For industrial users, it is essential that tools can be customized and integrated. Clearly there are many, many needs to satisfy. This means that various kinds of professional tools and services must be developed. Customizable solutions that go beyond what the freemium mass-market services can offer. Moreover, mass-market has started to grow slower than expected. Consumers have plenty of other options for entertainment. It is not easy to convince them to purchase yet another media device. Especially at times when technology develops fast many consumers tend to postpone purchasing: the next year's model is expected to be much better. Also, the language of storytelling must change to accommodate to this new medium. Consumers expect high-quality experiences. But content producers need time to experiment, to figure out what works and what doesn't. Telling stories in 360 degrees is different . Hence, the user experience has been lacking from what consumers are expecting and they are in the waiting mode. The professional market has no reason to wait for the consumer market to mature. The benefits of 360-degree media are available for them now . From consumer point-of-view, it is a question of spending money to yet another entertainment option. From professional point-of-view, it is an investment that can create great cost savings . To name a few examples, the ability to virtually visit a place can reduce travel costs and more efficient training makes new employees productive faster. Postponing spending money can be well justified, but postponing saving money cannot. All in all, there is great need for 360-degree solutions for professionals and motivation to take them in use today . LiveSYNC 1.0 \u00b6 In 2012 a Finnish software company Finwe Ltd. was looking for a new direction. They had years of experience in motion sensors, 3D graphics, and user interfaces. As an experiment, they created a 360-degree photo player. The purpose was to demonstrate a state-of-the-art sensor fusion algorithm on mobile devices. The player was soon extended to support 360-degree video playback. It worked so well that Finwe showcased it in Mobile World Congress in 2013. The reception was good and started to turn Finwe's focus on 360-degree media. It also kickstarted the development of Finwe's own 360-degree rendering engine, Orion360 . A few years later, Finwe was one of the leading companies in 360-degree app development. They had created over 100 custom apps for 360-degree video playback. Besides, their Orion360 engine had been licensed to hundreds of other apps. Yet, the market was in change. The giants of the Internet announced support for 360-degree media one after the other. This meant that the need for custom apps was in decline. These apps were also very similar to each other. Most of the market could be handled with simple template-based variants. There was also a possibility that recent developments in HTML5 would make custom apps obsolete. The 360-degree video was becoming commonplace. It was quickly transforming from a novelty to a utility. It was time for another change in direction. To stay ahead of things, Finwe decided to create a new product that would combine some of their best technologies. The target was a generic player app with an easy-to-use remote control feature. The idea was to create a tool for professionals for presenting content live to different sizes of audiences. In the heart of the app was a technology that allowed mirroring the screen of one device to another. This was done in a way that very little data needed to be transmitted. It allowed controlling and observing many clients in parallel. The solution also worked in crowded places where Wifi was congested. Finwe demoed the concept at Mobile World Congress in February 2017. The feedback was very encouraging. After two months of furious development, Finwe released LiveSYNC 1.0 for iOS and Android in April 2017. Use Cases \u00b6 360-degree content is best experienced using a virtual reality headset. The experience is so immersive that it also isolates the viewer from the rest of the world. It becomes a problem when you are presenting content to another person. How to communicate with him during playback? For example, to give instructions, ask for an opinion, or to share the experience. Not being able to see what he sees forces you to repeatedly ask what's on the screen. How awkward! LiveSYNC removes this barrier by mirroring the VR headset's view to a control device. This view is updated in real-time and can be further shared to a big screen: a TV or a projector, for instance. With LiveSYNC, you can connect multiple headsets to a single control device. Play content on all devices in sync, and observe their views from a gorgeous video mosaic. You can also mix in regular 2D photos and videos, for example, your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool. During the presentation, another communication problem frequently arises. By observing the other guy's view you know what he is currently looking at . But now you have to find the words to explain where he should be looking at , to see something you want him to notice. That becomes equally awkward. With LiveSYNC, you can drag an arrow over the 360-degree view. This icon will appear on all connected devices. You can even move it in real-time by dragging it to another place. This makes communication much easier: look at where the arrow is pointing or it is under the arrow . Simple. Note You may wonder if the other guy's view can be panned from the control device. Wouldn't this make things even easier? Just turn his view to the correct direction, right? Consider a moment that it is you who are wearing a VR headset. Suddenly your whole world begins to spin when another person remotely pans your view. You will likely feel nauseated. Hence, this kind of control is not allowed. It is also good to remember that exploration is what makes 360-degree content interesting. Forcing the point of view in any way should be minimized. It is better to use hints such as arrows, lights, or sounds. The same things that we use in real life to get someone's attention. We don't just go and turn someone else's head! Imagine that you are selling your summer cottage. You've made a set of 360-degree photos and videos. Potential buyers can virtually visit the place without driving hours to the destination. Everybody wins when uninterested buyers can be filtered out early on. But wouldn't it be convenient if you could also add a few notes to your images? To highlight things you want them to notice and remember. Or, automatically stop your 360-degree video tour to certain frames? Saving all annotations would also help. You probably need to show the presentation several times. All these are built-in features in LiveSYNC. Drag & drop icons from clipart sets over the photo or video content. Add a caption for each tag, or create your own signs and use them instead. Save the annotations and they will be loaded the next time you play this content. Powerful annotation features make LiveSYNC stand out. The enterprise version of the LiveSYNC app goes much further. It has been developed in close co-operation with industrial clients. You can use a built-in editor to create or edit a project. Import media content from the camera roll. Add interactive hotspots. These can even fetch data from the network in real-time. Import 2D and 3D maps with camera paths. A really useful feature is to be able to export all the notes you've made as screenshots. They are placed into a visual PDF report, which you can send to your colleagues right away. As an example, take a quick tour on your construction site with a 360-degree camera. Add new work instructions. Export to PDF and mail away. All in 15 minutes. That's what we call instant digitalization . Since its initial release, LiveSYNC has come a long way. We are very excited about the features that are being developed right now. We feel that LiveSYNC is just spreading its wings. After years of working with 360-degree media, we get still excited about it. It is amazing what is possible when an image or a video covers the full 360-degree view. Key Concepts \u00b6 As every software product, also LiveSYNC is based on a few key concepts. For the user of the product, it is very useful to be aware of these: everything feels more logical. Also reading the documentation becomes easier. Some of the terms we use have very specific meanings. Hence, the reader should get familiar with the terms and concepts presented next. Presenting \u00b6 When presenting content to other people, we assume that one person is in control of the situation. Let us call this person the director . The people who are following the presentation are called the audience . The director acts in the role of the presenter. He uses a computing device for sharing content to the eyes of the audience. Let us call this device the control device . The control device is usually a tablet. Sometimes, we may refer to this device as the director's device or the device running in the director mode. In a typical setting for giving a presentation, the room contains a large TV or a projector. For example, most meeting rooms have one. We will call this device the big screen . The connection between the control device and the big screen can be wired or wireless. Using the big screen is optional but often useful: everyone can see the same content. With 360-degree media, it is essential that everyone can explore the view also personally . Look at things they find interesting, and at their own pace. Thus, every person in the audience should have own viewing device . These devices can be phones, tablets, or VR headsets. They can be personal devices or devices that are shared and managed by the presenter. The latter is often more practical. Sometimes, we may refer to these devices as an audience device or a device running in the audience mode. To stay in control, the director's device must communicate with viewing devices. For example, to send a command to switch to the next slide. (This is the reason why it is called a control device). Before communication can begin, a connection must be established between the devices. It is a two-way connection. The control device sends commands to viewing devices. The viewing devices send status messages to the control device. Thus, the director is able to see that everything is OK on each viewing device and assist if necessary. Communication between the control device and the viewing devices is wireless. It is based on common radio technologies. For example, Bluetooth allows controlling a small group of devices without network infrastructure. It works well also in crowded places. A large number of viewing devices can be controlled via an Internet connection. This is based on a cloud service called GlobalSYNC . The service is run by Finwe, the developer of LiveSYNC. It requires either Wifi or mobile data for Internet access. Many presentations can take place simultaneously, even in the same room. This is needed for example in trade shows when many presentations are run at the same or adjacent booths. To make this workable, communication takes place on different LiveSYNC channels . Messages sent on one channel are received by devices on the same channel. To assist a particular audience member who needs help, each device is given an ID. We call it the device's LiveSYNC name . Once connected, this name appears in the control device. It is shown over a mirrored view from that particular device. Thus, on each device, one must configure a few things. Whether it is the director's device (director mode) or an audience device (audience mode). The name used for identification. Which connection method to use (e.g. Bluetooth or GlobalSYNC). Which channel number to join. These are typically pre-configured before a presentation. A new channel can be reserved and taken in use also spontaneously. Created configuration is called channel configuration . A device can have multiple channel configurations, but it can join only one channel at a time. A view to the lobby. Connection info is shown on the white screen. When a configured channel is selected from the app's home screen, a viewing device moves to lobby screen. The user will wait there until a connection to the control device is established. Hence, the lobby is sometimes called the waiting room. The control device will move to the director's workspace . That consists of multiple tabs. For example, the mosaic tab allows observing mirrored views from all connected devices. The player tab is used for controlling the presentation. Changing content, controlling video playback, adding points-of-interest, etc. We will go through it in detail later in this guide. These concepts will become more familiar in the next chapters. You don't need to memorize the terms or their exact meaning to be able to use the app. They are presented only to help you understand how the software works. Also, to make it clear what we mean by these terms. They appear frequently in the documentation. And, to make it easier to understand why certain things are asked during the setup phase. Understanding View Mirroring \u00b6 The ability to mirror a view is very central in LiveSYNC. You might be wondering how does it actually work, and what is possible with LiveSYNC and what is not. Next, we will discuss this further. In general, there are two ways to mirror a live view of one device's screen to another: Streaming video (screencasting) Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection. The mirrored view is an exact copy of what the other device is drawing on its screen. The mirroring feature can be implemented on the platform level. Thus, it is available for all apps or the whole desktop environment. There are some downsides. Streaming high-resolution video consumes a fair amount of CPU/GPU resources. This consumes power and dissipates heat. It also requires a lot of network bandwidth especially when there is movement on the screen. It does not scale well: the receiving end becomes a bottleneck when more devices connect. Furthermore, when using a VR headset, we do not actually want to stream an exact copy of what is on screen. Instead of the distorted double barrel view, we want to mirror a normal view. Streaming commands Principle: Integrate deep into the app and send only commands that allow the receiving end to reconstruct the view from the same assets. The benefits of this method include low CPU/GPU and power consumption. Trivial use of network bandwidth. Scalability. It is also possible to render the mirrored view a bit differently compared to the source device. There are some limitations. All devices must have local copies of the assets required for reconstructing the view. In the case of video, all devices must watch the same content in sync. Else, the control device needs to decode multiple streams. Then hardware resources become the bottleneck. Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. The public hotspots become congested and/or the organizers do not even allow using own Wifi hotspots. LiveSYNC uses the type 1 method for mirroring the view of the control device to the big screen . This works, because there is only one big screen to connect to. We also want to show an exact copy of the screen (or part of the screen). When Wifi doesn't work, HDMI cable can be used. LiveSYNC uses the type 2 method for playing content on the viewing devices and mirroring their views to the control device . This approach allows operation in crowded trade shows. Especially, when Bluetooth technology is used for communication. It also scales well. Observing a large number of viewer devices is possible. Note A common question is whether LiveSYNC can be used for mirroring the views of other applications, such as games. It becomes clear from the explanation above that this is not feasible. Integrating into 3rd party apps is not possible. Thus, LiveSYNC cannot add anything to the platform's own type 1 screen mirroring. To recap, presenting with LiveSYNC requires using the LiveSYNC app on each participating device. Mirroring works only between the LiveSYNC app instances. It is based on more efficient type 2 command streaming. The only exception is the big screen device, which does not need to run the LiveSYNC app. Mirroring from the control device to the big screen is type 1 streaming, typically via an HDMI cable.","title":"1. Introduction"},{"location":"user_guide/introduction/#1-introduction","text":"","title":"1. Introduction"},{"location":"user_guide/introduction/#welcome-to-livesync","text":"Since you are reading this user guide, you must have decided to learn more about 360-degree photos, videos, and a tool called LiveSYNC. That is awesome! We, too, believe it is time to finally get rid of the limitations of the image frame. It is time to start capturing the World as we humans experience it. In a way that a viewer can look around and feel like being there . In other words, in 360-degrees. To make most out of 360-degree content you need proper tools. Not only cameras and software for creating content. It is even more important what comes after that: how you will utilize your content. This means carrying out tasks such as annotating, presenting and sharing. This is also where LiveSYNC as a product comes to picture. You will be surprised how much you can achieve with it. We will start with the big picture to learn why this is the right time to start using 360-degree content. Then, we will discuss how LiveSYNC was born and what needs it was built to satisfy. We will conclude this chapter by going through some key concepts in LiveSYNC. Tip If you can't wait to start using the app, begin reading from Chapter 2. Installing . Continuing from there provides answers to how -questions. Return here when you are ready to find answers to why -questions.","title":"Welcome to LiveSYNC"},{"location":"user_guide/introduction/#the-big-picture","text":"Let's begin by taking a few steps back to see the big picture. We will discuss trends in human communication and photography, and how 360-degree imaging fits in. Next, we go through recent advancements in camera and network technologies, and how they make 360-degree content creation and delivery feasible. We will paint a picture of where we seem to be going, and learn what role 360-degree imaging has in the long run.","title":"The Big Picture"},{"location":"user_guide/introduction/#communication-becomes-visual","text":"During the past few decades, the way humans communicate with each other has changed. First, we learned to connect computers together locally, then globally. We made a giant web of wires where messages can travel from one corner of the planet to another in a fraction of a second. Building Internet was a major breakthrough in communication. In its early days, bandwidth was low and it was practical only for transmitting text . You had to pay for usage by the minute, so it was wise to write an email first and go online only to send it. How odd that must sound to the children of the current age! Along the years, many technological advancements made the web faster and faster. We started to use it for other forms of communication that need higher bandwidth. Images and GIF animations started to appear on web pages. Kids learned to share music as downloadable MP3s, then full-length movies. Low-latency streaming changed the landscape even more. Voice calls via Skype started to become popular. On-demand video rentals began, and Youtube showed up. Eventually, even traditional media gave up. They brought newspapers, radio shows, and live TV online. All forms of digital communication became available for a single low-cost data plan. Fixed monthly price made it possible to stay online all day long. As a consequence, we saw a paradigm shift in human communication. Our messaging began to change from textual to visual representation . Young people didn't bother to write long, well thought out emails. They chose to send lots of short messages that were read and replied almost instantly. Tiny images called smileys were added by combining special characters. They were used to communicate common feelings by mimicking facial expressions. Then more complicated, graphical emojis arrived and teens began to skip words. They communicated with cryptic abbreviations and sequences of emojis. Kind of a modern version of the Egyptian hieroglyphs! Later, textual part in many messages had reduced to a caption for a photograph. Now the image had become the message itself. Then we moved on to video. The age of Youtube stars had begun: anyone could start their own TV show with a press of a button. The world now is more visual that it has ever been in human history. 93% of all human communication is visual. (1) Why are we so fond of photos and videos? What is wrong with plain old books and letters, you may ask. There is room for all kinds of communication methods. The new ones do not replace the old ones; they add to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of the visual kind. We can even argue that humans are visual by nature: Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. (1) To simplify it, the more visual a communication method is the faster we understand the message it carries . Consider an ordinary video. If that small window is so effective, imagine the impact of widening the view to full 360 degrees. Then go further and show it via a virtual reality headset. It is telling that afterward people use words like when I was there , not when I watched that . This is a significant difference. The viewer does not see himself as an outsider, a spectator, but part of the story. Immersion is so powerful that it can even create false memories (2) . Another research shows that people recall information better through virtual reality (3) . That seems logical: our brains are much better in remembering events that happen to us than stories that are told to us . It is clear that 360-degree video is a very effective communication method.","title":"Communication Becomes Visual"},{"location":"user_guide/introduction/#interactivity-arouses-interest","text":"The advances in imaging technology are pushing another paradigm shift. Traditionally, most features of a photograph had to be decided in advance ie. before the camera shutter closed. Now more and more of these can be changed after taking the shot. Careful planning, expensive camera gear, and interest to technical details still have value but are not mandatory anymore. Ordinary people succeed fairly well even in difficult circumstances. And they enjoy the current interactive nature of photography, where many features are not designed in advance but selected based on their effect to the image. As a consequence, photography is more popular than ever before. Since the beginning of digital imaging cameras have developed in fast forward. We have reached a point where we have to ask again: what exactly is a camera? Or, who exactly is the photographer? Consider new phones that contain many physical camera modules. When you press the trigger, a set of images is captured. The camera modules trigger simultaneously and take many shots in sequence. Then, artificial intelligence is used for composing one perfect shot. Some of the frames are retrieved from a memory buffer and were taken before you pressed the trigger. Such a camera is a complex combination of hardware, software, and mathematical algorithms. The software is adopting the role of the photographer. It has become a decision maker in the creation process. So, can we still justify calling the person that pressed the trigger the photographer? All he did was point the camera to a certain direction! It is not surprising that also framing the shot is becoming history. Or, more specifically, we can leave that decision to the viewer of the image. In the 360-degree video, every direction is captured. There are no traces of a camera, a cameraman, or even a tripod. It is a kind of a miracle. Now there is no need to decide even where to point the camera, as everything around the camera can be captured. This has some profound effects, as we will soon see. Let's take a closer look at this technology. Traditionally, a photographer or a videographer has decided how to frame the shot. What is necessary, what can be left out. This decision is locked at the time of capture. It becomes a permanent limitation. At viewing time, it is not possible to see beyond the frame. In fact, the only possibility to re-frame the shot is to limit the viewing angle even more by cropping the image (ie. digital zoom in). The benefit is that the photographer has the power to make his audience focus on exactly what he wants them to see. But that can also become a lie. Beautiful places you see in photos and movies often look very different when you visit them in real life. For example, the pyramids of Giza are always photographed so that they appear to stand in the middle of a desert. If you turn around, there is Cairo with its 20 million inhabitants! Capturing in 360-degrees tends to show a much more realistic view of the World. But how exactly are those images made? Imagine a perfect zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what will happen if you keep zooming out? You will see more and more of the surrounding area become visible in the image. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. A 360-degree camera is essentially a camera with a very wide angle lens. When you are zooming in, there is no natural limit where to stop. But when you are zooming out, such a limit does exist: it is where the opposing sides of the image meet. In that sense, a 360-degree camera has a perfect wide-angle lens: we cannot expand the view any further. We may never be able to manufacture a single lens that could capture everything at once. Yet, 360-degree cameras do exist. How is it possible, then? Again, digital image processing comes to the rescue! Almost seamless representation can be achieved by stitching together many images. These are captured with two or more lenses. Current 360-degree photo and video panoramas capture the world in every direction. In that sense, they are perfect: everything around the camera is in the image. In reality, there is a small distance around the camera where an object can be hidden between the lenses. This has one useful side effect: it allows hiding a small tripod or a selfie stick that is holding the camera. What is the main difference in 360-degree imaging compared to old school photos? During playback, the viewer is in control of the viewing direction . He can turn around and even look up to the sky or down to his feet. It feels like magic when you experience this freedom the very first time. But soon you will get used to it and begin to expect that all images work this way. So much, that you feel annoyed when you attempt to pan an image and it won't budge; it is a plain old 2D image. You've probably seen how today's toddlers expect that every screen is a touch screen. Likewise, you begin to assume that all images can be panned. This interactivity is what makes consuming 360-degree media so interesting to us . The ability to explore.","title":"Interactivity Arouses Interest"},{"location":"user_guide/introduction/#360-cameras-for-all","text":"360-degree photography has come a long way in a short time. Only a few years ago it was unheard of to the general audience. Yet, we can hardly say that 360-degree photographs are anything new . Devoted photographers have been shooting in 360 degrees for quite some time. For a long time, you had to love this form of art to go through all the trouble. Capturing 360-degree photographs required purchasing a lot of camera gear. A DSLR camera, an expensive fisheye lens, and a tripod with a special panoramic head. To create a single image, you had to take multiple shots. Then move the files from your camera to a PC and run them through a special stitching software. You could spend half an hour adding control points, masking the tripod, and many other steps. It took several minutes from a powerful PC to render the image into a file. After months of trial-and-error, you learned to tune your camera gear and shooting process. Quite a learning curve! This is how professionals still work. Of course, everything has become faster, easier, and better looking. Yet, the real revolution was the arrival of the integrated consumer and prosumer level 360-degree cameras. With a few hundred dollars you can now buy a point-and-shoot camera and control it with your phone. The phone also stitches the image and does a pretty good job. The result is not comparable to a professionally made panorama. But, it is quick, cheap, and easy enough for anyone to make. Moreover, it is good enough to be useful . That is the recipe for general adoption. We cannot talk about 360-degree photography without also discussing 360-degree videos. It is, after all, a natural step forward. If you can create a single 360-degree photograph, you can also create a video. Just capture lots of photos in sequence, right? It is not that easy in practice. For video, you need many cameras which need to be synchronized. Because of their physical size, the cameras cannot be in the optimal pivoting point. You must deal with parallax and hence more difficult stitching issues. GoPro action cameras used to be very popular for 360-degree video. They were small, affordable, reliable, and produced good image quality. Popular layouts required using six cameras. Not one of them could fail without ruining the whole shot. Today, integrated 360-degree cameras capture video, too. All you need to do is press the REC button. After transferring the file to your phone, let it stitch the video together. You can even do live streams, straight from your phone to the tubes. 360-degree camera technology definitely has matured. Monoscopic 360-degree video can be considered a solved problem, and it is ready to be used by anyone. Stereoscopic (3D) 360-degree video is only a little behind. Note In the monoscopic 360-degree video there is no depth information. When viewed through a virtual reality headset both eyes see the same image. In the stereoscopic 360-degree video the left and the right eye see a different image. The scene appears to contain depth ie. it is three dimensional. The difference is the same than in movie theaters, where you choose between 2D or 3D screening.","title":"360 Cameras for All"},{"location":"user_guide/introduction/#bandwidth-brings-presence","text":"Being able to look around is not all that it takes to make us feel being there . Not even if the image is stereoscopic. Another key to immersion is the amount of detail . It is another aspect of the amount of information an image represents. A 360-degree image covers the whole field-of-view. But we must also preserve as much detail per degree as possible to make the image look realistic. Or, at least as much as a human eye can distinguish! This is more easily said than done since a human eye is an amazing camera and a lens in a very compact form. It is able to capture an enormous amount of information in a fraction of a second. This brings us back to bandwidth and the Internet. Whenever transfer rates take a step forward, we find a richer communication method. Something that provides a better way to capture and share what is happening around us. Often it is a way that communicates presence better. First, we had to describe everything in text, then we could add images and photos, then sound and music, then video... Can you imagine what will be the next step on this path? One way to understand it all is to see the changes as stepping stones towards telepresence . Fancy word, but it is only a combination of technologies that allow a person to feel as if they were present in another place. And maybe also in another time. That other place can be real or imaginary. This is where we are heading. And step by step, that feeling of being there is becoming stronger and stronger. When we are reading books, we see the story unfold in our own imagination. When we are looking at photos, we see snapshots and imagine the rest. When we are watching movies, we see what someone else has already imagined. When we are wearing a VR headset, we become eyewitnesses to the story. Note Notice that we are not talking about a solution where a human would not be able to recognize if the world is real or virtual. Telepresence is good enough already when we allow ourselves to forget the fact that it isn't real. Tracing the path backward from the goal to the present, we are not there yet but not that far off either. During the past few years, technology has matured enough to solve one profound roadblock. This is the image frame that limits our field of view. Think about it: how could you ever feel being there if you weren't even able to turn your head to look around? Least, you should be able to observe the surroundings by looking in any direction you want. And experience the view and soundscape change corresponding to your head movements. Virtual reality headsets are one way to make this workable. Computer games and movies are an endless source for imaginary worlds. They are the driving force for virtual reality. However, the real world surrounding us must not be forgotten: it is, after all, where we live in! It can be documented with 360-degree photography and 360-degree videography. Accompanied by 360-degree audio, perhaps. Of course, 360-degree cameras and VR headsets are not enough. They are solutions for content creation and consumption. We must also be able to transmit the content from the camera to the headsets. But, moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate. It is not coincident that 360-degree content is becoming popular now . Internet is just becoming fast enough to carry the amount of data 360-degree video requires. Traditional photos are great for saving memories. Videos are perfect for telling stories and documenting events. What is the main reason for shooting something in 360-degrees? It is the feeling of presence . And it is much stronger when the video is combined with 360-degree audio and viewed through a VR headset. The memories are brought alive as space and time where you can step inside. You'll feel like being there . This feeling of (tele)presence is what we want to achieve. We need enough bandwidth to make it look so good that our brains allow us to forget it isn't real. And we are gaining this capability now. Fiber and 5G networks are exactly what 360-degree content needs to thrive. Example Consider a single frame of 4K video that contains 3840 x 2160 = 8.3M pixels. A wide-angle shot could contain 90-degree field-of-view (horizontal). This yields 3840 / 90 = ~43 pixels per degree. To produce a 360-degree view (horizontal), we need four similar frames side-by-side. This yields 4 x 3840 = 15360 pixels. A full spherical image is 360 degrees horizontally but only 180 degrees vertically. Thus, we need only half of that number vertically. To experience \"4K quality in 360-degrees\", our image frame would have to be 15360 x 7680 = 118M pixels. This is over 14 times the amount of pixels in a 4K video frame! What if we settle with \"FullHD quality in 360-degrees\" or 1920 / 90 = 21.3 pixels per degree? Then our image frame size becomes 7680 x 3840 = 29.5M pixels, which is \"only\" 3.5 times the amount of pixels in a 4K video frame. 360-degree cameras that capture video in the 7680x3840 resolution are becoming commonplace. This means that we are now entering the \"FullHD\" time of the 360-degree video. \"4K\" time of the 360-degree video is still a few years away. We can also calculate it in another way. High quality compressed 4K video requires roughly 40 Mbit/s bitrate. Scaling up with factor 3.5 yields 140 Mbit/s data rate for \"FullHD quality in 360-degrees\". This could pass through the best current mobile LTE networks that offer a 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for \"4K quality in 360-degrees\". We will need one gigabit Internet connection to view the 360-degree video in a quality that we know as \"4K\". Upcoming mobile 5G networks promise such speeds. In reality, many clever solutions have and will be developed to reduce the need for bandwidth. The capability to transmit high-quality 360-degree video is coming. And sooner than it appears when only bandwidth is considered.","title":"Bandwidth Brings Presence"},{"location":"user_guide/introduction/#whats-next","text":"This is the big picture: we've gone through text, image, sound, and video already, and now 360-degree imaging is here. It is easy and cost-effective enough to be used in daily activities. It is time to take it in use. We can also see the fog disappearing and revealing the next stepping stone. Monoscopic 360-degree video allows the viewer to turn around. The stereoscopic video provides depth cues, but you still cannot move. Volumetric video changes this. It will allow moving your head and even taking a step inside the 360-degree view. This will make us feel about being ever more present. That technology is still in its infancy and needs a few more years to begin to work. And many more before the cameras and networks catch up. There is no reason to wait, though. The future will always bring better, more advanced technology. We can also argue that moving from monoscopic to stereoscopic to volumetric are evolutionary steps, but moving from square video to 360-degree video is a revolution. Improving depth perception is great as the feeling of presence becomes stronger. But widening the field-of-view is one remarkable leap that opens so many possibilities at once. So, let us consider what it means that we can now let go of the limitations of the image frame. After 200 years of photography, that is already a miracle of its own!","title":"What's Next?"},{"location":"user_guide/introduction/#making-livesync","text":"Now we have covered the big picture and understand the significance of 360-degree imaging. It is time to move on to the LiveSYNC application and its role in this playing field. First, we will take a look at the market situation of 360-degree software in general. Next, a brief summary of the history of the LiveSYNC product follows. We will also discuss use cases that had a great impact on designing the user interface and original features of the app.","title":"Making LiveSYNC"},{"location":"user_guide/introduction/#market-situation","text":"The rise of 360-degree content production did not go unnoticed. In the early days, special software was needed for stitching and editing 360-degree content. And then again for rendering it on your website. Now, traditional video editing tools have been extended to support 360-degree video. You can stitch and edit a 360-degree photo with the de facto standard Photoshop application. The outcome can be embedded in your Wordpress blog. Previously, you needed a custom app if you wanted to play 360-degree video in a phone or a tablet. Today, most social media services support at least basic monoscopic 360-degree content. Some also support viewing via a VR headset. This is great if you shoot and share content for your friends or potential customers. In other words, if it is for fun or part of your mass-marketing strategy. But what about content that is not intended to be available in public? Or, not allowed to be uploaded to 3rd party services, to be shown next to your competitor's ad? How about controlled situations, such as private product presentations or employee training? What about using 360-degree media as part of speeches and lectures? Or, using 360-degree content as a canvas for annotating points-of-interest? And then sharing this information as visual memos or work instructions? Images and videos are very versatile, and 360-degree variants even more so. For industrial users, it is essential that tools can be customized and integrated. Clearly there are many, many needs to satisfy. This means that various kinds of professional tools and services must be developed. Customizable solutions that go beyond what the freemium mass-market services can offer. Moreover, mass-market has started to grow slower than expected. Consumers have plenty of other options for entertainment. It is not easy to convince them to purchase yet another media device. Especially at times when technology develops fast many consumers tend to postpone purchasing: the next year's model is expected to be much better. Also, the language of storytelling must change to accommodate to this new medium. Consumers expect high-quality experiences. But content producers need time to experiment, to figure out what works and what doesn't. Telling stories in 360 degrees is different . Hence, the user experience has been lacking from what consumers are expecting and they are in the waiting mode. The professional market has no reason to wait for the consumer market to mature. The benefits of 360-degree media are available for them now . From consumer point-of-view, it is a question of spending money to yet another entertainment option. From professional point-of-view, it is an investment that can create great cost savings . To name a few examples, the ability to virtually visit a place can reduce travel costs and more efficient training makes new employees productive faster. Postponing spending money can be well justified, but postponing saving money cannot. All in all, there is great need for 360-degree solutions for professionals and motivation to take them in use today .","title":"Market Situation"},{"location":"user_guide/introduction/#livesync-10","text":"In 2012 a Finnish software company Finwe Ltd. was looking for a new direction. They had years of experience in motion sensors, 3D graphics, and user interfaces. As an experiment, they created a 360-degree photo player. The purpose was to demonstrate a state-of-the-art sensor fusion algorithm on mobile devices. The player was soon extended to support 360-degree video playback. It worked so well that Finwe showcased it in Mobile World Congress in 2013. The reception was good and started to turn Finwe's focus on 360-degree media. It also kickstarted the development of Finwe's own 360-degree rendering engine, Orion360 . A few years later, Finwe was one of the leading companies in 360-degree app development. They had created over 100 custom apps for 360-degree video playback. Besides, their Orion360 engine had been licensed to hundreds of other apps. Yet, the market was in change. The giants of the Internet announced support for 360-degree media one after the other. This meant that the need for custom apps was in decline. These apps were also very similar to each other. Most of the market could be handled with simple template-based variants. There was also a possibility that recent developments in HTML5 would make custom apps obsolete. The 360-degree video was becoming commonplace. It was quickly transforming from a novelty to a utility. It was time for another change in direction. To stay ahead of things, Finwe decided to create a new product that would combine some of their best technologies. The target was a generic player app with an easy-to-use remote control feature. The idea was to create a tool for professionals for presenting content live to different sizes of audiences. In the heart of the app was a technology that allowed mirroring the screen of one device to another. This was done in a way that very little data needed to be transmitted. It allowed controlling and observing many clients in parallel. The solution also worked in crowded places where Wifi was congested. Finwe demoed the concept at Mobile World Congress in February 2017. The feedback was very encouraging. After two months of furious development, Finwe released LiveSYNC 1.0 for iOS and Android in April 2017.","title":"LiveSYNC 1.0"},{"location":"user_guide/introduction/#use-cases","text":"360-degree content is best experienced using a virtual reality headset. The experience is so immersive that it also isolates the viewer from the rest of the world. It becomes a problem when you are presenting content to another person. How to communicate with him during playback? For example, to give instructions, ask for an opinion, or to share the experience. Not being able to see what he sees forces you to repeatedly ask what's on the screen. How awkward! LiveSYNC removes this barrier by mirroring the VR headset's view to a control device. This view is updated in real-time and can be further shared to a big screen: a TV or a projector, for instance. With LiveSYNC, you can connect multiple headsets to a single control device. Play content on all devices in sync, and observe their views from a gorgeous video mosaic. You can also mix in regular 2D photos and videos, for example, your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool. During the presentation, another communication problem frequently arises. By observing the other guy's view you know what he is currently looking at . But now you have to find the words to explain where he should be looking at , to see something you want him to notice. That becomes equally awkward. With LiveSYNC, you can drag an arrow over the 360-degree view. This icon will appear on all connected devices. You can even move it in real-time by dragging it to another place. This makes communication much easier: look at where the arrow is pointing or it is under the arrow . Simple. Note You may wonder if the other guy's view can be panned from the control device. Wouldn't this make things even easier? Just turn his view to the correct direction, right? Consider a moment that it is you who are wearing a VR headset. Suddenly your whole world begins to spin when another person remotely pans your view. You will likely feel nauseated. Hence, this kind of control is not allowed. It is also good to remember that exploration is what makes 360-degree content interesting. Forcing the point of view in any way should be minimized. It is better to use hints such as arrows, lights, or sounds. The same things that we use in real life to get someone's attention. We don't just go and turn someone else's head! Imagine that you are selling your summer cottage. You've made a set of 360-degree photos and videos. Potential buyers can virtually visit the place without driving hours to the destination. Everybody wins when uninterested buyers can be filtered out early on. But wouldn't it be convenient if you could also add a few notes to your images? To highlight things you want them to notice and remember. Or, automatically stop your 360-degree video tour to certain frames? Saving all annotations would also help. You probably need to show the presentation several times. All these are built-in features in LiveSYNC. Drag & drop icons from clipart sets over the photo or video content. Add a caption for each tag, or create your own signs and use them instead. Save the annotations and they will be loaded the next time you play this content. Powerful annotation features make LiveSYNC stand out. The enterprise version of the LiveSYNC app goes much further. It has been developed in close co-operation with industrial clients. You can use a built-in editor to create or edit a project. Import media content from the camera roll. Add interactive hotspots. These can even fetch data from the network in real-time. Import 2D and 3D maps with camera paths. A really useful feature is to be able to export all the notes you've made as screenshots. They are placed into a visual PDF report, which you can send to your colleagues right away. As an example, take a quick tour on your construction site with a 360-degree camera. Add new work instructions. Export to PDF and mail away. All in 15 minutes. That's what we call instant digitalization . Since its initial release, LiveSYNC has come a long way. We are very excited about the features that are being developed right now. We feel that LiveSYNC is just spreading its wings. After years of working with 360-degree media, we get still excited about it. It is amazing what is possible when an image or a video covers the full 360-degree view.","title":"Use Cases"},{"location":"user_guide/introduction/#key-concepts","text":"As every software product, also LiveSYNC is based on a few key concepts. For the user of the product, it is very useful to be aware of these: everything feels more logical. Also reading the documentation becomes easier. Some of the terms we use have very specific meanings. Hence, the reader should get familiar with the terms and concepts presented next.","title":"Key Concepts"},{"location":"user_guide/introduction/#presenting","text":"When presenting content to other people, we assume that one person is in control of the situation. Let us call this person the director . The people who are following the presentation are called the audience . The director acts in the role of the presenter. He uses a computing device for sharing content to the eyes of the audience. Let us call this device the control device . The control device is usually a tablet. Sometimes, we may refer to this device as the director's device or the device running in the director mode. In a typical setting for giving a presentation, the room contains a large TV or a projector. For example, most meeting rooms have one. We will call this device the big screen . The connection between the control device and the big screen can be wired or wireless. Using the big screen is optional but often useful: everyone can see the same content. With 360-degree media, it is essential that everyone can explore the view also personally . Look at things they find interesting, and at their own pace. Thus, every person in the audience should have own viewing device . These devices can be phones, tablets, or VR headsets. They can be personal devices or devices that are shared and managed by the presenter. The latter is often more practical. Sometimes, we may refer to these devices as an audience device or a device running in the audience mode. To stay in control, the director's device must communicate with viewing devices. For example, to send a command to switch to the next slide. (This is the reason why it is called a control device). Before communication can begin, a connection must be established between the devices. It is a two-way connection. The control device sends commands to viewing devices. The viewing devices send status messages to the control device. Thus, the director is able to see that everything is OK on each viewing device and assist if necessary. Communication between the control device and the viewing devices is wireless. It is based on common radio technologies. For example, Bluetooth allows controlling a small group of devices without network infrastructure. It works well also in crowded places. A large number of viewing devices can be controlled via an Internet connection. This is based on a cloud service called GlobalSYNC . The service is run by Finwe, the developer of LiveSYNC. It requires either Wifi or mobile data for Internet access. Many presentations can take place simultaneously, even in the same room. This is needed for example in trade shows when many presentations are run at the same or adjacent booths. To make this workable, communication takes place on different LiveSYNC channels . Messages sent on one channel are received by devices on the same channel. To assist a particular audience member who needs help, each device is given an ID. We call it the device's LiveSYNC name . Once connected, this name appears in the control device. It is shown over a mirrored view from that particular device. Thus, on each device, one must configure a few things. Whether it is the director's device (director mode) or an audience device (audience mode). The name used for identification. Which connection method to use (e.g. Bluetooth or GlobalSYNC). Which channel number to join. These are typically pre-configured before a presentation. A new channel can be reserved and taken in use also spontaneously. Created configuration is called channel configuration . A device can have multiple channel configurations, but it can join only one channel at a time. A view to the lobby. Connection info is shown on the white screen. When a configured channel is selected from the app's home screen, a viewing device moves to lobby screen. The user will wait there until a connection to the control device is established. Hence, the lobby is sometimes called the waiting room. The control device will move to the director's workspace . That consists of multiple tabs. For example, the mosaic tab allows observing mirrored views from all connected devices. The player tab is used for controlling the presentation. Changing content, controlling video playback, adding points-of-interest, etc. We will go through it in detail later in this guide. These concepts will become more familiar in the next chapters. You don't need to memorize the terms or their exact meaning to be able to use the app. They are presented only to help you understand how the software works. Also, to make it clear what we mean by these terms. They appear frequently in the documentation. And, to make it easier to understand why certain things are asked during the setup phase.","title":"Presenting"},{"location":"user_guide/introduction/#understanding-view-mirroring","text":"The ability to mirror a view is very central in LiveSYNC. You might be wondering how does it actually work, and what is possible with LiveSYNC and what is not. Next, we will discuss this further. In general, there are two ways to mirror a live view of one device's screen to another: Streaming video (screencasting) Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection. The mirrored view is an exact copy of what the other device is drawing on its screen. The mirroring feature can be implemented on the platform level. Thus, it is available for all apps or the whole desktop environment. There are some downsides. Streaming high-resolution video consumes a fair amount of CPU/GPU resources. This consumes power and dissipates heat. It also requires a lot of network bandwidth especially when there is movement on the screen. It does not scale well: the receiving end becomes a bottleneck when more devices connect. Furthermore, when using a VR headset, we do not actually want to stream an exact copy of what is on screen. Instead of the distorted double barrel view, we want to mirror a normal view. Streaming commands Principle: Integrate deep into the app and send only commands that allow the receiving end to reconstruct the view from the same assets. The benefits of this method include low CPU/GPU and power consumption. Trivial use of network bandwidth. Scalability. It is also possible to render the mirrored view a bit differently compared to the source device. There are some limitations. All devices must have local copies of the assets required for reconstructing the view. In the case of video, all devices must watch the same content in sync. Else, the control device needs to decode multiple streams. Then hardware resources become the bottleneck. Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. The public hotspots become congested and/or the organizers do not even allow using own Wifi hotspots. LiveSYNC uses the type 1 method for mirroring the view of the control device to the big screen . This works, because there is only one big screen to connect to. We also want to show an exact copy of the screen (or part of the screen). When Wifi doesn't work, HDMI cable can be used. LiveSYNC uses the type 2 method for playing content on the viewing devices and mirroring their views to the control device . This approach allows operation in crowded trade shows. Especially, when Bluetooth technology is used for communication. It also scales well. Observing a large number of viewer devices is possible. Note A common question is whether LiveSYNC can be used for mirroring the views of other applications, such as games. It becomes clear from the explanation above that this is not feasible. Integrating into 3rd party apps is not possible. Thus, LiveSYNC cannot add anything to the platform's own type 1 screen mirroring. To recap, presenting with LiveSYNC requires using the LiveSYNC app on each participating device. Mirroring works only between the LiveSYNC app instances. It is based on more efficient type 2 command streaming. The only exception is the big screen device, which does not need to run the LiveSYNC app. Mirroring from the control device to the big screen is type 1 streaming, typically via an HDMI cable.","title":"Understanding View Mirroring"},{"location":"user_guide/photo_content/","text":"File Naming Conventions \u00b6 360-degree Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 180-degree Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 2D Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 In LiveSYNC the player will automatically downscale your own 360 images in the /Movies/LiveSYNC folder to the maximum texture size allowed by the playback device, and currently you need to copy them to /Movies/LiveSYNC , not /Pictures/LiveSYNC . LiveSYNC supports both Over-Under and Side-by-Side formats. The only thing you need to to is to add the following prefix to the video/image file: _3DOU.mp4 OR _3DOU.jpg _3SBS.mp4 OR _3SBS.jpg _3DTB.mp4 OR _3DTB.jpg _3DBT.mp4 OR _3DBT.jpg So for example if your file name is myvideo.mp4, you just need to modify the filename to myvideo_3DOU.mp4 or in case of images myimage_3DOU.jpg. Tip Where to add own contents (photo, video, hotspot icons)? On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start) Note Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images","title":"7. Photo Content"},{"location":"user_guide/photo_content/#file-naming-conventions","text":"","title":"File Naming Conventions"},{"location":"user_guide/photo_content/#360-degree-content","text":"","title":"360-degree Content"},{"location":"user_guide/photo_content/#monoscopic","text":"","title":"Monoscopic"},{"location":"user_guide/photo_content/#stereoscopic","text":"","title":"Stereoscopic"},{"location":"user_guide/photo_content/#180-degree-content","text":"","title":"180-degree Content"},{"location":"user_guide/photo_content/#monoscopic_1","text":"","title":"Monoscopic"},{"location":"user_guide/photo_content/#stereoscopic_1","text":"","title":"Stereoscopic"},{"location":"user_guide/photo_content/#2d-content","text":"","title":"2D Content"},{"location":"user_guide/photo_content/#monoscopic_2","text":"","title":"Monoscopic"},{"location":"user_guide/photo_content/#stereoscopic_2","text":"In LiveSYNC the player will automatically downscale your own 360 images in the /Movies/LiveSYNC folder to the maximum texture size allowed by the playback device, and currently you need to copy them to /Movies/LiveSYNC , not /Pictures/LiveSYNC . LiveSYNC supports both Over-Under and Side-by-Side formats. The only thing you need to to is to add the following prefix to the video/image file: _3DOU.mp4 OR _3DOU.jpg _3SBS.mp4 OR _3SBS.jpg _3DTB.mp4 OR _3DTB.jpg _3DBT.mp4 OR _3DBT.jpg So for example if your file name is myvideo.mp4, you just need to modify the filename to myvideo_3DOU.mp4 or in case of images myimage_3DOU.jpg. Tip Where to add own contents (photo, video, hotspot icons)? On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start) Note Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images","title":"Stereoscopic"},{"location":"user_guide/preface/","text":"Preface \u00b6 Abstract \u00b6 The LiveSYNC User Guide is the official manual for the LiveSYNC software application . It explains what you can do with the LiveSYNC application and how to work with it in different situations. The first few chapters focus on presenting monoscopic 360-degree photos and videos to an audience. The rest of the guide is devoted to more advanced use cases and other content types. You will learn that LiveSYNC is a very versatile tool. Version \u00b6 The user guide does not describe a particular release or distribution of the LiveSYNC app but rather pertains to all recent versions of it. It is frequently updated when new features are released and different use cases covered. Overlap \u00b6 The user guide is hosted on a website called LiveSYNC Learning Center . The website contains also other forms of product documentation, such as quick start guides, tutorials, and articles. You will notice some overlap between the user guide and other documentation. The user guide is the most comprehensive. Audience \u00b6 The user guide is designed for a wide range of readers. It does not require extensive experience in content production, although some experience using a 360-degree camera is helpful. It is appropriate for the following readers: Students occasionally using 360-degree content in their course work Basic users who simply need to present content in trade shows etc. Power users and executives who frequently give speeches or sales pitches 360-degree video professionals who need to create standalone setups Industrial users who focus on annotating and reporting All users of the 360 Video Starter Kit product Features \u00b6 This user guide is organized for ease of use in different situations. For example, you can read it from cover to cover to learn the LiveSYNC app from the ground up. Alternatively, once you are comfortable using the app, you can use this guide as a reference: Look up a topic of interest in the table of contents or index and read about it. Or, use the search feature on this site to find sections that mention a specific keyword. Supplements \u00b6 The Downloads section on this site contains downloadable resources. You will find various example images and videos that you can use while practicing. There are also tag icon libraries that you can freely use in your presentations, example configuration files for customizing the user interface, etc. Thanks \u00b6 This user guide and the LiveSYNC app are much better products because of our beta testers . A big \"Thank You\" to all of you! Your feedback and suggestions are invaluable to us. Authors \u00b6 This document is written and maintained by Tapani Rantakokko , CTO of Finwe Tewodros Mengesha , LiveSYNC support specialist at Finwe Feedback \u00b6 If you find typos, factual errors, deprecated instructions, or wish to suggest additions to the manual, please contact us via the Support page.","title":"Preface"},{"location":"user_guide/preface/#preface","text":"","title":"Preface"},{"location":"user_guide/preface/#abstract","text":"The LiveSYNC User Guide is the official manual for the LiveSYNC software application . It explains what you can do with the LiveSYNC application and how to work with it in different situations. The first few chapters focus on presenting monoscopic 360-degree photos and videos to an audience. The rest of the guide is devoted to more advanced use cases and other content types. You will learn that LiveSYNC is a very versatile tool.","title":"Abstract"},{"location":"user_guide/preface/#version","text":"The user guide does not describe a particular release or distribution of the LiveSYNC app but rather pertains to all recent versions of it. It is frequently updated when new features are released and different use cases covered.","title":"Version"},{"location":"user_guide/preface/#overlap","text":"The user guide is hosted on a website called LiveSYNC Learning Center . The website contains also other forms of product documentation, such as quick start guides, tutorials, and articles. You will notice some overlap between the user guide and other documentation. The user guide is the most comprehensive.","title":"Overlap"},{"location":"user_guide/preface/#audience","text":"The user guide is designed for a wide range of readers. It does not require extensive experience in content production, although some experience using a 360-degree camera is helpful. It is appropriate for the following readers: Students occasionally using 360-degree content in their course work Basic users who simply need to present content in trade shows etc. Power users and executives who frequently give speeches or sales pitches 360-degree video professionals who need to create standalone setups Industrial users who focus on annotating and reporting All users of the 360 Video Starter Kit product","title":"Audience"},{"location":"user_guide/preface/#features","text":"This user guide is organized for ease of use in different situations. For example, you can read it from cover to cover to learn the LiveSYNC app from the ground up. Alternatively, once you are comfortable using the app, you can use this guide as a reference: Look up a topic of interest in the table of contents or index and read about it. Or, use the search feature on this site to find sections that mention a specific keyword.","title":"Features"},{"location":"user_guide/preface/#supplements","text":"The Downloads section on this site contains downloadable resources. You will find various example images and videos that you can use while practicing. There are also tag icon libraries that you can freely use in your presentations, example configuration files for customizing the user interface, etc.","title":"Supplements"},{"location":"user_guide/preface/#thanks","text":"This user guide and the LiveSYNC app are much better products because of our beta testers . A big \"Thank You\" to all of you! Your feedback and suggestions are invaluable to us.","title":"Thanks"},{"location":"user_guide/preface/#authors","text":"This document is written and maintained by Tapani Rantakokko , CTO of Finwe Tewodros Mengesha , LiveSYNC support specialist at Finwe","title":"Authors"},{"location":"user_guide/preface/#feedback","text":"If you find typos, factual errors, deprecated instructions, or wish to suggest additions to the manual, please contact us via the Support page.","title":"Feedback"},{"location":"user_guide/presenting/","text":"LiveSYNC presentation solution \u00b6 LiveSYNC is a technology that allows controlling and observing 360 VR video playback on a large number of local and remote devices from a single master device. LiveSYNC also supports mirroring the view of a local device to a big screen via the master device. LiveSYNC is easy to use, lightweight and works well also in crowded places. LiveSYNC's director view. Make sure you copy presentation contents to both director and audience devices. To start a presentation drag a photo/video content from video menu to director view and all audience devices will have the same presentation content as director's view. Tip Connected audience devices will be seen on the bottom of director device.","title":"6. Presenting"},{"location":"user_guide/presenting/#livesync-presentation-solution","text":"LiveSYNC is a technology that allows controlling and observing 360 VR video playback on a large number of local and remote devices from a single master device. LiveSYNC also supports mirroring the view of a local device to a big screen via the master device. LiveSYNC is easy to use, lightweight and works well also in crowded places. LiveSYNC's director view. Make sure you copy presentation contents to both director and audience devices. To start a presentation drag a photo/video content from video menu to director view and all audience devices will have the same presentation content as director's view. Tip Connected audience devices will be seen on the bottom of director device.","title":"LiveSYNC presentation solution"},{"location":"user_guide/streaming/","text":"Streaming \u00b6 Does LiveSYNC support live streaming? \u00b6 At the moment all videos and contents must be copied to the director and all audience devices before you start presenting contents. In LiveSYNC it is possible to stream contents if you provide URLs in JSON format. The JSON includes only three fields: name , video url and an optional field layout which tells video projection. The layout can be any of the following types: \"2D\" , \"360\" , 360TB\" , \"360SBS\" . { \"name\":\"Yle\", \"url\":\"http://yletv-lh.akamaihd.net/i/yletv1hls_1@103188/master.m3u8\", \"layout\":\"2D\" } Save the JSON file with the .videostream extension, for example, myvideo.videostream. This works the way that LiveSYNC doesn't download the file to the memory, instead it plays it live from the Internet. LiveSYNC works for live broadcasts, if you just copy a Vimeo or YouTube stream link to the .videostream file, then it might only work for a short period of time until the service generates new links. If so, make sure you update the link to the .videostream file with newly generated link/s. With Vimeo Pro, this is not a problem, because links are made to stay as they are just for cases like this.","title":"10. Streaming"},{"location":"user_guide/streaming/#streaming","text":"","title":"Streaming"},{"location":"user_guide/streaming/#does-livesync-support-live-streaming","text":"At the moment all videos and contents must be copied to the director and all audience devices before you start presenting contents. In LiveSYNC it is possible to stream contents if you provide URLs in JSON format. The JSON includes only three fields: name , video url and an optional field layout which tells video projection. The layout can be any of the following types: \"2D\" , \"360\" , 360TB\" , \"360SBS\" . { \"name\":\"Yle\", \"url\":\"http://yletv-lh.akamaihd.net/i/yletv1hls_1@103188/master.m3u8\", \"layout\":\"2D\" } Save the JSON file with the .videostream extension, for example, myvideo.videostream. This works the way that LiveSYNC doesn't download the file to the memory, instead it plays it live from the Internet. LiveSYNC works for live broadcasts, if you just copy a Vimeo or YouTube stream link to the .videostream file, then it might only work for a short period of time until the service generates new links. If so, make sure you update the link to the .videostream file with newly generated link/s. With Vimeo Pro, this is not a problem, because links are made to stay as they are just for cases like this.","title":"Does LiveSYNC support live streaming?"},{"location":"user_guide/troubleshooting/","text":"A quick checklist \u00b6 Install LiveSYNC app for your director device (iPad) and all client devices Ensure that Bluetooth is enabled on all devices On your iPad, set up a new presentation channel as Director and take note the 4-digit channel number that is given to you. On your client devices, set up the presentation channel as Audience and make sure to use the 4-digit channel number you got in previous step. On your iPad, select your presentation channel (\"Control\") to go to Director mode, you'll enter the Mosaic view first. On your client devices, select the presentation channel (\"Join) to go to Audience mode, you'll enter the waiting room first. Shortly, the devices should connect automatically ie. the clients should appear on Director's Mosaic view, and clients should notify that presentation begins soon. Once connected, on your iPad move to Player view and drag one of the default contents to Presentation view (PowerPark or Olos), it should play on all connected devices. Note Notice that when you use your own content, you need to copy it to Director devices AND all Audience devices before you can present them.. In case you have connection issues try these quick fixes in order: \u00b6 Double check that Bluetooth is enabled and you are not in Airplane mode. Turn Bluetooth off and back on shortly after, especially on the iPad. Check that your device supports Bluetooth 4.0 (should not be possible to install from store if it doesn't). If your own content files do not work: \u00b6 Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to device's maximum size. On iOS devices, make sure to use stereo audio - not multi-channel If your audience devices do not connect. Make sure Bluetooth is enabled on all devices. In temporary connection problems, first disable and re-enable bluetooth on the director device, then (if necessary) on the client device.","title":"14. Troubleshooting"},{"location":"user_guide/troubleshooting/#a-quick-checklist","text":"Install LiveSYNC app for your director device (iPad) and all client devices Ensure that Bluetooth is enabled on all devices On your iPad, set up a new presentation channel as Director and take note the 4-digit channel number that is given to you. On your client devices, set up the presentation channel as Audience and make sure to use the 4-digit channel number you got in previous step. On your iPad, select your presentation channel (\"Control\") to go to Director mode, you'll enter the Mosaic view first. On your client devices, select the presentation channel (\"Join) to go to Audience mode, you'll enter the waiting room first. Shortly, the devices should connect automatically ie. the clients should appear on Director's Mosaic view, and clients should notify that presentation begins soon. Once connected, on your iPad move to Player view and drag one of the default contents to Presentation view (PowerPark or Olos), it should play on all connected devices. Note Notice that when you use your own content, you need to copy it to Director devices AND all Audience devices before you can present them..","title":"A quick checklist"},{"location":"user_guide/troubleshooting/#in-case-you-have-connection-issues-try-these-quick-fixes-in-order","text":"Double check that Bluetooth is enabled and you are not in Airplane mode. Turn Bluetooth off and back on shortly after, especially on the iPad. Check that your device supports Bluetooth 4.0 (should not be possible to install from store if it doesn't).","title":"In case you have connection issues try these quick fixes in order:"},{"location":"user_guide/troubleshooting/#if-your-own-content-files-do-not-work","text":"Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to device's maximum size. On iOS devices, make sure to use stereo audio - not multi-channel If your audience devices do not connect. Make sure Bluetooth is enabled on all devices. In temporary connection problems, first disable and re-enable bluetooth on the director device, then (if necessary) on the client device.","title":"If your own content files do not work:"},{"location":"user_guide/user_guide/","text":"User Guide \u00b6 Welcome to the LiveSYNC User Guide ! Choose a topic from below to find answers, get step-by-step instructions, and develop your skills. Warning The User Guide is currently under construction. Preface \u00b6 Abstract Version Overlap Audience Features Supplements Thanks Authors Feedback 1. Introduction \u00b6 Welcome to LiveSYNC The Big Picture Communication Becomes Visual Interactivity Arouses Interest 360 Cameras for All Bandwidth Brings Presenece What's Next Making LiveSYNC Market Situation LiveSYNC 1.0 Use Cases Key Concepts Presenting Understanding View Mirroring 2. Installing \u00b6 Versions Platforms Store vs. Beta Roles Version Number System Requirements iOS Android Oculus Installing the Store Version iOS Android Oculus Installing the Beta Version iOS Android Oculus License Trial Period Licensing Model Watermarks Buying License Code Store vs. Beta Locked Features Enterprise Features 360 Video Starter Kit 3. Configuration \u00b6 Overview Director Role iOS & Android Oculus Audience Role iOS & Android Oculus Configuration Tips Device Role Identification Connection Type Channel Number View Mode 4. Managing Assets \u00b6 5. Workspace \u00b6 6. Presenting \u00b6 7. Photo Content \u00b6 8. Video Content \u00b6 9. Tags & Hotspots \u00b6 10. Streaming \u00b6 11. Editor \u00b6 12. Workflows \u00b6 13. Customizing \u00b6 14. Troubleshooting \u00b6","title":"Index"},{"location":"user_guide/user_guide/#user-guide","text":"Welcome to the LiveSYNC User Guide ! Choose a topic from below to find answers, get step-by-step instructions, and develop your skills. Warning The User Guide is currently under construction.","title":"User Guide"},{"location":"user_guide/user_guide/#preface","text":"Abstract Version Overlap Audience Features Supplements Thanks Authors Feedback","title":"Preface"},{"location":"user_guide/user_guide/#1-introduction","text":"Welcome to LiveSYNC The Big Picture Communication Becomes Visual Interactivity Arouses Interest 360 Cameras for All Bandwidth Brings Presenece What's Next Making LiveSYNC Market Situation LiveSYNC 1.0 Use Cases Key Concepts Presenting Understanding View Mirroring","title":"1. Introduction"},{"location":"user_guide/user_guide/#2-installing","text":"Versions Platforms Store vs. Beta Roles Version Number System Requirements iOS Android Oculus Installing the Store Version iOS Android Oculus Installing the Beta Version iOS Android Oculus License Trial Period Licensing Model Watermarks Buying License Code Store vs. Beta Locked Features Enterprise Features 360 Video Starter Kit","title":"2. Installing"},{"location":"user_guide/user_guide/#3-configuration","text":"Overview Director Role iOS & Android Oculus Audience Role iOS & Android Oculus Configuration Tips Device Role Identification Connection Type Channel Number View Mode","title":"3. Configuration"},{"location":"user_guide/user_guide/#4-managing-assets","text":"","title":"4. Managing Assets"},{"location":"user_guide/user_guide/#5-workspace","text":"","title":"5. Workspace"},{"location":"user_guide/user_guide/#6-presenting","text":"","title":"6. Presenting"},{"location":"user_guide/user_guide/#7-photo-content","text":"","title":"7. Photo Content"},{"location":"user_guide/user_guide/#8-video-content","text":"","title":"8. Video Content"},{"location":"user_guide/user_guide/#9-tags-hotspots","text":"","title":"9. Tags &amp; Hotspots"},{"location":"user_guide/user_guide/#10-streaming","text":"","title":"10. Streaming"},{"location":"user_guide/user_guide/#11-editor","text":"","title":"11. Editor"},{"location":"user_guide/user_guide/#12-workflows","text":"","title":"12. Workflows"},{"location":"user_guide/user_guide/#13-customizing","text":"","title":"13. Customizing"},{"location":"user_guide/user_guide/#14-troubleshooting","text":"","title":"14. Troubleshooting"},{"location":"user_guide/video_content/","text":"File Naming Conventions \u00b6 360-degree Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 180-degree Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 2D Content \u00b6 Monoscopic \u00b6 Stereoscopic \u00b6 LiveSYNC uses media player on the back-end and that limits the resolution when it comes to playable video files. Any video that plays with the device's own video player should also work with LiveSYNC. Choosing the right resolution and encoding is the key to an excellent experience. We advise using .mp4 format video files with resolution 1920x960 or 3840x1920 since these are best supported across devices. However, only 4k capable devices will play 3840x1920. We recommend one can get better balance by targeting a bitrate somewhere between 30-60 Mbps. If you have a mix of older and newer devices, it is much safer to use 3840x1920 for 4k capable devices and for old devices 1920x960 by using the same file name for the same video content with different resolutions. We have built support for Facebook 360 audio (.tbe files). This has not yet been released for all platforms but it is available on request for example for GearVR. The maximum resolution depends on each device's hardware capabilities. Notice that offline video files need to be copied for each device during the setup phase, and this opens a possibility to use a different resolution (but same filename) variants on different devices, in order to get the best possible quality on each platform. Does LiveSYNC support stereoscopic content? \u00b6 We have stereoscopic 360 content support already built for equirectangular panoramas (the usual equirectangular configurations over-under and side-by-side), but it hasn't been released yet officially. This will be included in the next release that is coming within a few weeks of time. However, if you are in a hurry we may be able to provide a custom build earlier. Does LiveSYNC support cubemap projection? \u00b6 Currently, no. We have supported cubemap projection in some apps earlier and if there is demand for it, we will definitely add it to LiveSYNC as well. But with the current version, you'll have to export the images in an equirectangular format or convert your existing cubemaps to equirectangular. Note If your own content files do not work: Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to the device's maximum size.","title":"8. Video Content"},{"location":"user_guide/video_content/#file-naming-conventions","text":"","title":"File Naming Conventions"},{"location":"user_guide/video_content/#360-degree-content","text":"","title":"360-degree Content"},{"location":"user_guide/video_content/#monoscopic","text":"","title":"Monoscopic"},{"location":"user_guide/video_content/#stereoscopic","text":"","title":"Stereoscopic"},{"location":"user_guide/video_content/#180-degree-content","text":"","title":"180-degree Content"},{"location":"user_guide/video_content/#monoscopic_1","text":"","title":"Monoscopic"},{"location":"user_guide/video_content/#stereoscopic_1","text":"","title":"Stereoscopic"},{"location":"user_guide/video_content/#2d-content","text":"","title":"2D Content"},{"location":"user_guide/video_content/#monoscopic_2","text":"","title":"Monoscopic"},{"location":"user_guide/video_content/#stereoscopic_2","text":"LiveSYNC uses media player on the back-end and that limits the resolution when it comes to playable video files. Any video that plays with the device's own video player should also work with LiveSYNC. Choosing the right resolution and encoding is the key to an excellent experience. We advise using .mp4 format video files with resolution 1920x960 or 3840x1920 since these are best supported across devices. However, only 4k capable devices will play 3840x1920. We recommend one can get better balance by targeting a bitrate somewhere between 30-60 Mbps. If you have a mix of older and newer devices, it is much safer to use 3840x1920 for 4k capable devices and for old devices 1920x960 by using the same file name for the same video content with different resolutions. We have built support for Facebook 360 audio (.tbe files). This has not yet been released for all platforms but it is available on request for example for GearVR. The maximum resolution depends on each device's hardware capabilities. Notice that offline video files need to be copied for each device during the setup phase, and this opens a possibility to use a different resolution (but same filename) variants on different devices, in order to get the best possible quality on each platform.","title":"Stereoscopic"},{"location":"user_guide/video_content/#does-livesync-support-stereoscopic-content","text":"We have stereoscopic 360 content support already built for equirectangular panoramas (the usual equirectangular configurations over-under and side-by-side), but it hasn't been released yet officially. This will be included in the next release that is coming within a few weeks of time. However, if you are in a hurry we may be able to provide a custom build earlier.","title":"Does LiveSYNC support stereoscopic content?"},{"location":"user_guide/video_content/#does-livesync-support-cubemap-projection","text":"Currently, no. We have supported cubemap projection in some apps earlier and if there is demand for it, we will definitely add it to LiveSYNC as well. But with the current version, you'll have to export the images in an equirectangular format or convert your existing cubemaps to equirectangular. Note If your own content files do not work: Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to the device's maximum size.","title":"Does LiveSYNC support cubemap projection?"},{"location":"user_guide/workflows/","text":"Camera Path Normalization \u00b6 Normalization converts a camera path's coordinates to another coordinate space with given reference points. How to use path normalization? \u00b6 Use a cam path that's not already normalized to same real-world coordinates. In editor open new or saved the project. You need to place three calibration point type hotspots and set real-world positions in properties. They also need to have calculated world positions by using auto tracking. \"Normalize\" button appears, click that and confirm. After normalization in 3D map make sure that the path looks good and calibration point spheres are aligned with the three cross-shaped objects that are real-world reference positions. Saving the project saves the hotspots and creates a copy of this normalized path in the same folder with \"_normalized\" postfix e.g. someCamPath.mp4_normalized.txt Optionally you can now delete calibration points if you wish. After normalization, you can denormalize using the button. For now, all it does it delete the normalized cam path and forces to reload project with nonnormalized cam path. That means if your hotspots are placed in normalized cam path then they won't appear in the correct positions in nonnormalized cam path. In normalization, all hotspot positions are converted to new normalized coordinate but same for denormalization has not yet been implemented. Watch a tutorial video on camera path normalization Video not working? Click here to open it in Vimeo. Note Original cam path is not modified by the normalization. If you didn't save your project then you need to normalize again when you load your project. You can freely move hotspots in 3D space by changing position values in hotspot properties. For more accurate path normalization it's better to have calibration points as far apart as possible from each other.","title":"12. Workflows"},{"location":"user_guide/workflows/#camera-path-normalization","text":"Normalization converts a camera path's coordinates to another coordinate space with given reference points.","title":"Camera Path Normalization"},{"location":"user_guide/workflows/#how-to-use-path-normalization","text":"Use a cam path that's not already normalized to same real-world coordinates. In editor open new or saved the project. You need to place three calibration point type hotspots and set real-world positions in properties. They also need to have calculated world positions by using auto tracking. \"Normalize\" button appears, click that and confirm. After normalization in 3D map make sure that the path looks good and calibration point spheres are aligned with the three cross-shaped objects that are real-world reference positions. Saving the project saves the hotspots and creates a copy of this normalized path in the same folder with \"_normalized\" postfix e.g. someCamPath.mp4_normalized.txt Optionally you can now delete calibration points if you wish. After normalization, you can denormalize using the button. For now, all it does it delete the normalized cam path and forces to reload project with nonnormalized cam path. That means if your hotspots are placed in normalized cam path then they won't appear in the correct positions in nonnormalized cam path. In normalization, all hotspot positions are converted to new normalized coordinate but same for denormalization has not yet been implemented. Watch a tutorial video on camera path normalization Video not working? Click here to open it in Vimeo. Note Original cam path is not modified by the normalization. If you didn't save your project then you need to normalize again when you load your project. You can freely move hotspots in 3D space by changing position values in hotspot properties. For more accurate path normalization it's better to have calibration points as far apart as possible from each other.","title":"How to use path normalization?"},{"location":"user_guide/workspace/","text":"After choosing a channel number from the Home page, the first view that you will be seen is the Mosaic view . Tip If you don't have a valid license, it is possible to use LiveSYNC in a trial mode. On Mosaaic view You can see the views from your audience devices. The next tab is the Channel tab . On the channel tab there is 4 digit channel number and a QR code generated for audience devices to connect. Number of connected devices can also be seen on this tab. The player tab . The player tab is where the magic happens. Contents including bundled demo contents are located on the left side of the view. If own contents are copied make sure to pull contents to refresh the contents list. The Content area is where contents are presented on, choose a content you want to present and drag and drop it to the content area. When playing videos, a Control Panel appears at the bottom of the Presentation area. Play, Pause and Seekbar controls work just like in any other video player. It is also possible to enable Looping to continuously repeat the video. Keep an eye on the remaining playback time so that you know when it is time to change content. Change content whenever necessary by simply drag another media item to the Presentation area. Existing photo or video will be immediately replaced with the new one. Connected devices are listed on the bottom side of the screen. Bundled tags are listed on the right side of the window. It is possible to add own tags and hotspot icons. Tip It is possible to hide/show Content , Devices and Tags views by simply tapping on the text \"Content\", \"Devices\" and \"Tags. \"","title":"5. Workspace"}]}