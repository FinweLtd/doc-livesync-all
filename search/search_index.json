{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to LiveSYNC\u2122 Learning Center This site contains the documentation for LiveSYNC Presentation Solution , a software tool developed by Finwe Ltd . Here you will find material for self-learning and problem-solving, as well as various downloadable resources. Solutions for learning How to Use This Site Read about different options for accessing this site, learn how to navigate, and get familiar with the used conventions. Quick Start Quick start guides are short introductions that summarize the steps to take to accomplish a task. These will get you going in minutes. Start from here if you are in a hurry. Tutorials Tutorials focus on a specific topic and collect the relevant information in one place. User Guide User Guide contains step-by-step instructions and deepens your understanding by providing background knowledge. Articles Articles are written by LiveSYNC experts. They focus on broader topics and specific use cases. Here you can learn best practices and find example configurations. FAQ Got a question? Check out frequently asked questions to get an answer quickly. Support Contact us to ask a question from the developers, send a feature request, suggest an article, or notify us of an error in the docs - we are listening. Resources Content Samples Download FREE sample files that demonstrate features and allow experimenting first hand. Templates Download FREE templates to be used as a basis for creating your own configuration. License Creative Commons This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License , excluding Twitter, Facebook, and other social media posts that link to this content Downloadable resource files, which are distributed under a different license","title":"Home"},{"location":"#welcome-to-livesynctm-learning-center","text":"This site contains the documentation for LiveSYNC Presentation Solution , a software tool developed by Finwe Ltd . Here you will find material for self-learning and problem-solving, as well as various downloadable resources.","title":"Welcome to LiveSYNC\u2122 Learning Center"},{"location":"#solutions-for-learning","text":"","title":"Solutions for learning"},{"location":"#how-to-use-this-site","text":"Read about different options for accessing this site, learn how to navigate, and get familiar with the used conventions.","title":"How to Use This Site"},{"location":"#quick-start","text":"Quick start guides are short introductions that summarize the steps to take to accomplish a task. These will get you going in minutes. Start from here if you are in a hurry.","title":"Quick Start"},{"location":"#tutorials","text":"Tutorials focus on a specific topic and collect the relevant information in one place.","title":"Tutorials"},{"location":"#user-guide","text":"User Guide contains step-by-step instructions and deepens your understanding by providing background knowledge.","title":"User Guide"},{"location":"#articles","text":"Articles are written by LiveSYNC experts. They focus on broader topics and specific use cases. Here you can learn best practices and find example configurations.","title":"Articles"},{"location":"#faq","text":"Got a question? Check out frequently asked questions to get an answer quickly.","title":"FAQ"},{"location":"#support","text":"Contact us to ask a question from the developers, send a feature request, suggest an article, or notify us of an error in the docs - we are listening.","title":"Support"},{"location":"#resources","text":"","title":"Resources"},{"location":"#content-samples","text":"Download FREE sample files that demonstrate features and allow experimenting first hand.","title":"Content Samples"},{"location":"#templates","text":"Download FREE templates to be used as a basis for creating your own configuration.","title":"Templates"},{"location":"#license","text":"","title":"License"},{"location":"#creative-commons","text":"This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License , excluding Twitter, Facebook, and other social media posts that link to this content Downloadable resource files, which are distributed under a different license","title":"Creative Commons"},{"location":"SUMMARY/","text":"Table of contents Initial page","title":"Table of contents"},{"location":"SUMMARY/#table-of-contents","text":"Initial page","title":"Table of contents"},{"location":"TODO/","text":"TODO Downloads Add links to video streams (.mp4 and .m3u8) Add more resolution variants to downloadable files Add stereo (3D) test images Add spatial audio (.obb) samples Add hotspot set Encoding For Oculus Go: https://creator.oculus.com/blog/encoding-high-resolution-360-and-180-video-for-oculus-go/?fbclid=IwAR0oS59ITYOxxkXZ_Ags-JS4ZEnnFW_pEAOE4sK7EaNpp8Fn-mySdqapfTs","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#downloads","text":"Add links to video streams (.mp4 and .m3u8) Add more resolution variants to downloadable files Add stereo (3D) test images Add spatial audio (.obb) samples Add hotspot set","title":"Downloads"},{"location":"TODO/#encoding","text":"For Oculus Go: https://creator.oculus.com/blog/encoding-high-resolution-360-and-180-video-for-oculus-go/?fbclid=IwAR0oS59ITYOxxkXZ_Ags-JS4ZEnnFW_pEAOE4sK7EaNpp8Fn-mySdqapfTs","title":"Encoding"},{"location":"articles/articles/","text":"Articles Articles focus on specific topics and use cases. Here you can learn best practises and find example configurations. Understanding Resolution in 360\u00b0 Images This article discusses what image resolution means for 360-degree content, how we can measure it, how it compares to resolution in traditional 2D content, and presents some example calculations.","title":"Index"},{"location":"articles/articles/#articles","text":"Articles focus on specific topics and use cases. Here you can learn best practises and find example configurations.","title":"Articles"},{"location":"articles/articles/#understanding-resolution-in-360-images","text":"This article discusses what image resolution means for 360-degree content, how we can measure it, how it compares to resolution in traditional 2D content, and presents some example calculations.","title":"Understanding Resolution in 360\u00b0 Images"},{"location":"articles/resolution/","text":"Understanding Resolution in 360\u00b0 Images Abstract This article discusses image quality. We focus on image resolution: the amount of detail and clarity. These are very significant to the perceived quality of an image. A content producer must find a reasonable balance between image quality and size. There are also many technical limitations to take into account. We have collected relevant information to one place to make it easier to choose a resolution. We also provide insights from Finwe experts. The article starts with the basics. We explore many different ways to understand, describe, and measure resolution. Then, we expand what we have learned to panoramic images. Devices used for content production and playback are presented and technical limitations discussed. Finally, we present our best practices and recommendations. Definition First, let us define what we mean by image resolution . Image resolution is the detail an image holds ... Higher resolution means more image detail. ( 1 ) The concept of image resolution applies to raster digital images ( 2 ), such as JPG, PNG and BMP images. Images created with vector graphics ( 3 ) are different: they are defined mathematically. One example is SVG images, which are used in apps and on web pages. For vector images, a resolution is meaningful only when they are saved as raster images. Digital cameras produce raster images. 360-degree photos and video frames make no exception. Sometimes a 360-degree image is rendered from a mathematical model, such as a CAD model of a building. The concept of resolution applies to the rasterized output file. To be specific, in this article we mostly discuss pixel resolution ie. the pixel count in the image. We also sometimes refer to spatial resolution ie. how closely lines can be resolved in an image, for example in a printed photograph. We do not discuss spectral resolution ie. the ability to resolve spectral features and bands, for example, colors. From now on, when we use the words image resolution or resolution alone, we mean pixel resolution. Pixel resolution illustration by Wikipedia ( 1 ) Resolution in 2D images Image resolution There are several methods to describe image resolution. In this article, we will use many of them: ... the convention is to describe the pixel resolution with the set of two positive integer numbers, where the first number is the number of pixel columns (width) and the second is the number of pixel rows (height), for example as 7680 \u00d7 6876. Another popular convention is to cite resolution as the total number of pixels in the image, typically given as number of megapixels, which can be calculated by multiplying pixel columns by pixel rows and dividing by one million. Other conventions include describing pixels per length unit or pixels per area unit, such as pixels per inch or per square inch. ( 1 ) As an example, the image below is 1024 pixels wide and 1024 pixels tall. We can describe its resolution as 1024 x 1024 pixels (width x height). We can also calculate the total number of pixels the image contains by multiplying width and height: 1024 px x 1024 px = 1048576 px After dividing this value by one million, we get the resolution in megapixels. It is usually rounded to the nearest integer, but sometimes to the first decimal: 1048576 px / 1000000 = 1.048576 MPx ~ 1.0 MPx ~ 1 MPx In other words, this image has a 1024x1024 or 1-megapixel resolution. It looks pretty sharp on the screen. It means that its resolution is high enough for the amount of detail it represents. We cannot say much about its number of pixels per length unit or per area unit . The reason is that on this website its size will be scaled to match with the width of this text column. The physical size of this text column depends on the size of the display device and application window. Hence, at the time of writing this, it is impossible to know the size of the image as it appears on your screen. As an example, let us consider a tablet's screen. The width of this column could be, say 163 mm. Hence, we could argue that its resolution must be about 6 pixels per millimeter: 1024 px / 163 mm = ~6.28 px/mm A more common representation uses the imperial notation, pixels per inch (ppi): 1024 px / 6.4 in = 160 ppi The value calculated above is not correct. If you are reading this from a computer screen, try changing the width of your browser window. You should see that the image above will be automatically resized. The resolution of the image file that your browser has downloaded has not changed. What you see on screen is actually a copy of the original image that your browser has created and scaled to match with this column's width. That copy probably has a different resolution than the original file does. Hence, our statement is using a wrong pixel count and is not true. How to find out the resolution of an image that appears on screen ? We need to know the physical size of the image on the screen and the pixel density of the display device. For example, a modern Apple iPad has 264 pixels per inch (ppi) ( 4 ). Thus, an image that is 6.4 inches wide should have about 1690 pixels when it is drawn on an iPad's screen. Curiously, this happens to be 666 pixels more than we have in the original image! Where did these extra pixels come from? 264 ppi * 6.4 in = ~1690 px 1690 px - 1024 px = +666 px Screen resolution When an image is captured and saved into a file, the file maintains the original image resolution ie. the number of pixels that were captured. When the image is loaded from a file and rendered on screen, it has a new property called screen resolution . This is the number of display pixels that are used for drawing the image on the screen. The former describes the amount of detail in the image but has nothing to do with its physical size. The latter has everything to do with physical size but says nothing about detail. Detail and size can be both expressed as a number of pixels, yet they are two different things. Now, although they are different things, they both play a part in the end result. Thus, we must understand the relation between image resolution and screen resolution. Consider our test image again: what happens if we reduce its image resolution to 100 x 100 pixels? Certainly, it would lose most of its detail! You cannot express such a complex image properly with so few pixels. Yet, on screen the browser would still scale it to match with the width of this column, no questions asked. This loss of detail is illustrated below. A 1024 x 1024 pixel image is first downscaled to 100 x 100 pixels and then upscaled back to 1024 x 1024 pixels. The result is a blurry mess. Left: Original image, 1024 x 1024 pixels. Center: Downscaled image, 100 x 100 pixels. Most of the detail has been permanently lost. Right: Upscaled image (1024 x 1024 pixels). Lost detail could not be recovered. How does this happen? Obviously, your computer must use much more than 100 x 100 pixels to draw the image on the screen. The display pixels are small and you need lots of them to draw a large image. But your computer cannot magically recreate the detail that was lost when we scaled down the original image. It is not possible, even if you see this happen regularly in crime series on TV. Hence, to create a larger image ie. a scaled-up version, the only option is to copy the same (or interpolated) pixels side by side. The resulting image on the screen will indeed have lots of pixels. But that only means it has high screen resolution ie. large size. Its image resolution is still 100 x 100 pixels, and contains very little detail. How to ensure that we get a sharp image on the screen? We must have enough space for detail (high enough image resolution). Or, we must decrease image size (low enough screen resolution). In the beginning, we briefly mentioned spatial resolution . This is what it describes: the ability to distinguish detail from an image that has a certain physical size. Or, more simply put: the clarity of the image. Spatial resolution is not just the value of pixels per inch. The maximum spatial resolution of an image is limited by the properties of the system that created the image. As an example, the lens of your camera is the first loop in a long chain. You can easily make things worse by manipulating image resolution and/or screen resolution, but you cannot make it better. (Well, maybe a little, by applying a filter that emphasizes certain features in the image.) What is the optimal relation between image resolution and screen resolution? When we prepare the final version of our image, then the optimal relation is that they are equal . Screen resolution defines what will be drawn on the screen ie. what user will see. Higher image resolution does not bring any benefit, as added detail cannot be seen. On the other hand, it may cause downscaling artifacts and will require a larger file size. Lower image resolution leads to upscaling ie. less detail than what an equal resolution would provide. Print resolution To complete our analysis for 2D images, let us briefly consider printing . If we printed our test image on paper in 100 x 100 mm size, we would have point density of ~10 points per millimeter. Correct? 1024 px / 100 mm = 10.24 points/mm A more common representation uses the imperial notation, dots per inch (dpi): 1024 px / 3.94 in = ~260 dpi It is not correct. A printer cannot print any number of points per millimeter. It has a resolution of its own, just like a display device has. The calculated value is a bit less than what a typical 300 dpi resolution laser printer is capable of. If we wish to use the printer's maximum resolution to reach the best possible print quality, we must satisfy with smaller image size than 100 mm x 100 mm: 1024 px / 300 dpi = 3.41 in ~ 87 mm 1024 x 1024 pixel image (left) printed from Photoshop in 300 dpi with a laser printer (right) is about 87 x 87 mm on paper. If we decide to print the image in 100 x 100 mm size, then the printer's driver must scale up our image to produce content for the \"missing\" pixels (actually dots). This is similar to what a web browser does when it stretches an image to a larger size on display by copying/interpolating pixels. 300 dpi * 3.94 in = 1182 px 1182 px - 1024 px = +152 px In reality, a laser printer users a rasterization algorithm to produce gray colors. It does this by alternating empty space between black printed dots. Hence, there will not be a 1:1 mapping from pixels to dots when you print a photograph. You can see this in action in the printed image above by looking at the grayscale squares. Nowadays many laser printers have 600 dpi or higher resolution. Inkjet printers often claim much higher dpi values. This is most aggressive marketing, but they also measure dpi as the number of drops of ink per inch. Moreover, they use algorithms that are supposed to improve print quality via optimization. Furthermore, in an image file the pixels are next to each other but on paper, the drops of ink have space between them. All in all, it is not straightforward to understand what is the actual mapping of image pixels to dots on paper. One rule of thumb is that most book publishers require images in 300 ppi resolution. Hence, if you want to print an image in 100 x 100 mm size on a book page, your image file should be at least 1182 x 1182 pixels. 300 dpi * 3.94 in = 1182 px Resolution in 360-degree images Equirectangular projection Now we will move on to discuss image resolution in 360-degree images. Let us begin by creating a wide-angle version of our test image. We will use our original test image as a single cube face and make five copies of it to produce the six faces of a cube. Then we will put a virtual camera to its center point and render a spherical 360-degree image. To save that 3D image into a flat PNG file, we must choose a projection . It must be one of those that are designed for projecting a sphere into a planar surface. (This is similar to how we project a map of our dear planet Earth to a piece of paper.) Here we will use the most common panorama projection: the equirectangular. The result looks like this: 360-degree image rendered from inside a test cube using equirectangular projection. Take a moment to examine the image above and find the four cube faces at the horizon level (front, right, back, left) as well as the cube's top and bottom faces (they are heavily distorted). The image covers 360 degrees horizontally: at the center of the cube, our virtual camera has turned around full 360 degrees along the yaw angle. As a consequence, left and right edges of the image match with each other seamlessly. The image also covers 180 degrees vertically: at the center of the cube, our virtual camera has turned around 180 degrees along the pitch angle. The top row of pixels are all the same and they illustrate what is exactly above the camera (zenith). The bottom row of pixels are also all the same and they illustrate what is exactly below the camera (natural direction, nadir). Thus, this is a full spherical 360x180 degree image and it covers every possible viewing direction that can be seen from the center of the cube. Of course, the described camera rotation is just a convention. It could have turned 180 degrees horizontally and 360 degrees vertically to produce a spherical image with a 1:2 aspect ratio. Also, the rotation angles do not need to align with the main axis at all to produce a spherical image, they just need to be perpendicular to each other. Notice that a spherical image is NOT 360x360 degrees; that is mathematically incorrect representation although it is sometimes seen in marketing materials. A 720-degree image is even more wrong! To see how a video player projects equirectangular video frames on screen, play the video below. Try interacting with it by panning. It is easier if you first select the fullscreen mode. While playing the video you may notice that image clarity suddenly changes. This is adaptive streaming in action: the player measures network bandwidth during playback and changes to the maximum resolution that bandwidth allows. Aspect ratio Equirectangular projection looks perhaps a bit weird at first sight. To understand it better, let us remind ourselves how the map of the World looks like in this projection. You have probably seen it before: An example of equirectangular projection by Wikipedia ( 5 ) In our previous example, we started from a 3D cube . We saw how straight lines got bent when the equirectangular projection was applied. This time we start from a 3D sphere (the Earth). Notice how the meridians are projected as vertical straight lines of constant spacing . And circles of latitude as horizontal straight lines of constant spacing . The equirectangular projection has many fallacies. It is not an equal area projection (see how large the Antarctic appears). It is not conformal (angles are not preserved). It is wasteful (lots of redundant pixels). Yet, the equirectangular projection is the de-facto standard in 360-degree imaging. This is probably related to the particularly simple relationship between the position of a pixel in the image and corresponding point on the surface of a sphere. It makes the life of software developers easy. Consider a full spherical equirectangular image whose resolution is 1024 x 512 pixels. (The map above will do.) Choose any pixel in the image. Now, if you want to move 30 degrees east, you can calculate this in pixels as shown below. To move in a vertical direction, you use 180\u00b0 degrees instead of 360\u00b0. If you pass the edge of the image, continue in the same direction from the opposite edge of the image. That's it. Simple linear relationship. It may not be obvious, but other projections are considerably more complex to handle. 1024 px * (30\u00b0/360\u00b0) = 85 px 2D images have a property called aspect ratio . It describes the proportional relationship between its width and its height. For example, a square has aspect ratio 1:1 because width and height are the same. An image whose resolution is 1920 x 1080 pixels has aspect ratio 16:9 or 1.77:1. 1920 x 1080 = (120 * 16) x (120 * 9) 16 : 9 = 1.77 In the World map image, horizontal and vertical lines appear at 15-degree intervals. Count the lines. You will notice that there is twice the number of vertical lines than horizontal lines. We can make a conclusion that proper aspect ratio for a full spherical equirectangular image must be 2:1 . This is because horizontally there are twice the amount of degrees than vertically (360 x 180). Also, we want to treat horizontal and vertical degrees equally ie. use the same amount of pixels per degree. 2:1 aspect ratio is a bit different than what we have used to, but not that far off from common video formats: Aspect ratio Normalized ratio Typical use 4:3 1.33:1 Traditional TV 16:9 1.78:1 HD video 1.85:1 1.85:1 Most movies 2:1 2.00:1 Equirectangular panoramas 2.39:1 2.39:1 Widescreen movies When you deliver 360-degree images or videos using the equirectangular projection, use 2:1 aspect ratio. Image width should be two times its height. For example, 1920 x 960 has proper aspect ratio 2:1. Commonly used FullHD resolution 1920 x 1080 is 16:9. It is taller . With that resolution, your image has more capacity for detail vertically than horizontally. 360-degree video players can handle that, but from the content point of view it makes little sense. Likewise, if you aim for 4K quality use 3840x1920 (2:1), not 3840x2160 (16:9). Left: 360x180 equirectangular image in correct aspect ratio 2:1. Right: The same image in incorrect 16:9 aspect ratio. You may wonder what harm is in using one of 16:9 resolutions. The answer is: not much. Consider a case where you need to downscale a video from 6K camera to 4K output. With 3840x2160 (16:9) you have better panorama resolution vertically than horizontally. You can do that, but why would you? Often this happens accidentally when you transcode a video into a different format. The default values in video software tend to lead to a 16:9 aspect ratio. If you are not careful, your 3840x1920 (2:1) video may become 3840x2160 (16:9). In such a case, you will add redundant data and create a larger file size without any benefit. Field-of-view We often refer to full spherical panorama images as 360x180 degree image. Or shortly, 360-degree image. What exactly does this value describe? The correct term is field-of-view or shortly FOV. Field-of-view is simply the size of an observable area, described in degrees ( 8 ). You have probably used a camera that has a zoom lens. When you utilize the zoom feature, you are effectively changing the field-of-view. When you zoom in , objects appear closer but the view becomes narrower. Hence, you have smaller field-of-view. When you zoom out , you see wider area through the viewfinder. You have larger field-of-view. Horizontal, vertical, and diagonal field-of-view illustraded by Wikipedia ( 8 ) We must be careful with field-of-view values, though. It can be horizontal, vertical, or diagonal measure of a rectangular viewable area. Unfortunately, it is not always mentioned which of these the given number is referring to. If you know one value and the aspect ratio, you can calculate the others using simple trigonometry. Abbreviation Meaning FOV Field-of-view in general (ambiguous) HFOV Horizontal field-of-view VFOV Vertical field-of-view DFOV Diagonal field-of-view When we view 360-degree content, there are effectively two different FOV values that play a part. Image field-of-view refers to the total observable area in the whole image frame. For example, in a spherical panorama image HFOV is 360 degrees and VFOV is 180 degrees. Diagonal FOV is not used. Viewport field-of-view is the part that is visible in the panorama image viewer or video player. For example, a pair of VR glasses could have HFOV 94 degrees. In many players this value is adjustable: user can zoom by changing the viewport's FOV. Panorama resolution Now, scroll back up and take a closer look at our equirectangular image of a cube. If you observe it carefully, you will notice that this image does not look as sharp as our original cube face. They are both 1024 pixels wide images and also scaled to the same width on the screen. Hence, both image resolution and screen resolution are exactly the same (horizontally). What is different? Recall that we added 5 more cube faces - much more information - to this new image. Yet we didn't increase image resolution ie. the amount of detail the image can hold. Major error. Besides, we changed the projection and halved the vertical resolution from 1024 px to 512 px. How can we measure the amount of detail in panoramic images, where field-of-view (the observable area) varies? As shown above, image resolution and screen resolution do not take this into account. We need something else. Consider that four cube faces (front, right, back, left) cover 360-degree spin at the horizon level. This means that one cube face must cover 90 degrees field-of-view (360/4=90). The width of our equirectangular image of a cube is 1024 pixels and its height is 512 pixels (2:1 aspect ratio). Thus, we can calculate its resolution per degree as follows and express it in pixels per degree (ppd): 1024 px / 360\u00b0 = ~2.84 ppd 512 px / 180\u00b0 = ~2.84 ppd To compare, our original cube covers only 90 degrees and has a higher resolution per degree: 1024 px / 90\u00b0 = ~11.38 ppd How can we make our 360-degree cube image carry as much detail as the original 90-degree cube face? Let us focus on the horizon level where the projection does not distort the image much. We need 4x the resolution that we have now: 360\u00b0 * 11.38 ppd = ~4096 px What if we made a 180-degree version, assuming viewers will focus to the front direction? How much resolution do we need to maintain detail? Simple: 180\u00b0 * 11.38 ppd = ~2048 px Resolution per degree appears to be a useful method for expressing the amount of detail in panoramic images. The benefit is that values are comparable despite used field-of-view. In this article, we will use the term panorama resolution to reference it. Notice that there are projections that do not provide constant resolution per degree throughout the image. To be specific, we probably should use the term equirectangular resolution . Here we use the easier and shorter term because equirectangular projection is so widely used. We have also learned that 360-degree images need much higher image resolution than ordinary 2D images to \"look the same quality\". This is often hard to understand for end users. People have a mental model of what \"FullHD\" or \"4K\" content is supposed to look like. But through a 360-degree player, you are likely seeing about 1/8th of the total amount of pixels at any moment of time! Let us rephrase the primary concepts of resolution for 360-degree images: Image resolution = total amount of detail in an image file / in a decoded video frame, in pixels Image field-of-view = total size of an observable area captured into an image, in degrees Panorama resolution = amount of detail per degree, in ppd Screen resolution = viewport size on screen, in pixels Viewport field-of-view = size of an observable area through a viewport, in degrees Towards perfect resolution Pixel perfect Retina resolution In order to make an image look sharp on screen, both image resolution and screen resolution must be high enough: you need enough pixels to store the details and enough pixels to draw them on screen so that they remain distinguishable. We have already said that it is best if they match perfectly: you can avoid aliasing errors that come from scaling if your image has exactly the same amount of pixels in file and on screen. You will also preserve all the detail without wasting memory for something that cannot be seen by user. A well known example is matching image resolutions of iOS app icons and button graphics with their screen resolutions so that they will look \"perfect\" as no scaling artifacts will occur. Nowadays it is becoming less common to aim for pixel perfect presentation as there are so many different aspect ratios and screen resolutions that need to be supported. For example, on Android the approach is different: to support thousands of different device models developers cannot make large amount of variants of each app icon; instead they are supposed to use vector graphics and let the device rasterize a pixel perfect copy at runtime. This approach of course does not work with images that have been captured with a camera and are raster images by nature. The solution is simple: use images whose pixel resolution matches with the highest screen resolution that will be needed. The operating system will downscale the images to lower resolutions on other device models. You can also choose to provide a fistful of different sizes and let the system select the nearest match. It is not perfect solution but good enough. It is also worth to realize that the concept of pixel perfect imaging mainly applies to traditional 2D images; pixel-to-pixel matching is not possible with spherical 360-degree images as they need to be projected from a spherical (curved) surface to a flat surface in order to be stored in common image and video formats, then projected back to spherical (curved) surface for 3D presentation within the image viewer / video player application, and once again to flat display surface when it is time to render part of the 360-degree image on screen. Try to preserve pixel-to-pixel matching through those operations! This leads us to a question: what would be the equivalent of pixel perfect presentation in 360-degree images? Since we cannot know all the internal processing that occurs when a specific pipeline renders our image on screen, in practice we just need an image that has enough resolution so that all the processing will not become visible in the end result. Then, what resolution is needed to make a 360-degree image look \"perfect\"? There is no single answer, as it depends on many parameters such as display resolution, used field-of-view, viewing distance, and projection. Let's discuss a few selected cases. When Apple introduced iPhone4, they claimed that their new Retina displays have high enough pixel density that the human eye cannot notice pixelation at a typical viewing distance. In other words, the display would appear to look perfect (when it comes to resolution) since you are unable to see the small dots that the image is made of. According to Apple the spatial resolution required for this is about 300 ppi for a device held 10-12 inches from the eye . In reality, a typical iPad has 9.7\" inch screen size (diagonally), 264 ppi display panel and 2048 x 1536 resolution. Now, if we want to view a 360-degree photograph from an iPad's screen that we will hold 10 inches from the eye, what would be the minimum pixel resolution for our 360-degree image so that it would provide optimal quality ie. have enough pixels for that display when we turn full 360 degrees around? Let's do some math: Consider a circle that aligns with the horizon around a spectators head. This is the path the iPad travels when we hold it in front of our eyes and turn around 360 degrees to see the complete 360-degree image. To simplify matters, we will assume a single eye at the center of the circle. The radius of the circle is 10 inches (25.4 cm) and its circumference is: 2 * PI * 10 in = 62.8 in ~160 cm Let us imagine that instead of moving a real iPad along this circular path, Apple would produce a round cylindrical display and we would simply go stand at the center of it. Our imaginary round iPad retina display would be 62.8 inches \"wide\" (circumference). Then, how many pixels would it have (horizontally)? 62.8 in * 264 ppi = ~16588 px If our imaginary display would be spherical , its resolution would be 16588 x 8294 pixels (because of aspect ratio 2:1). That would be a big display with almost 140 megapixels and not practical at all (how would you even get inside?) Of course, in reality we will move an ordinary tablet along a circular path, detect the movement with sensors, and update image content on screen. 16588 x 8294 = 137580872 ~ 138 Mpx Yet from content creation point of view our target is that imaginary spherical display. Creating content for such a display is not impossible at all: we can easily create much, much larger 360-degree photographs, and 8K 360-degree video is becoming commonplace - when we double that to 16K a few years later, we have achieved retina resolution for iPads. Neat. What if our 360-degree image is viewed through VR glasses? It is difficult to say what would be the correct distance for the radius of the circle that we used in the previous analysis: the display itself is really close to the eye, so the radius appears to be a small value. But in reality the image is viewed through a lens, and that must be taken into account. The focal distances in different headsets is usually not mentioned in the product specifications. Fortunately, there is a nice shortcut. Let us consider Oculus Go, whose display resolution is 2560 x 1440 in total, and 1280 x 1440 per eye. The horizontal field-of-view that a single eye covers in Oculus Go is said to be about the same as in Oculus Rift, where it was 94 degrees. Therefore, a 360-degree panorama image for Oculus Go should have at least 4902 x 2451 resolution or about 12 megapixels. This can be easily satisfied already by filming 360-degree photos and videos in 8K (or 6K) and providing final output in about 5K. 1280 px / 94\u00b0 = 13.61 px/\u00b0 360\u00b0 * 13.61 px/\u00b0 = 4902 px 4902 px / 2 = 2451 px 4902 px * 2451 px = ~12015115 px ~ 12.0 Mpx Human eye resolution However, Oculus Go is a nice headset but far from the concept of a retina display. If we want to future-proof our content and target for VR headsets that have \"retina displays\", what resolution do we need? This is the same question as what is the resolution of a human eye itself, how many megapixels? Human eye illustration by Wikipedia ( 6 ) According to ( 7 ), let us consider a view in front of a spectator that is 90 degrees by 90 degrees. The number of pixels a human eye could see through such window is ~324 Mpx. Remember our cube face that was also 90 by 90 degrees? To have full 360-degree view we need 6 cube faces, resulting to 6 * 324 Mpx = 1944 Mpx or 1.9 Gpx. If we wanted to use equirectangular projection instead, we will find that if square contains 324 Mpx then its side length is 18000 px and at horizon line we need 4 x 90 degrees for a 360-degree images, therefore 4 x 18000 px = 72000 px. Aspect ratio 2:1 yields 72000 x 36000 px and in total 2592 Mpx or 2.6 Gpx. Why do we need so much more pixels with equirectangular projection, compared to a cube map? This is because equirectangular projection is wasteful: there are multiple copies of same pixels ie. pixels that provide no new information. Cubemap projection requires 25% less pixels to produce same output. The interesting point is that about 2-3 Gpx image (72000 x 36000 px) should be enough for human eye, now and in the future. Hey, wait a minute? What about Apple's retina displays - we already calculated that 138 Mpx should be enough, right? Well, it is kind of apples and oranges case: The claim about retina displays is that at the typical viewing distance of 10-12 inches user is not suppose to see individual pixels, which is not exactly the same thing as how much details a human eye can see in best conditions. Also, ( 7 ) adds an interesting scientific point of view - apparently humans can see more details than our retina alone is able to distinguish: The eye is not a single frame snapshot camera. It is more like a video stream. The eye moves rapidly in small angular amounts and continually updates the image in one's brain to \"paint\" the detail. We also have two eyes, and our brains combine the signals to increase the resolution further. We also typically move our eyes around the scene to gather more information. Because of these factors, the eye plus brain assembles a higher resolution image than possible with the number of photoreceptors in the retina. So the megapixel equivalent numbers below refer to the spatial detail in an image that would be required to show what the human eye could see when you view a scene.","title":"Understanding resolution"},{"location":"articles/resolution/#understanding-resolution-in-360-images","text":"","title":"Understanding Resolution in 360\u00b0 Images"},{"location":"articles/resolution/#abstract","text":"This article discusses image quality. We focus on image resolution: the amount of detail and clarity. These are very significant to the perceived quality of an image. A content producer must find a reasonable balance between image quality and size. There are also many technical limitations to take into account. We have collected relevant information to one place to make it easier to choose a resolution. We also provide insights from Finwe experts. The article starts with the basics. We explore many different ways to understand, describe, and measure resolution. Then, we expand what we have learned to panoramic images. Devices used for content production and playback are presented and technical limitations discussed. Finally, we present our best practices and recommendations.","title":"Abstract"},{"location":"articles/resolution/#definition","text":"First, let us define what we mean by image resolution . Image resolution is the detail an image holds ... Higher resolution means more image detail. ( 1 ) The concept of image resolution applies to raster digital images ( 2 ), such as JPG, PNG and BMP images. Images created with vector graphics ( 3 ) are different: they are defined mathematically. One example is SVG images, which are used in apps and on web pages. For vector images, a resolution is meaningful only when they are saved as raster images. Digital cameras produce raster images. 360-degree photos and video frames make no exception. Sometimes a 360-degree image is rendered from a mathematical model, such as a CAD model of a building. The concept of resolution applies to the rasterized output file. To be specific, in this article we mostly discuss pixel resolution ie. the pixel count in the image. We also sometimes refer to spatial resolution ie. how closely lines can be resolved in an image, for example in a printed photograph. We do not discuss spectral resolution ie. the ability to resolve spectral features and bands, for example, colors. From now on, when we use the words image resolution or resolution alone, we mean pixel resolution. Pixel resolution illustration by Wikipedia ( 1 )","title":"Definition"},{"location":"articles/resolution/#resolution-in-2d-images","text":"","title":"Resolution in 2D images"},{"location":"articles/resolution/#image-resolution","text":"There are several methods to describe image resolution. In this article, we will use many of them: ... the convention is to describe the pixel resolution with the set of two positive integer numbers, where the first number is the number of pixel columns (width) and the second is the number of pixel rows (height), for example as 7680 \u00d7 6876. Another popular convention is to cite resolution as the total number of pixels in the image, typically given as number of megapixels, which can be calculated by multiplying pixel columns by pixel rows and dividing by one million. Other conventions include describing pixels per length unit or pixels per area unit, such as pixels per inch or per square inch. ( 1 ) As an example, the image below is 1024 pixels wide and 1024 pixels tall. We can describe its resolution as 1024 x 1024 pixels (width x height). We can also calculate the total number of pixels the image contains by multiplying width and height: 1024 px x 1024 px = 1048576 px After dividing this value by one million, we get the resolution in megapixels. It is usually rounded to the nearest integer, but sometimes to the first decimal: 1048576 px / 1000000 = 1.048576 MPx ~ 1.0 MPx ~ 1 MPx In other words, this image has a 1024x1024 or 1-megapixel resolution. It looks pretty sharp on the screen. It means that its resolution is high enough for the amount of detail it represents. We cannot say much about its number of pixels per length unit or per area unit . The reason is that on this website its size will be scaled to match with the width of this text column. The physical size of this text column depends on the size of the display device and application window. Hence, at the time of writing this, it is impossible to know the size of the image as it appears on your screen. As an example, let us consider a tablet's screen. The width of this column could be, say 163 mm. Hence, we could argue that its resolution must be about 6 pixels per millimeter: 1024 px / 163 mm = ~6.28 px/mm A more common representation uses the imperial notation, pixels per inch (ppi): 1024 px / 6.4 in = 160 ppi The value calculated above is not correct. If you are reading this from a computer screen, try changing the width of your browser window. You should see that the image above will be automatically resized. The resolution of the image file that your browser has downloaded has not changed. What you see on screen is actually a copy of the original image that your browser has created and scaled to match with this column's width. That copy probably has a different resolution than the original file does. Hence, our statement is using a wrong pixel count and is not true. How to find out the resolution of an image that appears on screen ? We need to know the physical size of the image on the screen and the pixel density of the display device. For example, a modern Apple iPad has 264 pixels per inch (ppi) ( 4 ). Thus, an image that is 6.4 inches wide should have about 1690 pixels when it is drawn on an iPad's screen. Curiously, this happens to be 666 pixels more than we have in the original image! Where did these extra pixels come from? 264 ppi * 6.4 in = ~1690 px 1690 px - 1024 px = +666 px","title":"Image resolution"},{"location":"articles/resolution/#screen-resolution","text":"When an image is captured and saved into a file, the file maintains the original image resolution ie. the number of pixels that were captured. When the image is loaded from a file and rendered on screen, it has a new property called screen resolution . This is the number of display pixels that are used for drawing the image on the screen. The former describes the amount of detail in the image but has nothing to do with its physical size. The latter has everything to do with physical size but says nothing about detail. Detail and size can be both expressed as a number of pixels, yet they are two different things. Now, although they are different things, they both play a part in the end result. Thus, we must understand the relation between image resolution and screen resolution. Consider our test image again: what happens if we reduce its image resolution to 100 x 100 pixels? Certainly, it would lose most of its detail! You cannot express such a complex image properly with so few pixels. Yet, on screen the browser would still scale it to match with the width of this column, no questions asked. This loss of detail is illustrated below. A 1024 x 1024 pixel image is first downscaled to 100 x 100 pixels and then upscaled back to 1024 x 1024 pixels. The result is a blurry mess. Left: Original image, 1024 x 1024 pixels. Center: Downscaled image, 100 x 100 pixels. Most of the detail has been permanently lost. Right: Upscaled image (1024 x 1024 pixels). Lost detail could not be recovered. How does this happen? Obviously, your computer must use much more than 100 x 100 pixels to draw the image on the screen. The display pixels are small and you need lots of them to draw a large image. But your computer cannot magically recreate the detail that was lost when we scaled down the original image. It is not possible, even if you see this happen regularly in crime series on TV. Hence, to create a larger image ie. a scaled-up version, the only option is to copy the same (or interpolated) pixels side by side. The resulting image on the screen will indeed have lots of pixels. But that only means it has high screen resolution ie. large size. Its image resolution is still 100 x 100 pixels, and contains very little detail. How to ensure that we get a sharp image on the screen? We must have enough space for detail (high enough image resolution). Or, we must decrease image size (low enough screen resolution). In the beginning, we briefly mentioned spatial resolution . This is what it describes: the ability to distinguish detail from an image that has a certain physical size. Or, more simply put: the clarity of the image. Spatial resolution is not just the value of pixels per inch. The maximum spatial resolution of an image is limited by the properties of the system that created the image. As an example, the lens of your camera is the first loop in a long chain. You can easily make things worse by manipulating image resolution and/or screen resolution, but you cannot make it better. (Well, maybe a little, by applying a filter that emphasizes certain features in the image.) What is the optimal relation between image resolution and screen resolution? When we prepare the final version of our image, then the optimal relation is that they are equal . Screen resolution defines what will be drawn on the screen ie. what user will see. Higher image resolution does not bring any benefit, as added detail cannot be seen. On the other hand, it may cause downscaling artifacts and will require a larger file size. Lower image resolution leads to upscaling ie. less detail than what an equal resolution would provide.","title":"Screen resolution"},{"location":"articles/resolution/#print-resolution","text":"To complete our analysis for 2D images, let us briefly consider printing . If we printed our test image on paper in 100 x 100 mm size, we would have point density of ~10 points per millimeter. Correct? 1024 px / 100 mm = 10.24 points/mm A more common representation uses the imperial notation, dots per inch (dpi): 1024 px / 3.94 in = ~260 dpi It is not correct. A printer cannot print any number of points per millimeter. It has a resolution of its own, just like a display device has. The calculated value is a bit less than what a typical 300 dpi resolution laser printer is capable of. If we wish to use the printer's maximum resolution to reach the best possible print quality, we must satisfy with smaller image size than 100 mm x 100 mm: 1024 px / 300 dpi = 3.41 in ~ 87 mm 1024 x 1024 pixel image (left) printed from Photoshop in 300 dpi with a laser printer (right) is about 87 x 87 mm on paper. If we decide to print the image in 100 x 100 mm size, then the printer's driver must scale up our image to produce content for the \"missing\" pixels (actually dots). This is similar to what a web browser does when it stretches an image to a larger size on display by copying/interpolating pixels. 300 dpi * 3.94 in = 1182 px 1182 px - 1024 px = +152 px In reality, a laser printer users a rasterization algorithm to produce gray colors. It does this by alternating empty space between black printed dots. Hence, there will not be a 1:1 mapping from pixels to dots when you print a photograph. You can see this in action in the printed image above by looking at the grayscale squares. Nowadays many laser printers have 600 dpi or higher resolution. Inkjet printers often claim much higher dpi values. This is most aggressive marketing, but they also measure dpi as the number of drops of ink per inch. Moreover, they use algorithms that are supposed to improve print quality via optimization. Furthermore, in an image file the pixels are next to each other but on paper, the drops of ink have space between them. All in all, it is not straightforward to understand what is the actual mapping of image pixels to dots on paper. One rule of thumb is that most book publishers require images in 300 ppi resolution. Hence, if you want to print an image in 100 x 100 mm size on a book page, your image file should be at least 1182 x 1182 pixels. 300 dpi * 3.94 in = 1182 px","title":"Print resolution"},{"location":"articles/resolution/#resolution-in-360-degree-images","text":"","title":"Resolution in 360-degree images"},{"location":"articles/resolution/#equirectangular-projection","text":"Now we will move on to discuss image resolution in 360-degree images. Let us begin by creating a wide-angle version of our test image. We will use our original test image as a single cube face and make five copies of it to produce the six faces of a cube. Then we will put a virtual camera to its center point and render a spherical 360-degree image. To save that 3D image into a flat PNG file, we must choose a projection . It must be one of those that are designed for projecting a sphere into a planar surface. (This is similar to how we project a map of our dear planet Earth to a piece of paper.) Here we will use the most common panorama projection: the equirectangular. The result looks like this: 360-degree image rendered from inside a test cube using equirectangular projection. Take a moment to examine the image above and find the four cube faces at the horizon level (front, right, back, left) as well as the cube's top and bottom faces (they are heavily distorted). The image covers 360 degrees horizontally: at the center of the cube, our virtual camera has turned around full 360 degrees along the yaw angle. As a consequence, left and right edges of the image match with each other seamlessly. The image also covers 180 degrees vertically: at the center of the cube, our virtual camera has turned around 180 degrees along the pitch angle. The top row of pixels are all the same and they illustrate what is exactly above the camera (zenith). The bottom row of pixels are also all the same and they illustrate what is exactly below the camera (natural direction, nadir). Thus, this is a full spherical 360x180 degree image and it covers every possible viewing direction that can be seen from the center of the cube. Of course, the described camera rotation is just a convention. It could have turned 180 degrees horizontally and 360 degrees vertically to produce a spherical image with a 1:2 aspect ratio. Also, the rotation angles do not need to align with the main axis at all to produce a spherical image, they just need to be perpendicular to each other. Notice that a spherical image is NOT 360x360 degrees; that is mathematically incorrect representation although it is sometimes seen in marketing materials. A 720-degree image is even more wrong! To see how a video player projects equirectangular video frames on screen, play the video below. Try interacting with it by panning. It is easier if you first select the fullscreen mode. While playing the video you may notice that image clarity suddenly changes. This is adaptive streaming in action: the player measures network bandwidth during playback and changes to the maximum resolution that bandwidth allows.","title":"Equirectangular projection"},{"location":"articles/resolution/#aspect-ratio","text":"Equirectangular projection looks perhaps a bit weird at first sight. To understand it better, let us remind ourselves how the map of the World looks like in this projection. You have probably seen it before: An example of equirectangular projection by Wikipedia ( 5 ) In our previous example, we started from a 3D cube . We saw how straight lines got bent when the equirectangular projection was applied. This time we start from a 3D sphere (the Earth). Notice how the meridians are projected as vertical straight lines of constant spacing . And circles of latitude as horizontal straight lines of constant spacing . The equirectangular projection has many fallacies. It is not an equal area projection (see how large the Antarctic appears). It is not conformal (angles are not preserved). It is wasteful (lots of redundant pixels). Yet, the equirectangular projection is the de-facto standard in 360-degree imaging. This is probably related to the particularly simple relationship between the position of a pixel in the image and corresponding point on the surface of a sphere. It makes the life of software developers easy. Consider a full spherical equirectangular image whose resolution is 1024 x 512 pixels. (The map above will do.) Choose any pixel in the image. Now, if you want to move 30 degrees east, you can calculate this in pixels as shown below. To move in a vertical direction, you use 180\u00b0 degrees instead of 360\u00b0. If you pass the edge of the image, continue in the same direction from the opposite edge of the image. That's it. Simple linear relationship. It may not be obvious, but other projections are considerably more complex to handle. 1024 px * (30\u00b0/360\u00b0) = 85 px 2D images have a property called aspect ratio . It describes the proportional relationship between its width and its height. For example, a square has aspect ratio 1:1 because width and height are the same. An image whose resolution is 1920 x 1080 pixels has aspect ratio 16:9 or 1.77:1. 1920 x 1080 = (120 * 16) x (120 * 9) 16 : 9 = 1.77 In the World map image, horizontal and vertical lines appear at 15-degree intervals. Count the lines. You will notice that there is twice the number of vertical lines than horizontal lines. We can make a conclusion that proper aspect ratio for a full spherical equirectangular image must be 2:1 . This is because horizontally there are twice the amount of degrees than vertically (360 x 180). Also, we want to treat horizontal and vertical degrees equally ie. use the same amount of pixels per degree. 2:1 aspect ratio is a bit different than what we have used to, but not that far off from common video formats: Aspect ratio Normalized ratio Typical use 4:3 1.33:1 Traditional TV 16:9 1.78:1 HD video 1.85:1 1.85:1 Most movies 2:1 2.00:1 Equirectangular panoramas 2.39:1 2.39:1 Widescreen movies When you deliver 360-degree images or videos using the equirectangular projection, use 2:1 aspect ratio. Image width should be two times its height. For example, 1920 x 960 has proper aspect ratio 2:1. Commonly used FullHD resolution 1920 x 1080 is 16:9. It is taller . With that resolution, your image has more capacity for detail vertically than horizontally. 360-degree video players can handle that, but from the content point of view it makes little sense. Likewise, if you aim for 4K quality use 3840x1920 (2:1), not 3840x2160 (16:9). Left: 360x180 equirectangular image in correct aspect ratio 2:1. Right: The same image in incorrect 16:9 aspect ratio. You may wonder what harm is in using one of 16:9 resolutions. The answer is: not much. Consider a case where you need to downscale a video from 6K camera to 4K output. With 3840x2160 (16:9) you have better panorama resolution vertically than horizontally. You can do that, but why would you? Often this happens accidentally when you transcode a video into a different format. The default values in video software tend to lead to a 16:9 aspect ratio. If you are not careful, your 3840x1920 (2:1) video may become 3840x2160 (16:9). In such a case, you will add redundant data and create a larger file size without any benefit.","title":"Aspect ratio"},{"location":"articles/resolution/#field-of-view","text":"We often refer to full spherical panorama images as 360x180 degree image. Or shortly, 360-degree image. What exactly does this value describe? The correct term is field-of-view or shortly FOV. Field-of-view is simply the size of an observable area, described in degrees ( 8 ). You have probably used a camera that has a zoom lens. When you utilize the zoom feature, you are effectively changing the field-of-view. When you zoom in , objects appear closer but the view becomes narrower. Hence, you have smaller field-of-view. When you zoom out , you see wider area through the viewfinder. You have larger field-of-view. Horizontal, vertical, and diagonal field-of-view illustraded by Wikipedia ( 8 ) We must be careful with field-of-view values, though. It can be horizontal, vertical, or diagonal measure of a rectangular viewable area. Unfortunately, it is not always mentioned which of these the given number is referring to. If you know one value and the aspect ratio, you can calculate the others using simple trigonometry. Abbreviation Meaning FOV Field-of-view in general (ambiguous) HFOV Horizontal field-of-view VFOV Vertical field-of-view DFOV Diagonal field-of-view When we view 360-degree content, there are effectively two different FOV values that play a part. Image field-of-view refers to the total observable area in the whole image frame. For example, in a spherical panorama image HFOV is 360 degrees and VFOV is 180 degrees. Diagonal FOV is not used. Viewport field-of-view is the part that is visible in the panorama image viewer or video player. For example, a pair of VR glasses could have HFOV 94 degrees. In many players this value is adjustable: user can zoom by changing the viewport's FOV.","title":"Field-of-view"},{"location":"articles/resolution/#panorama-resolution","text":"Now, scroll back up and take a closer look at our equirectangular image of a cube. If you observe it carefully, you will notice that this image does not look as sharp as our original cube face. They are both 1024 pixels wide images and also scaled to the same width on the screen. Hence, both image resolution and screen resolution are exactly the same (horizontally). What is different? Recall that we added 5 more cube faces - much more information - to this new image. Yet we didn't increase image resolution ie. the amount of detail the image can hold. Major error. Besides, we changed the projection and halved the vertical resolution from 1024 px to 512 px. How can we measure the amount of detail in panoramic images, where field-of-view (the observable area) varies? As shown above, image resolution and screen resolution do not take this into account. We need something else. Consider that four cube faces (front, right, back, left) cover 360-degree spin at the horizon level. This means that one cube face must cover 90 degrees field-of-view (360/4=90). The width of our equirectangular image of a cube is 1024 pixels and its height is 512 pixels (2:1 aspect ratio). Thus, we can calculate its resolution per degree as follows and express it in pixels per degree (ppd): 1024 px / 360\u00b0 = ~2.84 ppd 512 px / 180\u00b0 = ~2.84 ppd To compare, our original cube covers only 90 degrees and has a higher resolution per degree: 1024 px / 90\u00b0 = ~11.38 ppd How can we make our 360-degree cube image carry as much detail as the original 90-degree cube face? Let us focus on the horizon level where the projection does not distort the image much. We need 4x the resolution that we have now: 360\u00b0 * 11.38 ppd = ~4096 px What if we made a 180-degree version, assuming viewers will focus to the front direction? How much resolution do we need to maintain detail? Simple: 180\u00b0 * 11.38 ppd = ~2048 px Resolution per degree appears to be a useful method for expressing the amount of detail in panoramic images. The benefit is that values are comparable despite used field-of-view. In this article, we will use the term panorama resolution to reference it. Notice that there are projections that do not provide constant resolution per degree throughout the image. To be specific, we probably should use the term equirectangular resolution . Here we use the easier and shorter term because equirectangular projection is so widely used. We have also learned that 360-degree images need much higher image resolution than ordinary 2D images to \"look the same quality\". This is often hard to understand for end users. People have a mental model of what \"FullHD\" or \"4K\" content is supposed to look like. But through a 360-degree player, you are likely seeing about 1/8th of the total amount of pixels at any moment of time! Let us rephrase the primary concepts of resolution for 360-degree images: Image resolution = total amount of detail in an image file / in a decoded video frame, in pixels Image field-of-view = total size of an observable area captured into an image, in degrees Panorama resolution = amount of detail per degree, in ppd Screen resolution = viewport size on screen, in pixels Viewport field-of-view = size of an observable area through a viewport, in degrees","title":"Panorama resolution"},{"location":"articles/resolution/#towards-perfect-resolution","text":"","title":"Towards perfect resolution"},{"location":"articles/resolution/#pixel-perfect","text":"","title":"Pixel perfect"},{"location":"articles/resolution/#retina-resolution","text":"In order to make an image look sharp on screen, both image resolution and screen resolution must be high enough: you need enough pixels to store the details and enough pixels to draw them on screen so that they remain distinguishable. We have already said that it is best if they match perfectly: you can avoid aliasing errors that come from scaling if your image has exactly the same amount of pixels in file and on screen. You will also preserve all the detail without wasting memory for something that cannot be seen by user. A well known example is matching image resolutions of iOS app icons and button graphics with their screen resolutions so that they will look \"perfect\" as no scaling artifacts will occur. Nowadays it is becoming less common to aim for pixel perfect presentation as there are so many different aspect ratios and screen resolutions that need to be supported. For example, on Android the approach is different: to support thousands of different device models developers cannot make large amount of variants of each app icon; instead they are supposed to use vector graphics and let the device rasterize a pixel perfect copy at runtime. This approach of course does not work with images that have been captured with a camera and are raster images by nature. The solution is simple: use images whose pixel resolution matches with the highest screen resolution that will be needed. The operating system will downscale the images to lower resolutions on other device models. You can also choose to provide a fistful of different sizes and let the system select the nearest match. It is not perfect solution but good enough. It is also worth to realize that the concept of pixel perfect imaging mainly applies to traditional 2D images; pixel-to-pixel matching is not possible with spherical 360-degree images as they need to be projected from a spherical (curved) surface to a flat surface in order to be stored in common image and video formats, then projected back to spherical (curved) surface for 3D presentation within the image viewer / video player application, and once again to flat display surface when it is time to render part of the 360-degree image on screen. Try to preserve pixel-to-pixel matching through those operations! This leads us to a question: what would be the equivalent of pixel perfect presentation in 360-degree images? Since we cannot know all the internal processing that occurs when a specific pipeline renders our image on screen, in practice we just need an image that has enough resolution so that all the processing will not become visible in the end result. Then, what resolution is needed to make a 360-degree image look \"perfect\"? There is no single answer, as it depends on many parameters such as display resolution, used field-of-view, viewing distance, and projection. Let's discuss a few selected cases. When Apple introduced iPhone4, they claimed that their new Retina displays have high enough pixel density that the human eye cannot notice pixelation at a typical viewing distance. In other words, the display would appear to look perfect (when it comes to resolution) since you are unable to see the small dots that the image is made of. According to Apple the spatial resolution required for this is about 300 ppi for a device held 10-12 inches from the eye . In reality, a typical iPad has 9.7\" inch screen size (diagonally), 264 ppi display panel and 2048 x 1536 resolution. Now, if we want to view a 360-degree photograph from an iPad's screen that we will hold 10 inches from the eye, what would be the minimum pixel resolution for our 360-degree image so that it would provide optimal quality ie. have enough pixels for that display when we turn full 360 degrees around? Let's do some math: Consider a circle that aligns with the horizon around a spectators head. This is the path the iPad travels when we hold it in front of our eyes and turn around 360 degrees to see the complete 360-degree image. To simplify matters, we will assume a single eye at the center of the circle. The radius of the circle is 10 inches (25.4 cm) and its circumference is: 2 * PI * 10 in = 62.8 in ~160 cm Let us imagine that instead of moving a real iPad along this circular path, Apple would produce a round cylindrical display and we would simply go stand at the center of it. Our imaginary round iPad retina display would be 62.8 inches \"wide\" (circumference). Then, how many pixels would it have (horizontally)? 62.8 in * 264 ppi = ~16588 px If our imaginary display would be spherical , its resolution would be 16588 x 8294 pixels (because of aspect ratio 2:1). That would be a big display with almost 140 megapixels and not practical at all (how would you even get inside?) Of course, in reality we will move an ordinary tablet along a circular path, detect the movement with sensors, and update image content on screen. 16588 x 8294 = 137580872 ~ 138 Mpx Yet from content creation point of view our target is that imaginary spherical display. Creating content for such a display is not impossible at all: we can easily create much, much larger 360-degree photographs, and 8K 360-degree video is becoming commonplace - when we double that to 16K a few years later, we have achieved retina resolution for iPads. Neat. What if our 360-degree image is viewed through VR glasses? It is difficult to say what would be the correct distance for the radius of the circle that we used in the previous analysis: the display itself is really close to the eye, so the radius appears to be a small value. But in reality the image is viewed through a lens, and that must be taken into account. The focal distances in different headsets is usually not mentioned in the product specifications. Fortunately, there is a nice shortcut. Let us consider Oculus Go, whose display resolution is 2560 x 1440 in total, and 1280 x 1440 per eye. The horizontal field-of-view that a single eye covers in Oculus Go is said to be about the same as in Oculus Rift, where it was 94 degrees. Therefore, a 360-degree panorama image for Oculus Go should have at least 4902 x 2451 resolution or about 12 megapixels. This can be easily satisfied already by filming 360-degree photos and videos in 8K (or 6K) and providing final output in about 5K. 1280 px / 94\u00b0 = 13.61 px/\u00b0 360\u00b0 * 13.61 px/\u00b0 = 4902 px 4902 px / 2 = 2451 px 4902 px * 2451 px = ~12015115 px ~ 12.0 Mpx","title":"Retina resolution"},{"location":"articles/resolution/#human-eye-resolution","text":"However, Oculus Go is a nice headset but far from the concept of a retina display. If we want to future-proof our content and target for VR headsets that have \"retina displays\", what resolution do we need? This is the same question as what is the resolution of a human eye itself, how many megapixels? Human eye illustration by Wikipedia ( 6 ) According to ( 7 ), let us consider a view in front of a spectator that is 90 degrees by 90 degrees. The number of pixels a human eye could see through such window is ~324 Mpx. Remember our cube face that was also 90 by 90 degrees? To have full 360-degree view we need 6 cube faces, resulting to 6 * 324 Mpx = 1944 Mpx or 1.9 Gpx. If we wanted to use equirectangular projection instead, we will find that if square contains 324 Mpx then its side length is 18000 px and at horizon line we need 4 x 90 degrees for a 360-degree images, therefore 4 x 18000 px = 72000 px. Aspect ratio 2:1 yields 72000 x 36000 px and in total 2592 Mpx or 2.6 Gpx. Why do we need so much more pixels with equirectangular projection, compared to a cube map? This is because equirectangular projection is wasteful: there are multiple copies of same pixels ie. pixels that provide no new information. Cubemap projection requires 25% less pixels to produce same output. The interesting point is that about 2-3 Gpx image (72000 x 36000 px) should be enough for human eye, now and in the future. Hey, wait a minute? What about Apple's retina displays - we already calculated that 138 Mpx should be enough, right? Well, it is kind of apples and oranges case: The claim about retina displays is that at the typical viewing distance of 10-12 inches user is not suppose to see individual pixels, which is not exactly the same thing as how much details a human eye can see in best conditions. Also, ( 7 ) adds an interesting scientific point of view - apparently humans can see more details than our retina alone is able to distinguish: The eye is not a single frame snapshot camera. It is more like a video stream. The eye moves rapidly in small angular amounts and continually updates the image in one's brain to \"paint\" the detail. We also have two eyes, and our brains combine the signals to increase the resolution further. We also typically move our eyes around the scene to gather more information. Because of these factors, the eye plus brain assembles a higher resolution image than possible with the number of photoreceptors in the retina. So the megapixel equivalent numbers below refer to the spatial detail in an image that would be required to show what the human eye could see when you view a scene.","title":"Human eye resolution"},{"location":"downloads/downloads/","text":"Downloads Here you will find an archive of downloadable files. Please read the license below before using them. Content Samples Demo Photo Photo Copyright (c) Tapani Rantakokko This photo was shot with Canon 6D DSLR camera through a 10mm fisheye lens, and stitched with PtGui. The same photo is bundled with the app (\"Olos\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 photo, equirectangular projection, 24-bit JPG Download 4096x2048 (2.6 MB) Download 8192x4096 (9.2 MB) Demo Video Video Copyright (c) Juha Kela This video was shot with 6x GoPro Hero cameras, and stitched with Kolor AVP. The same video is bundled with the app (\"PowerPark\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 video, equirectangular projection, MP4 Download 1920x1080 25fps 55sec h264 (81.5 MB) Download 3840x2160 25fps 55sec h264 (155.7 MB) Test Image Image Copyright (c) Finwe Ltd. This image was drawn with Inkscape (6x cube walls) and stitched with PtGui. Full spherical 360x180\u00b0 image, equirectangular projection, 24-bit JPG Download 2048x1024 (2.2 MB) Download 4096x2048 (7.7 MB) Download 8192x4096 (22.9 MB) Test Video Video Copyright (c) Finwe Ltd. This video was first drawn with Inkscape (6x cube walls) and stitched with PtGui, then encoded to video with ffmpeg. Download 1920x960 30fps 30sec h264 (2.6 MB) Download 1920x960 60fps 30sec h264 (4.5 MB) Download 3840x1920 30fps 30sec h264 (8.3 MB) Download 3840x1920 60fps 30sec h264 (14.5 MB) Stereo (3D) Test Video Video Copyright (c) Finwe Ltd. This image was first drawn with Blender and rendered to stereo (3D) image, then encoded to video with ffmpeg. Notice that top half contains left eye image and bottom half right eye image. Use a VR headset for viewing this video in stereo; on phone/tablet only left eye image is shown. This video was encoded with more efficient h265 encoder. Not all devices support it. Download 3D 3840x1920 30fps 30sec h265 (3.4 MB) Download 3D 3840x3840 30fps 30sec h265 (8.3 MB) Templates settings.ini settings.ini is an optional configuration file that you can copy to the LiveSYNC root folder (same location where content files are copied to). The file contains many configuration options in a user-friendly windows ini-file format. Each setting is described with a comment. Download latest version License Creative Commons This work is licensed under a Creative Commons Attribution-Share Alike 3.0 Unported License","title":"Index"},{"location":"downloads/downloads/#downloads","text":"Here you will find an archive of downloadable files. Please read the license below before using them.","title":"Downloads"},{"location":"downloads/downloads/#content-samples","text":"","title":"Content Samples"},{"location":"downloads/downloads/#demo-photo","text":"Photo Copyright (c) Tapani Rantakokko This photo was shot with Canon 6D DSLR camera through a 10mm fisheye lens, and stitched with PtGui. The same photo is bundled with the app (\"Olos\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 photo, equirectangular projection, 24-bit JPG Download 4096x2048 (2.6 MB) Download 8192x4096 (9.2 MB)","title":"Demo Photo"},{"location":"downloads/downloads/#demo-video","text":"Video Copyright (c) Juha Kela This video was shot with 6x GoPro Hero cameras, and stitched with Kolor AVP. The same video is bundled with the app (\"PowerPark\") as a smaller file / lower quality version. Full spherical 360x180\u00b0 video, equirectangular projection, MP4 Download 1920x1080 25fps 55sec h264 (81.5 MB) Download 3840x2160 25fps 55sec h264 (155.7 MB)","title":"Demo Video"},{"location":"downloads/downloads/#test-image","text":"Image Copyright (c) Finwe Ltd. This image was drawn with Inkscape (6x cube walls) and stitched with PtGui. Full spherical 360x180\u00b0 image, equirectangular projection, 24-bit JPG Download 2048x1024 (2.2 MB) Download 4096x2048 (7.7 MB) Download 8192x4096 (22.9 MB)","title":"Test Image"},{"location":"downloads/downloads/#test-video","text":"Video Copyright (c) Finwe Ltd. This video was first drawn with Inkscape (6x cube walls) and stitched with PtGui, then encoded to video with ffmpeg. Download 1920x960 30fps 30sec h264 (2.6 MB) Download 1920x960 60fps 30sec h264 (4.5 MB) Download 3840x1920 30fps 30sec h264 (8.3 MB) Download 3840x1920 60fps 30sec h264 (14.5 MB)","title":"Test Video"},{"location":"downloads/downloads/#stereo-3d-test-video","text":"Video Copyright (c) Finwe Ltd. This image was first drawn with Blender and rendered to stereo (3D) image, then encoded to video with ffmpeg. Notice that top half contains left eye image and bottom half right eye image. Use a VR headset for viewing this video in stereo; on phone/tablet only left eye image is shown. This video was encoded with more efficient h265 encoder. Not all devices support it. Download 3D 3840x1920 30fps 30sec h265 (3.4 MB) Download 3D 3840x3840 30fps 30sec h265 (8.3 MB)","title":"Stereo (3D) Test Video"},{"location":"downloads/downloads/#templates","text":"","title":"Templates"},{"location":"downloads/downloads/#settingsini","text":"settings.ini is an optional configuration file that you can copy to the LiveSYNC root folder (same location where content files are copied to). The file contains many configuration options in a user-friendly windows ini-file format. Each setting is described with a comment. Download latest version","title":"settings.ini"},{"location":"downloads/downloads/#license","text":"","title":"License"},{"location":"downloads/downloads/#creative-commons","text":"This work is licensed under a Creative Commons Attribution-Share Alike 3.0 Unported License","title":"Creative Commons"},{"location":"faq/faq/","text":"Frequently Asked Questions (FAQ) This is a collection of frequently asked questions, answered by our experts. If you cannot find an answer to your question, please contact us via the form on the support page . TIP: Try the search feature in the top right corner! Hardware What iOS devices are supported? LiveSYNC can be installed on iPhones and iPads, but currently not on AppleTV. The app requires iOS 8.0 or later. While LiveSYNC works fairly well also on iOS devices that are several years old, some feature limitations exist: Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your iOS device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your iPad. I need to buy an iPad to be used as the director device. Which model do you recommend? In general, a new standard size (9.7\") iPad is a good value for money. You can also choose Pro version, but it is not necessary unless you want a larger size model (11\" or 12.9\"). What Android devices are supported? LiveSYNC can be installed on Android phones and tablets, but currently not on AndroidTV. The app requires Android 5.0 (API level 21) or later. While LiveSYNC works fairly well also on Android devices that are several years old, some feature limitations exist: Director mode with Bluetooth communication may not work on some old devices that have been upgraded to Android 5.0, due to Bluetooth chipset that does not support all required features. Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your Android device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your tablet. I need to buy an Android tablet to be used as the director device. Which model do you recommend? Just to name one, we've had good experience with Samsung Galaxy Tab S2 or newer. However, there are plenty of options. Installing Can I install LiveSYNC on my iPhone or iPad? Yes. On your iOS device, open App Store , tap Search , and type 'livesync'. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Can I install LiveSYNC on my Android phone or tablet? Yes. On your Android device, open Play Store , and type 'livesync' to the search field. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Can I install LiveSYNC on my Windows phone or tablet? No. Windows is currently not supported. Can I install LiveSYNC on my Windows/Mac/Linux computer? No. Desktop computer operating systems are currently not supported. Can I install LiveSYNC on my GearVR/Oculus Go headset? Yes, but currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus device, navigate to Oculus Store and type 'livesync' to the search field. Select 'LiveSYNC Oculus Go'. If you cannot find it, don't forget to check also the 'Not installed' tab. The app can be installed for FREE. Can I install LiveSYNC on my Oculus Rift/HTC Vive headset? No. LiveSYNC is currently available for standalone VR headsets, not for devices that depend on a desktop/laptop computer. I have a Cardboard (or compatible) VR headset. Can I use that with LiveSYNC? Yes. Both iOS and Android versions have built-in support for VR mode on phones (disabled in tablets). During channel configuration select 'VR' for viewing mode, or during playback long tap the screen to toggle between normal and VR mode. If you have a GearVR headset, please use our separate Oculus Go version from Oculus Store instead (invite only). The rendering quality and head tracking are far superior! Beta versions Can I test new features before they are released by installing a beta version? Yes. Please follow the instructions below: Android: We have an open beta channel in Google Play. Anyone can join simply by opting-in here and then installing the beta version by following instructions on that page. iOS: We have a closed beta channel in Apple TestFlight. If you wish to participate, please use the contact form in the support page for requesting an invite. GearVR/Oculus Go: We have a closed beta channel in Oculus Store. If you wish to participate, please use the contact form in the support page for requesting an invite. Notice: We need your Oculus username/email for sending the invite. Can I install BOTH the beta version and the store version into the same device? No. All the stores allow having only one version installed at a time. In general this will be the newest version ie. the one with the highest version number. Configuration Can I create multiple channel configurations, and swap between them during my presentation? Yes, sort of. Each channel has a channel number, and only those audience devices that are listening on the same channel that you are on will respond to your commands. You can control device group 1 first on channel 1234, and then move on to control device group 2 on channel 5678, for instance. However, when you leave a presentation to switch to another one, the audience devices that you were previously controlling will lose the connection and return to the lobby. Hence, you cannot have more than one presentation ongoing simultaneously with only one controller device. Content What file formats are supported? In general, LiveSYNC recognizes .mp4 video files, .jpg image files, and .m3u8 HLS streams. For custom hotspot images please use 32-bit .png with alpha. Notice that for video content the support depends on used hardware (HW decoder), .mp4 is just a container. A good rule of thumb is: if it plays with your device's own media player, it likely works with LiveSYNC, too. Where should I put my content files so that LiveSYNC can pick them up? On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start) Can I use an SD card on my Android device for storing content files? Yes. You need to manually create /Movies/LiveSYNC folder on your SD card and copy the files there. LiveSYNC will treat this folder on SD card similar to /Movies/LiveSYNC folder in internal storage, except that if same filename appears in both then internal memory takes precedence. Can I use folders to organize my content files? Yes, but only one level: \\LiveSYNC\\Summer is OK but \\LiveSYNC\\Summer\\June is not. Of course you can have as many subfolders directly under LiveSYNC root folder, for example \\LiveSYNC\\January, \\LiveSYNC\\February, etc. On iOS devices, create the folders on your PC/Mac and then drag the folder to iTunes. Notice that it is not possible to view the contents of already copied folder via iTunes - unless you drag the folder back from iTunes, for example, to your desktop. On Android devices, you can use the same method OR create the folders directly to your device and then copy there individual files. You can also view the contents of the folders via file manager.","title":"Index"},{"location":"faq/faq/#frequently-asked-questions-faq","text":"This is a collection of frequently asked questions, answered by our experts. If you cannot find an answer to your question, please contact us via the form on the support page . TIP: Try the search feature in the top right corner!","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/faq/#hardware","text":"","title":"Hardware"},{"location":"faq/faq/#what-ios-devices-are-supported","text":"LiveSYNC can be installed on iPhones and iPads, but currently not on AppleTV. The app requires iOS 8.0 or later. While LiveSYNC works fairly well also on iOS devices that are several years old, some feature limitations exist: Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your iOS device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your iPad.","title":"What iOS devices are supported?"},{"location":"faq/faq/#i-need-to-buy-an-ipad-to-be-used-as-the-director-device-which-model-do-you-recommend","text":"In general, a new standard size (9.7\") iPad is a good value for money. You can also choose Pro version, but it is not necessary unless you want a larger size model (11\" or 12.9\").","title":"I need to buy an iPad to be used as the director device. Which model do you recommend?"},{"location":"faq/faq/#what-android-devices-are-supported","text":"LiveSYNC can be installed on Android phones and tablets, but currently not on AndroidTV. The app requires Android 5.0 (API level 21) or later. While LiveSYNC works fairly well also on Android devices that are several years old, some feature limitations exist: Director mode with Bluetooth communication may not work on some old devices that have been upgraded to Android 5.0, due to Bluetooth chipset that does not support all required features. Bluetooth chipset may limit simultaneous connections (Director device) to 4 or 2 instead of 8. Please test with your device before purchasing a license (watermark will appear). Video playback on your Android device may be limited to FullHD quality instead of 4K. Notice that you can still use 4K video with your VR headsets if you convert the video to FullHD for your tablet.","title":"What Android devices are supported?"},{"location":"faq/faq/#i-need-to-buy-an-android-tablet-to-be-used-as-the-director-device-which-model-do-you-recommend","text":"Just to name one, we've had good experience with Samsung Galaxy Tab S2 or newer. However, there are plenty of options.","title":"I need to buy an Android tablet to be used as the director device. Which model do you recommend?"},{"location":"faq/faq/#installing","text":"","title":"Installing"},{"location":"faq/faq/#can-i-install-livesync-on-my-iphone-or-ipad","text":"Yes. On your iOS device, open App Store , tap Search , and type 'livesync'. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Can I install LiveSYNC on my iPhone or iPad?"},{"location":"faq/faq/#can-i-install-livesync-on-my-android-phone-or-tablet","text":"Yes. On your Android device, open Play Store , and type 'livesync' to the search field. Then select 'LiveSYNC Presentation Solution' by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Can I install LiveSYNC on my Android phone or tablet?"},{"location":"faq/faq/#can-i-install-livesync-on-my-windows-phone-or-tablet","text":"No. Windows is currently not supported.","title":"Can I install LiveSYNC on my Windows phone or tablet?"},{"location":"faq/faq/#can-i-install-livesync-on-my-windowsmaclinux-computer","text":"No. Desktop computer operating systems are currently not supported.","title":"Can I install LiveSYNC on my Windows/Mac/Linux computer?"},{"location":"faq/faq/#can-i-install-livesync-on-my-gearvroculus-go-headset","text":"Yes, but currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus device, navigate to Oculus Store and type 'livesync' to the search field. Select 'LiveSYNC Oculus Go'. If you cannot find it, don't forget to check also the 'Not installed' tab. The app can be installed for FREE.","title":"Can I install LiveSYNC on my GearVR/Oculus Go headset?"},{"location":"faq/faq/#can-i-install-livesync-on-my-oculus-rifthtc-vive-headset","text":"No. LiveSYNC is currently available for standalone VR headsets, not for devices that depend on a desktop/laptop computer.","title":"Can I install LiveSYNC on my Oculus Rift/HTC Vive headset?"},{"location":"faq/faq/#i-have-a-cardboard-or-compatible-vr-headset-can-i-use-that-with-livesync","text":"Yes. Both iOS and Android versions have built-in support for VR mode on phones (disabled in tablets). During channel configuration select 'VR' for viewing mode, or during playback long tap the screen to toggle between normal and VR mode. If you have a GearVR headset, please use our separate Oculus Go version from Oculus Store instead (invite only). The rendering quality and head tracking are far superior!","title":"I have a Cardboard (or compatible) VR headset. Can I use that with LiveSYNC?"},{"location":"faq/faq/#beta-versions","text":"","title":"Beta versions"},{"location":"faq/faq/#can-i-test-new-features-before-they-are-released-by-installing-a-beta-version","text":"Yes. Please follow the instructions below: Android: We have an open beta channel in Google Play. Anyone can join simply by opting-in here and then installing the beta version by following instructions on that page. iOS: We have a closed beta channel in Apple TestFlight. If you wish to participate, please use the contact form in the support page for requesting an invite. GearVR/Oculus Go: We have a closed beta channel in Oculus Store. If you wish to participate, please use the contact form in the support page for requesting an invite. Notice: We need your Oculus username/email for sending the invite.","title":"Can I test new features before they are released by installing a beta version?"},{"location":"faq/faq/#can-i-install-both-the-beta-version-and-the-store-version-into-the-same-device","text":"No. All the stores allow having only one version installed at a time. In general this will be the newest version ie. the one with the highest version number.","title":"Can I install BOTH the beta version and the store version into the same device?"},{"location":"faq/faq/#configuration","text":"","title":"Configuration"},{"location":"faq/faq/#can-i-create-multiple-channel-configurations-and-swap-between-them-during-my-presentation","text":"Yes, sort of. Each channel has a channel number, and only those audience devices that are listening on the same channel that you are on will respond to your commands. You can control device group 1 first on channel 1234, and then move on to control device group 2 on channel 5678, for instance. However, when you leave a presentation to switch to another one, the audience devices that you were previously controlling will lose the connection and return to the lobby. Hence, you cannot have more than one presentation ongoing simultaneously with only one controller device.","title":"Can I create multiple channel configurations, and swap between them during my presentation?"},{"location":"faq/faq/#content","text":"","title":"Content"},{"location":"faq/faq/#what-file-formats-are-supported","text":"In general, LiveSYNC recognizes .mp4 video files, .jpg image files, and .m3u8 HLS streams. For custom hotspot images please use 32-bit .png with alpha. Notice that for video content the support depends on used hardware (HW decoder), .mp4 is just a container. A good rule of thumb is: if it plays with your device's own media player, it likely works with LiveSYNC, too.","title":"What file formats are supported?"},{"location":"faq/faq/#where-should-i-put-my-content-files-so-that-livesync-can-pick-them-up","text":"On iOS devices, use iTunes application to copy your media files under LiveSYNC app. On Android/GearVR/Oculus Go devices, use file manager to copy your media files under /Movies/LiveSYNC folder (this should be generated automatically upon first start)","title":"Where should I put my content files so that LiveSYNC can pick them up?"},{"location":"faq/faq/#can-i-use-an-sd-card-on-my-android-device-for-storing-content-files","text":"Yes. You need to manually create /Movies/LiveSYNC folder on your SD card and copy the files there. LiveSYNC will treat this folder on SD card similar to /Movies/LiveSYNC folder in internal storage, except that if same filename appears in both then internal memory takes precedence.","title":"Can I use an SD card on my Android device for storing content files?"},{"location":"faq/faq/#can-i-use-folders-to-organize-my-content-files","text":"Yes, but only one level: \\LiveSYNC\\Summer is OK but \\LiveSYNC\\Summer\\June is not. Of course you can have as many subfolders directly under LiveSYNC root folder, for example \\LiveSYNC\\January, \\LiveSYNC\\February, etc. On iOS devices, create the folders on your PC/Mac and then drag the folder to iTunes. Notice that it is not possible to view the contents of already copied folder via iTunes - unless you drag the folder back from iTunes, for example, to your desktop. On Android devices, you can use the same method OR create the folders directly to your device and then copy there individual files. You can also view the contents of the folders via file manager.","title":"Can I use folders to organize my content files?"},{"location":"quick_start/presentation_channel/","text":"Quick start: Presentation Channel Follow these steps to start a presentation on the device configured to Director Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Director Mode and starts from the Mosaic tab Wait here until audience devices have joined (their views will appear on your screen) Note In case some viewer devices arrive late (during a presentation) they can still join the presentation.","title":"Channel"},{"location":"quick_start/presentation_channel/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_channel/#channel","text":"Follow these steps to start a presentation on the device configured to Director Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Director Mode and starts from the Mosaic tab Wait here until audience devices have joined (their views will appear on your screen) Note In case some viewer devices arrive late (during a presentation) they can still join the presentation.","title":"Channel"},{"location":"quick_start/presentation_configuration/","text":"Quick start: Presentation Configuration Director Mode To configure a device for the Director Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Director) Type device name or use the offered default name Select connection type Write down the given channel number Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Audience Mode To configure a device for the Audience Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Audience) Type device name or use the offered default name Select connection type Type the channel number (the one you wrote down earlier) If you are setting a phone, select normal or VR mode (not asked on tablets) Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Note On GearVR/Oculus Go Director Mode is not available and Audience Mode configuration is simpler.","title":"Configuration"},{"location":"quick_start/presentation_configuration/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_configuration/#configuration","text":"","title":"Configuration"},{"location":"quick_start/presentation_configuration/#director-mode","text":"To configure a device for the Director Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Director) Type device name or use the offered default name Select connection type Write down the given channel number Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen.","title":"Director Mode"},{"location":"quick_start/presentation_configuration/#audience-mode","text":"To configure a device for the Audience Mode follow these steps: Start the application from the device's application menu Once started, click the big round button with a '+' sign inside it Select device role (Audience) Type device name or use the offered default name Select connection type Type the channel number (the one you wrote down earlier) If you are setting a phone, select normal or VR mode (not asked on tablets) Pass the reminder about copying content Your channel configuration has been created and is visible on the app's Channels screen. Note On GearVR/Oculus Go Director Mode is not available and Audience Mode configuration is simpler.","title":"Audience Mode"},{"location":"quick_start/presentation_devices/","text":"Quick start: Presentation Devices In a LiveSYNC-enabled presentation, the person giving the presentation has a control device and the audience members each have a viewer device . Hence, to give a presentation using the LiveSYNC tool you will need: 1 device for controlling the presentation (LiveSYNC tool in Director Mode ) 1-n devices for viewing the presentation (LiveSYNC tool in Audience Mode ) Director Mode The Director Mode is currently available for iOS and Android . Thus, you can use either a phone or a tablet for controlling a presentation. Tip We highly recommend using a tablet for the Director Mode because of its larger screen. Audience Mode The Audience Mode is currently available for iOS , Android , GearVR , and Oculus Go . Thus, you can view a presentation either from a phone screen, a tablet screen, or via a phone based or standalone VR headset. Note VR mode is available also for iOS and Android phones. A Google Cardboard or a compatible VR headset can be used, but user experience is much superior with a high quality headset and dedicated app version (GearVR, Oculus Go). Networking Communication between the control device and the viewer devices is wireless . You can choose from available options during channel configuration . Make sure that the necessary radio technology is enabled on all devices. Depending on your selection this can be for example Bluetooth, Wifi or mobile data. Power During a presentation the devices will consume a lot of power. Power consumption is related to active radios (communication), photo and video decoding, continuous screen updates, display brightness, and disabled power saving (sleep). We recommend that you recharge the batteries of all devices before a presentation . If you use VR headsets with hand remotes, check the batteries inside the remotes. If you plan to run a presentation repeatedly, you probably need spare devices that are being recharged while others are in use. Big screen In addition to personal viewing devices you can use a big screen. This can be for example a large TV screen or a projector. We recommend a wired connection between the control device and the big screen. For example, an iPad can be connected to a TV using an HDMI cable via a separately sold adapter. Speakers or headphones Many presentations use sound. You must decide whether you want to use room audio (audience devices muted) or viewer device audio. In the latter case, headphones are highly recommended. Chairs If your presentation takes more than a few minutes, the audience probably wants to sit. In order to allow comfortable 360-degree photo/video exploration, swivel chairs with enough spacing between them are recommended.","title":"Devices"},{"location":"quick_start/presentation_devices/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_devices/#devices","text":"In a LiveSYNC-enabled presentation, the person giving the presentation has a control device and the audience members each have a viewer device . Hence, to give a presentation using the LiveSYNC tool you will need: 1 device for controlling the presentation (LiveSYNC tool in Director Mode ) 1-n devices for viewing the presentation (LiveSYNC tool in Audience Mode )","title":"Devices"},{"location":"quick_start/presentation_devices/#director-mode","text":"The Director Mode is currently available for iOS and Android . Thus, you can use either a phone or a tablet for controlling a presentation. Tip We highly recommend using a tablet for the Director Mode because of its larger screen.","title":"Director Mode"},{"location":"quick_start/presentation_devices/#audience-mode","text":"The Audience Mode is currently available for iOS , Android , GearVR , and Oculus Go . Thus, you can view a presentation either from a phone screen, a tablet screen, or via a phone based or standalone VR headset. Note VR mode is available also for iOS and Android phones. A Google Cardboard or a compatible VR headset can be used, but user experience is much superior with a high quality headset and dedicated app version (GearVR, Oculus Go).","title":"Audience Mode"},{"location":"quick_start/presentation_devices/#networking","text":"Communication between the control device and the viewer devices is wireless . You can choose from available options during channel configuration . Make sure that the necessary radio technology is enabled on all devices. Depending on your selection this can be for example Bluetooth, Wifi or mobile data.","title":"Networking"},{"location":"quick_start/presentation_devices/#power","text":"During a presentation the devices will consume a lot of power. Power consumption is related to active radios (communication), photo and video decoding, continuous screen updates, display brightness, and disabled power saving (sleep). We recommend that you recharge the batteries of all devices before a presentation . If you use VR headsets with hand remotes, check the batteries inside the remotes. If you plan to run a presentation repeatedly, you probably need spare devices that are being recharged while others are in use.","title":"Power"},{"location":"quick_start/presentation_devices/#big-screen","text":"In addition to personal viewing devices you can use a big screen. This can be for example a large TV screen or a projector. We recommend a wired connection between the control device and the big screen. For example, an iPad can be connected to a TV using an HDMI cable via a separately sold adapter.","title":"Big screen"},{"location":"quick_start/presentation_devices/#speakers-or-headphones","text":"Many presentations use sound. You must decide whether you want to use room audio (audience devices muted) or viewer device audio. In the latter case, headphones are highly recommended.","title":"Speakers or headphones"},{"location":"quick_start/presentation_devices/#chairs","text":"If your presentation takes more than a few minutes, the audience probably wants to sit. In order to allow comfortable 360-degree photo/video exploration, swivel chairs with enough spacing between them are recommended.","title":"Chairs"},{"location":"quick_start/presentation_files/","text":"Quick start: Presentation Files LiveSYNC does not stream the content from the control device to audience devices. Only playback commands are sent and received. Thus, each audience device must have its own copy of the files that are used in a presentation. This ensures scalability and good user experience. Presentation files typically consist of a set of 360-degree photos (.jpg), 360-degree videos (.mp4), and custom hotspots (.png). Also ordinary 2D photos and videos can be used. Note To prepare the devices for a presentation you must copy the necessary files to all devices in advance. iOS (iPhones & iPads) Copy the presentation files using Apple's iTunes application. Install iTunes application to your PC or Mac computer Start iTunes application Connect your iOS device to your computer using a Lightning to USB cable Once the device is detected in iTunes, click the phone/tablet icon Click File Sharing from left side menu Select LiveSYNC from the apps list Drag'n drop files and folders to the app's file area Android (phones & tablets) Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC Oculus (GearVR & Oculus Go) Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Files"},{"location":"quick_start/presentation_files/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_files/#files","text":"LiveSYNC does not stream the content from the control device to audience devices. Only playback commands are sent and received. Thus, each audience device must have its own copy of the files that are used in a presentation. This ensures scalability and good user experience. Presentation files typically consist of a set of 360-degree photos (.jpg), 360-degree videos (.mp4), and custom hotspots (.png). Also ordinary 2D photos and videos can be used. Note To prepare the devices for a presentation you must copy the necessary files to all devices in advance.","title":"Files"},{"location":"quick_start/presentation_files/#ios-iphones-ipads","text":"Copy the presentation files using Apple's iTunes application. Install iTunes application to your PC or Mac computer Start iTunes application Connect your iOS device to your computer using a Lightning to USB cable Once the device is detected in iTunes, click the phone/tablet icon Click File Sharing from left side menu Select LiveSYNC from the apps list Drag'n drop files and folders to the app's file area","title":"iOS (iPhones &amp; iPads)"},{"location":"quick_start/presentation_files/#android-phones-tablets","text":"Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Android (phones &amp; tablets)"},{"location":"quick_start/presentation_files/#oculus-gearvr-oculus-go","text":"Copy the presentation files using Windows Explorer (PC) or Android File Transfer application. Connect your Android device to your computer using a USB cable Once the device is detected it will appear in Explorer / Android File Transfer Find /Movies/LiveSYNC folder (auto generated during first run) Drag'n drop files and folders under /Movies/LiveSYNC","title":"Oculus (GearVR &amp; Oculus Go)"},{"location":"quick_start/presentation_joining/","text":"Quick start: Presentation Joining Follow these steps to join a presentation on a device configured to Audience Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Audience Mode and starts from the Lobby Wait here until connection to the control device has been established Once connected, you are ready and the presentation begins when the presenter decides to do so","title":"Joining"},{"location":"quick_start/presentation_joining/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_joining/#joining","text":"Follow these steps to join a presentation on a device configured to Audience Mode: Start the application from the device's application menu Once started, click the big round button with a channel number you have configured earlier The application switches into the Audience Mode and starts from the Lobby Wait here until connection to the control device has been established Once connected, you are ready and the presentation begins when the presenter decides to do so","title":"Joining"},{"location":"quick_start/presentation_mirroring/","text":"Quick start: Presentation Mirroring To mirror content from your control device to a big screen (a secondary display), follow the steps below. iOS Plug your Digital AV or VGA adapter into the charging port at the bottom of your iOS device. Connect an HDMI or VGA cable to your adapter. Notice that the adapter has also a charging port; you can charge your iPad during mirroring. Connect the other end of your HDMI or VGA cable to your secondary display (TV, monitor, or projector). Turn on your secondary display. If necessary, switch to the correct video source on your secondary display. If you need help, use your display's manual. Now the screen on your iPad should appear on your TV, display, or projector. Tip It is also possible to connect wirelessly using AirPlay if you connect an Apple TV to your secondary display. Modes LiveSYNC provides two screen mirroring modes: Presentation : shows only the Presentation area on the secondary display Mirroring : show the complete app on the secondary display Typically, Presentation mode is the correct one. However, if you want to show how you use the app (training) or for example share the Mosaic screen, then switch to Mirroring mode. There is also a third option Show touches . This visualizes touches on your control device's screen, so that audience watching the secondary display can better understand your actions. What's next You have barely scratched the surface of what you can do with the LiveSYNC tool. Learn more by reading some of the tutorials or dive into the User Guide .","title":"Mirroring"},{"location":"quick_start/presentation_mirroring/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_mirroring/#mirroring","text":"To mirror content from your control device to a big screen (a secondary display), follow the steps below.","title":"Mirroring"},{"location":"quick_start/presentation_mirroring/#ios","text":"Plug your Digital AV or VGA adapter into the charging port at the bottom of your iOS device. Connect an HDMI or VGA cable to your adapter. Notice that the adapter has also a charging port; you can charge your iPad during mirroring. Connect the other end of your HDMI or VGA cable to your secondary display (TV, monitor, or projector). Turn on your secondary display. If necessary, switch to the correct video source on your secondary display. If you need help, use your display's manual. Now the screen on your iPad should appear on your TV, display, or projector. Tip It is also possible to connect wirelessly using AirPlay if you connect an Apple TV to your secondary display.","title":"iOS"},{"location":"quick_start/presentation_mirroring/#modes","text":"LiveSYNC provides two screen mirroring modes: Presentation : shows only the Presentation area on the secondary display Mirroring : show the complete app on the secondary display Typically, Presentation mode is the correct one. However, if you want to show how you use the app (training) or for example share the Mosaic screen, then switch to Mirroring mode. There is also a third option Show touches . This visualizes touches on your control device's screen, so that audience watching the secondary display can better understand your actions.","title":"Modes"},{"location":"quick_start/presentation_mirroring/#whats-next","text":"You have barely scratched the surface of what you can do with the LiveSYNC tool. Learn more by reading some of the tutorials or dive into the User Guide .","title":"What's next"},{"location":"quick_start/presentation_presenting/","text":"Quick start: Presentation Presenting When viewer devices have connected, select Player tab from the bottom bar. To start the presentation, drag an item from the Content tab and drop it to the Presentation area at the center of the screen. The status of each viewer device appears in the Devices tab. If you want to draw the attention of the audience to something, drag an item from the Tags tab and drop it to a position where you want it to appear over the content being presented. To change content, drag another item from the Content tab and drop it to the Presentation area . Once your presentation is over, tap the Home button in the top left corner to exit presentation. Connections to viewer devices will be disconnected and they will return to the Lobby. Note If you haven't purchased a license, watermarks will be shown when you are presenting your own content files. This concerns both the control and the viewer devices. Only the control device requires a license. Watermarks will not appear on viewer devices when they are connected to a control device that has a valid license.","title":"Presenting"},{"location":"quick_start/presentation_presenting/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_presenting/#presenting","text":"When viewer devices have connected, select Player tab from the bottom bar. To start the presentation, drag an item from the Content tab and drop it to the Presentation area at the center of the screen. The status of each viewer device appears in the Devices tab. If you want to draw the attention of the audience to something, drag an item from the Tags tab and drop it to a position where you want it to appear over the content being presented. To change content, drag another item from the Content tab and drop it to the Presentation area . Once your presentation is over, tap the Home button in the top left corner to exit presentation. Connections to viewer devices will be disconnected and they will return to the Lobby. Note If you haven't purchased a license, watermarks will be shown when you are presenting your own content files. This concerns both the control and the viewer devices. Only the control device requires a license. Watermarks will not appear on viewer devices when they are connected to a control device that has a valid license.","title":"Presenting"},{"location":"quick_start/presentation_software/","text":"Quick start: Presentation Software The same application is used both in the control device (Director Mode) and viewer devices (Audience Mode). iOS (iPhones & iPads) On your iOS device, open App Store , tap Search , and type livesync . Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Android (phones and tablets) On your Android device, open Play Store , and type livesync to the search field. Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app. Oculus (GearVR & Oculus Go) LiveSYNC has not officially launched on these platforms yet. However, you can already start using it. Currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus Go device, navigate to Oculus Store and type livesync to the search field. Select LiveSYNC Oculus Go . If you cannot find it, don't forget to check also the Not installed tab. The app can be installed for FREE.","title":"Software"},{"location":"quick_start/presentation_software/#quick-start-presentation","text":"","title":"Quick start: Presentation"},{"location":"quick_start/presentation_software/#software","text":"The same application is used both in the control device (Director Mode) and viewer devices (Audience Mode).","title":"Software"},{"location":"quick_start/presentation_software/#ios-iphones-ipads","text":"On your iOS device, open App Store , tap Search , and type livesync . Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"iOS (iPhones &amp; iPads)"},{"location":"quick_start/presentation_software/#android-phones-and-tablets","text":"On your Android device, open Play Store , and type livesync to the search field. Then select LiveSYNC Presentation Solution by Finwe Ltd. The app can be installed for FREE. You can also use this direct link to find the app.","title":"Android (phones and tablets)"},{"location":"quick_start/presentation_software/#oculus-gearvr-oculus-go","text":"LiveSYNC has not officially launched on these platforms yet. However, you can already start using it. Currently you will need an invite to our Beta channel in the Oculus Store. Please use the contact form in the support page for requesting an invite. When you get an invite email from Oculus, click the link in the email to accept the invitation. Then on your GearVR/Oculus Go device, navigate to Oculus Store and type livesync to the search field. Select LiveSYNC Oculus Go . If you cannot find it, don't forget to check also the Not installed tab. The app can be installed for FREE.","title":"Oculus (GearVR &amp; Oculus Go)"},{"location":"quick_start/quick_start/","text":"Quick Start Presentation This is a quick start guide for giving a presentation with the LiveSYNC tool.","title":"Index"},{"location":"quick_start/quick_start/#quick-start","text":"","title":"Quick Start"},{"location":"quick_start/quick_start/#presentation","text":"This is a quick start guide for giving a presentation with the LiveSYNC tool.","title":"Presentation"},{"location":"site_howto/site_howto/","text":"How to Use This Site Accessing Web browser This site is publicly available in HTML format in this URL: https://docs.livesync.app You can use any modern web browser for accessing the site. Simply type the site's URL to the browser's address bar or click the link above. Your computing platform probably has a built-in web browser. If not, there are multiple supported options for each platform. These are listed below. Note This site does not require any specific browser application, any specific version, or any third party plugins. Windows PC Use the built-in Internet Explorer or Microsoft Edge browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome Mac Use the built-in Safari browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome Linux Use the browser included to your distribution (varies). Or, install a 3rd party application: Mozilla Firefox Google Chrome iOS (iPhone & iPad) Use the built-in Safari browser. Or, install a 3rd party application from Apple Appstore : Mozilla Firefox Google Chrome Android (phones & tablets) Use the built-in browser (varies, look for Internet app from the menu). Or, install a 3rd party application from Google Play : Mozilla Firefox Google Chrome GearVR Use the built-in Oculus Browser or Samsung Internet app. If you don't have them installed search from the Oculus Store . There are two ways to start them: a) Starting a browser from the Oculus GearVR app (outside VR): Remove the phone from the GearVR headset. In the Android app menu, find Oculus app (GearVR text in the icon), and start it. Once started, navigate to Library tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Put the phone into the GearVR headset when a dialog suggests that. Note If an application does not start when you tap it, you probably need to update your GearVR software. Put the phone into the GearVR headset. Update the software if a dialog suggests that (you have to remove the phone from the headset to do that). b) Starting a browser from the Oculus Home (inside VR): Put the phone into the GearVR headset. In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Tip You can also start the default browser by selecting Internet from the Navigate tab in the bottom bar. Oculus Go Use the built-in Oculus Browser . Or, install a 3rd party application: Firefox Reality Starting a browser from the Oculus Home (inside VR): In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Firefox Reality from the app grid, and start it. Tip You can also start the default browser by selecting Browser from the Navigate tab in the bottom bar. LiveSYNC App In the future, this site will be integrated into the LiveSYNC app. The network connection will be still required for reading the documentation. Once the update has been released the instructions will be updated here. Currently, you can use a web browser for accessing this site as explained above. Navigating Site menu Different areas of the LiveSYNC Learning Center are accessible via the site menu . The site menu appears in one of two different ways: Tabs & menu If your browser window is wide, different areas appear as tabs in the top bar. Simply click any of the tabs to navigate to another area of the site. When you scroll down, tabs disappear to save space. Scroll back up to reveal them again. The pages of an active area appear in a menu at the left side of the page. Click a title to navigate to another page. This option is typical for desktop and laptop computers. On wide screens, different site areas appear as tabs in the top bar. Pages that belong to an area are listed on the left. Collapsed menu If your browser window is narrow, the menu appears as a collapsed hamburger menu in the top bar . Click the hamburger icon to temporarily expand the site menu. Different areas of the site appear as submenus, which you can open by clicking them. To navigate to a page, click its title. Once selected, the menu will collapse again. This option is typical for phones, tablets, and VR headsets. On narrow screens, the site menu appears collapsed. It can be opened by clicking the hamburger menu icon. Clicking a submenu reveals pages that belong to that area. Clicking a page title opens that page. Table of contents Each page on this site has an automatically generated table-of-contents (TOC). You can navigate within a page by clicking the titles in the page's TOC. The TOC will appear on the right side of the page, but only if the browser window is wide enough. If not, it will be integrated to the site menu. On wide screens, table-of-contents appears on the right. On narrow screens, table-of-contents appears integrated to the site menu. Navigation arrows Some pages have one or two navigation arrows that appear at the bottom of the page. Some pages do not have them. Use the navigation arrows for moving to the next or to the previous page when you have read the current page. On some pages you can move forward or backward by clicking the navigation arrows. Search The top bar contains the search field . The search field may appear in one of two different ways: Expanded search field If your browser window is wide, the search bar appears permanently expanded on the right side of the top bar. Simply click the text field and begin to type. The results will appear in a floating window as you type. This option is typical for desktop and laptop computers, as well as tablets. On wide screens, the search field is expanded and results appear floating over the page. Collapsed search field If your browser window is narrow, the search bar appears as a collapsed widget on the right side of the top bar. You can recognize it from the magnifier glass icon. Simply click the icon and begin to type. The results will appear in full window mode as you type. This option is typical for phones and VR headsets. On narrow screens, the search field is collapsed and results appear in full window mode. Reading Languages The documentation is currently available only in English. Translating the documentation to other selected languages will be considered if there is enough customer demand. You can contact us via our contact form . Product variants Different customer segments need different features. For example, most 360-degree video professionals are interested in using LiveSYNC's presentation capabilities. On the other hand, industrial users focus on tagging and reporting. Thus, the LiveSYNC app is offered in multiple variants. The variants have a common core, but available features vary. The purchased license defines what features are available in a particular app installation. The documentation site is common to all versions. This means that parts of the documentation discuss features that are available only for a subset of customers. To differentiate such parts, they are clearly marked with banners . For example, users who do not have the Enterprise version of the app should skip sections that are marked with this banner: An example of a product variant specific documentation. The content below this banner, up to the next section/title, is relevant only for users of the Enterprise version of the LiveSYNC app. Product versions The LiveSYNC solution is continuously under development and updates are released frequently. We recommend that you keep the app up-to-date. There is one exception to this rule. Do you have an important presentation coming up soon and everything is already set up and working smoothly? Then we recommend that you do not update the app before the presentation. The documentation site contains documentation for the latest version of the LiveSYNC app. Conventions The goal when writing the documentation was to make it as readable as possible. Hence, we use many conventions throughout the text. We also wanted to provide the facility for readers to learn at their own pace. There are plenty of cross-references (links). Use them for learning more about a specific topic. Regular text appears like this. Important terms and text that appears in a user interface are written in italics . Product names are often written in black . Links appear like this . Numbered references appear inside parentheses (1.1) . Mathematical operations, configuration file contents, and software code appear in code blocks. Notice that you can easily copy its contents to the clipboard. Simply click the copy icon at the right edge of the block. 1 + 1 = 2 Quotations are indented: Life is short. Data is often presented as a table: Hours of sleep Laziness 5 Severe 7 Mild 8 None Lists can be ordered (numbered): Skip the manual Try to use the product Read the manual Lists can also be unordered (bulleted): First things first Second things second Sometimes, a horizontal rule is used for separating things: Throughout the documentation, you'll find various types of notes complementing the regular text: Note A note is designed to provide an important piece of information. Tip A tip is something that will help when you need to perform the task being described. Alternatively, it can be something can make your life easier when using the LiveSYNC tool. Example An example applies theory to practice. It will help your understanding of the topic being discussed. Question A question is a short practice task for the reader. Summary A summary is a collection of the key points, typically at the end of a section. Caution A caution is something you should certainly pay attention to. It warns of a hidden danger or caveat. Danger A danger is a something that can potentially harm you. You probably should not do what is being discussed.","title":"How to use this site"},{"location":"site_howto/site_howto/#how-to-use-this-site","text":"","title":"How to Use This Site"},{"location":"site_howto/site_howto/#accessing","text":"","title":"Accessing"},{"location":"site_howto/site_howto/#web-browser","text":"This site is publicly available in HTML format in this URL: https://docs.livesync.app You can use any modern web browser for accessing the site. Simply type the site's URL to the browser's address bar or click the link above. Your computing platform probably has a built-in web browser. If not, there are multiple supported options for each platform. These are listed below. Note This site does not require any specific browser application, any specific version, or any third party plugins.","title":"Web browser"},{"location":"site_howto/site_howto/#windows-pc","text":"Use the built-in Internet Explorer or Microsoft Edge browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Windows PC"},{"location":"site_howto/site_howto/#mac","text":"Use the built-in Safari browser. Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Mac"},{"location":"site_howto/site_howto/#linux","text":"Use the browser included to your distribution (varies). Or, install a 3rd party application: Mozilla Firefox Google Chrome","title":"Linux"},{"location":"site_howto/site_howto/#ios-iphone-ipad","text":"Use the built-in Safari browser. Or, install a 3rd party application from Apple Appstore : Mozilla Firefox Google Chrome","title":"iOS (iPhone &amp; iPad)"},{"location":"site_howto/site_howto/#android-phones-tablets","text":"Use the built-in browser (varies, look for Internet app from the menu). Or, install a 3rd party application from Google Play : Mozilla Firefox Google Chrome","title":"Android (phones &amp; tablets)"},{"location":"site_howto/site_howto/#gearvr","text":"Use the built-in Oculus Browser or Samsung Internet app. If you don't have them installed search from the Oculus Store . There are two ways to start them: a) Starting a browser from the Oculus GearVR app (outside VR): Remove the phone from the GearVR headset. In the Android app menu, find Oculus app (GearVR text in the icon), and start it. Once started, navigate to Library tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Put the phone into the GearVR headset when a dialog suggests that. Note If an application does not start when you tap it, you probably need to update your GearVR software. Put the phone into the GearVR headset. Update the software if a dialog suggests that (you have to remove the phone from the headset to do that). b) Starting a browser from the Oculus Home (inside VR): Put the phone into the GearVR headset. In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Samsung Internet from the app grid, and start it. Tip You can also start the default browser by selecting Internet from the Navigate tab in the bottom bar.","title":"GearVR"},{"location":"site_howto/site_howto/#oculus-go","text":"Use the built-in Oculus Browser . Or, install a 3rd party application: Firefox Reality Starting a browser from the Oculus Home (inside VR): In Oculus Home, select Library from the Navigate tab in the bottom bar. Find Oculus Browser or Firefox Reality from the app grid, and start it. Tip You can also start the default browser by selecting Browser from the Navigate tab in the bottom bar.","title":"Oculus Go"},{"location":"site_howto/site_howto/#livesync-app","text":"In the future, this site will be integrated into the LiveSYNC app. The network connection will be still required for reading the documentation. Once the update has been released the instructions will be updated here. Currently, you can use a web browser for accessing this site as explained above.","title":"LiveSYNC App"},{"location":"site_howto/site_howto/#navigating","text":"","title":"Navigating"},{"location":"site_howto/site_howto/#site-menu","text":"Different areas of the LiveSYNC Learning Center are accessible via the site menu . The site menu appears in one of two different ways:","title":"Site menu"},{"location":"site_howto/site_howto/#tabs-menu","text":"If your browser window is wide, different areas appear as tabs in the top bar. Simply click any of the tabs to navigate to another area of the site. When you scroll down, tabs disappear to save space. Scroll back up to reveal them again. The pages of an active area appear in a menu at the left side of the page. Click a title to navigate to another page. This option is typical for desktop and laptop computers. On wide screens, different site areas appear as tabs in the top bar. Pages that belong to an area are listed on the left.","title":"Tabs &amp; menu"},{"location":"site_howto/site_howto/#collapsed-menu","text":"If your browser window is narrow, the menu appears as a collapsed hamburger menu in the top bar . Click the hamburger icon to temporarily expand the site menu. Different areas of the site appear as submenus, which you can open by clicking them. To navigate to a page, click its title. Once selected, the menu will collapse again. This option is typical for phones, tablets, and VR headsets. On narrow screens, the site menu appears collapsed. It can be opened by clicking the hamburger menu icon. Clicking a submenu reveals pages that belong to that area. Clicking a page title opens that page.","title":"Collapsed menu"},{"location":"site_howto/site_howto/#table-of-contents","text":"Each page on this site has an automatically generated table-of-contents (TOC). You can navigate within a page by clicking the titles in the page's TOC. The TOC will appear on the right side of the page, but only if the browser window is wide enough. If not, it will be integrated to the site menu. On wide screens, table-of-contents appears on the right. On narrow screens, table-of-contents appears integrated to the site menu.","title":"Table of contents"},{"location":"site_howto/site_howto/#navigation-arrows","text":"Some pages have one or two navigation arrows that appear at the bottom of the page. Some pages do not have them. Use the navigation arrows for moving to the next or to the previous page when you have read the current page. On some pages you can move forward or backward by clicking the navigation arrows.","title":"Navigation arrows"},{"location":"site_howto/site_howto/#search","text":"The top bar contains the search field . The search field may appear in one of two different ways:","title":"Search"},{"location":"site_howto/site_howto/#expanded-search-field","text":"If your browser window is wide, the search bar appears permanently expanded on the right side of the top bar. Simply click the text field and begin to type. The results will appear in a floating window as you type. This option is typical for desktop and laptop computers, as well as tablets. On wide screens, the search field is expanded and results appear floating over the page.","title":"Expanded search field"},{"location":"site_howto/site_howto/#collapsed-search-field","text":"If your browser window is narrow, the search bar appears as a collapsed widget on the right side of the top bar. You can recognize it from the magnifier glass icon. Simply click the icon and begin to type. The results will appear in full window mode as you type. This option is typical for phones and VR headsets. On narrow screens, the search field is collapsed and results appear in full window mode.","title":"Collapsed search field"},{"location":"site_howto/site_howto/#reading","text":"","title":"Reading"},{"location":"site_howto/site_howto/#languages","text":"The documentation is currently available only in English. Translating the documentation to other selected languages will be considered if there is enough customer demand. You can contact us via our contact form .","title":"Languages"},{"location":"site_howto/site_howto/#product-variants","text":"Different customer segments need different features. For example, most 360-degree video professionals are interested in using LiveSYNC's presentation capabilities. On the other hand, industrial users focus on tagging and reporting. Thus, the LiveSYNC app is offered in multiple variants. The variants have a common core, but available features vary. The purchased license defines what features are available in a particular app installation. The documentation site is common to all versions. This means that parts of the documentation discuss features that are available only for a subset of customers. To differentiate such parts, they are clearly marked with banners . For example, users who do not have the Enterprise version of the app should skip sections that are marked with this banner: An example of a product variant specific documentation. The content below this banner, up to the next section/title, is relevant only for users of the Enterprise version of the LiveSYNC app.","title":"Product variants"},{"location":"site_howto/site_howto/#product-versions","text":"The LiveSYNC solution is continuously under development and updates are released frequently. We recommend that you keep the app up-to-date. There is one exception to this rule. Do you have an important presentation coming up soon and everything is already set up and working smoothly? Then we recommend that you do not update the app before the presentation. The documentation site contains documentation for the latest version of the LiveSYNC app.","title":"Product versions"},{"location":"site_howto/site_howto/#conventions","text":"The goal when writing the documentation was to make it as readable as possible. Hence, we use many conventions throughout the text. We also wanted to provide the facility for readers to learn at their own pace. There are plenty of cross-references (links). Use them for learning more about a specific topic. Regular text appears like this. Important terms and text that appears in a user interface are written in italics . Product names are often written in black . Links appear like this . Numbered references appear inside parentheses (1.1) . Mathematical operations, configuration file contents, and software code appear in code blocks. Notice that you can easily copy its contents to the clipboard. Simply click the copy icon at the right edge of the block. 1 + 1 = 2 Quotations are indented: Life is short. Data is often presented as a table: Hours of sleep Laziness 5 Severe 7 Mild 8 None Lists can be ordered (numbered): Skip the manual Try to use the product Read the manual Lists can also be unordered (bulleted): First things first Second things second Sometimes, a horizontal rule is used for separating things: Throughout the documentation, you'll find various types of notes complementing the regular text: Note A note is designed to provide an important piece of information. Tip A tip is something that will help when you need to perform the task being described. Alternatively, it can be something can make your life easier when using the LiveSYNC tool. Example An example applies theory to practice. It will help your understanding of the topic being discussed. Question A question is a short practice task for the reader. Summary A summary is a collection of the key points, typically at the end of a section. Caution A caution is something you should certainly pay attention to. It warns of a hidden danger or caveat. Danger A danger is a something that can potentially harm you. You probably should not do what is being discussed.","title":"Conventions"},{"location":"support/support/","text":"Support Got a question that is not answered in the documentation? A feature request? Or maybe you found an error in the docs or wish to suggest a topic for an article? Feel free to contact us by leaving a message! * { box-sizing: border-box; } input[class=contact], textarea { width: 100%; padding: 12px; border: 1px solid #ccc; margin-top: 6px; margin-bottom: 16px; resize: vertical; } input[type=submit] { background-color: #4CAF50; color: white; padding: 12px 20px; margin-top: 10px; border: none; cursor: pointer; } input[type=submit]:hover { background-color: #45a049; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 10px; } Contact Us Name Company Email Message","title":"Index"},{"location":"support/support/#support","text":"Got a question that is not answered in the documentation? A feature request? Or maybe you found an error in the docs or wish to suggest a topic for an article? Feel free to contact us by leaving a message! * { box-sizing: border-box; } input[class=contact], textarea { width: 100%; padding: 12px; border: 1px solid #ccc; margin-top: 6px; margin-bottom: 16px; resize: vertical; } input[type=submit] { background-color: #4CAF50; color: white; padding: 12px 20px; margin-top: 10px; border: none; cursor: pointer; } input[type=submit]:hover { background-color: #45a049; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 10px; }","title":"Support"},{"location":"tutorials/oculus_go_device/","text":"LiveSYNC on Oculus Go Oculus Go standalone VR headset. The Device What is it? Oculus Go is an affordable standalone VR headset from Oculus , a company owned by Facebook . Oculus was founded in 2012 and allegedly started the current VR boom. Oculus released their first headset Oculus DK1 for developers back in 2013. Facebook famously acquired the company in 2014 for US$2.3 billion. Oculus is considered one of the leading VR companies. Oculus Go was released 5 years after DK1 in may 2018. It is mainly targeted for 360-degree media and simple VR games. The headset has many similarities to GearVR , a product made by Samsung . In fact, GearVR uses technology and software from Oculus. It is also marketed under Powered by Oculus term. GearVR was released already in November 2015, but it is still very similar to the original version despite of yearly updates. The main difference between GearVR and Oculus Go? Oculus Go is a new standalone device whereas GearVR consists of a combination of a VR frame and a compatible Samsung phone. In both cases applications are installed from Oculus Store and same app builds can work on both products. The user experience, image quality and interaction mechanism are almost the same. Both products are 3DOF devices (three degrees-of-freedom). This means that user can look around in VR by turning his head, but not walk in VR by physically taking steps. In September 2018, Oculus announced a new headset Oculus Quest . It will be a 6DOF device and thus able to track walking. Yet, it is more expensive, mainly targeted for games, and brings little benefit for 360-degree media users. Thus, Oculus Go is not going to become outdated anytime soon. Compared to PC or game console based VR, mobile headsets have less rendering power. From 360-degree media point-of-view this is not very significant. Mobile headsets also have a couple of huge benefits: no computer, no wires. You can go anywhere you like, take the headset out of your bag, put it on your face, and start using it right away. Usage is simple with the bundled hand remote. It works as a pointer when selecting objects from the screen. You can also swipe the touchpad with your thumb to scroll. The touchpad can be clicked but there is also a selection button for your index finger. For navigation, the remote offers a separate Back button and an Oculus button. The latter is used for calibrating the hand remote (long press) or returning to Oculus Home (short press). The headset itself has only power and volume up/down buttons. Built-in speakers are cleverly hidden inside the head strap. They provide stereo or spatial audio without a need to plug in external headphones. The head strap is flexible and easy to adjust. The headset has a detachable soft face mask, which makes it fairly comfortable to wear. Oculus Go is quite heavy, though. Normal size eyeglasses fit inside, but you can also order prescription lens inserts. All in all, Oculus Go is a great VR headset especially for 360-degree media. Should I get an Oculus Go or a GearVR? If you already own a compatible Samsung phone, purchasing only the VR frame is an affordable choice (about $110). GearVR headset is a lightweight addition to your bag, which you will probably value if you travel a lot and carry a large-screen phone anyway. However, in most cases Oculus Go is a better choice. Since it is a standalone device it has a battery of its own and you can dedicate the device for its sole purpose: VR. You will also save a fortune if you need a large number of headsets. To learn more about their differences, read comparison articles: Android Central: Oculus Go vs Samsung Gear VR How-To Geek: Gear VR vs Oculus Go: Which One is Better? ThreeSixtyCameras.com: Gear VR vs Oculus Go: Which is the better VR experience? How do I get one? You can purchase an Oculus Go directly from Oculus , from various online stores such as Amazon , or from local computer and electronics stores. A 32 GB model costs $199 (\u20ac219) and a 64 GB model $249 (\u20ac269).","title":"Device"},{"location":"tutorials/oculus_go_device/#livesync-on-oculus-go","text":"Oculus Go standalone VR headset.","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_device/#the-device","text":"","title":"The Device"},{"location":"tutorials/oculus_go_device/#what-is-it","text":"Oculus Go is an affordable standalone VR headset from Oculus , a company owned by Facebook . Oculus was founded in 2012 and allegedly started the current VR boom. Oculus released their first headset Oculus DK1 for developers back in 2013. Facebook famously acquired the company in 2014 for US$2.3 billion. Oculus is considered one of the leading VR companies. Oculus Go was released 5 years after DK1 in may 2018. It is mainly targeted for 360-degree media and simple VR games. The headset has many similarities to GearVR , a product made by Samsung . In fact, GearVR uses technology and software from Oculus. It is also marketed under Powered by Oculus term. GearVR was released already in November 2015, but it is still very similar to the original version despite of yearly updates. The main difference between GearVR and Oculus Go? Oculus Go is a new standalone device whereas GearVR consists of a combination of a VR frame and a compatible Samsung phone. In both cases applications are installed from Oculus Store and same app builds can work on both products. The user experience, image quality and interaction mechanism are almost the same. Both products are 3DOF devices (three degrees-of-freedom). This means that user can look around in VR by turning his head, but not walk in VR by physically taking steps. In September 2018, Oculus announced a new headset Oculus Quest . It will be a 6DOF device and thus able to track walking. Yet, it is more expensive, mainly targeted for games, and brings little benefit for 360-degree media users. Thus, Oculus Go is not going to become outdated anytime soon. Compared to PC or game console based VR, mobile headsets have less rendering power. From 360-degree media point-of-view this is not very significant. Mobile headsets also have a couple of huge benefits: no computer, no wires. You can go anywhere you like, take the headset out of your bag, put it on your face, and start using it right away. Usage is simple with the bundled hand remote. It works as a pointer when selecting objects from the screen. You can also swipe the touchpad with your thumb to scroll. The touchpad can be clicked but there is also a selection button for your index finger. For navigation, the remote offers a separate Back button and an Oculus button. The latter is used for calibrating the hand remote (long press) or returning to Oculus Home (short press). The headset itself has only power and volume up/down buttons. Built-in speakers are cleverly hidden inside the head strap. They provide stereo or spatial audio without a need to plug in external headphones. The head strap is flexible and easy to adjust. The headset has a detachable soft face mask, which makes it fairly comfortable to wear. Oculus Go is quite heavy, though. Normal size eyeglasses fit inside, but you can also order prescription lens inserts. All in all, Oculus Go is a great VR headset especially for 360-degree media.","title":"What is it?"},{"location":"tutorials/oculus_go_device/#should-i-get-an-oculus-go-or-a-gearvr","text":"If you already own a compatible Samsung phone, purchasing only the VR frame is an affordable choice (about $110). GearVR headset is a lightweight addition to your bag, which you will probably value if you travel a lot and carry a large-screen phone anyway. However, in most cases Oculus Go is a better choice. Since it is a standalone device it has a battery of its own and you can dedicate the device for its sole purpose: VR. You will also save a fortune if you need a large number of headsets. To learn more about their differences, read comparison articles: Android Central: Oculus Go vs Samsung Gear VR How-To Geek: Gear VR vs Oculus Go: Which One is Better? ThreeSixtyCameras.com: Gear VR vs Oculus Go: Which is the better VR experience?","title":"Should I get an Oculus Go or a GearVR?"},{"location":"tutorials/oculus_go_device/#how-do-i-get-one","text":"You can purchase an Oculus Go directly from Oculus , from various online stores such as Amazon , or from local computer and electronics stores. A 32 GB model costs $199 (\u20ac219) and a 64 GB model $249 (\u20ac269).","title":"How do I get one?"},{"location":"tutorials/oculus_go_preparing/","text":"LiveSYNC on Oculus Go Preparing Next, we will go through basic preparations before a presentation. As a reminder, presenting with the LiveSYNC tool works as follows: The presenter controls the presentation with a separate control device. This is typically an iOS or an Android tablet. The presenter will reserve a communication channel for the presentation. A channel number consists of four digits from range 1000-9999. Each member of the audience uses a personal viewing device. This can be an iOS/Android phone or tablet, or GearVR/Oculus Go headset. The viewer devices will join the communication channel using the channel number. Channel configuration Note Here we assume that you have already set up a channel in Director Mode on your control device (tablet). As an example, we will use channel number 5034 . Refer to User Guide for more information. To view a presentation, the headset must join the same channel that the control device is using. Follow these steps to configure a new presentation channel to your Oculus Go headset: On your Oculus Go device, start the LiveSYNC app, and select '+' from the Home screen. Point it with the hand remote controller and click the controller's selection button. Type in the channel number using the virtual numeric keypad, and click OK . The configured channel appears on the Home screen. Tip If you have multiple control devices (tablets), you can configure a separate channel for each by repeating the steps 1-3. This way you can easily choose which channel number to join ie. whose presentation to follow. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. When one of the representatives needs a break, they will temporarily connect all headsets to one iPad. Their iPads are using channels 2054 and 5039. Joan configures both channels to all six headsets. Now they can quickly swap a headset from one iPad to the other. Note Bluetooth connections are local and hence a channel number is reserved from a control device's own pool. Thus, you can reuse a configured channel as many times as you want. This is different when you use Finwe's GlobalSYNC connection (a cloud service). A channel number is reserved from a globally shared pool for a limited period of time. After your lease time expires, the channel will be returned to the pool. When this happens, you must reserve a new channel for your next presentation. Connection test To test a configured channel, perform the following steps: On your control device (tablet), start the LiveSYNC app, and select a channel from the Home screen. The Mosaic tab will appear, showing My device item only (tablet's own view). On your Oculus Go device, select the same channel from the Home screen. Note If this is the first time you join a channel from this device, a permission dialog appears. It is shown also if you have removed file access permission. Select Allow to grant file access permission. Else, LiveSYNC app cannot access your own presentation files. Next, the Lobby appears. Audience members will wait here for a connection to the control device. The Lobby is a 360-degree environment where users can look around. The front wall contains a 2D screen panel, where notifications to the user are presented. The headset connects to the control device automatically as soon as it is available and within reach. User does not have to do anything . Note Usually, this takes only a few seconds. However, with Bluetooth technology and multiple devices, it can take up to tens of seconds. The devices share the same radio frequencies. Because of this, connection times become longer when the number of devices increases. Once the connection is established, live view from the Oculus Go headset appears on the control device's screen. The Mosaic view now contains a new item: Simultaneously on the headset, a notification tells that the device is ready for presentation: Tip If necessary, you can connect more viewing devices by repeating steps 2-4. They will all appear in the Mosaic view. You can freely mix all kinds of viewing devices: phones, tablets, and VR headsets. Note Your license type, used connection method, and used hardware set an upper limit for the number of simultaneous connections. You should test this in advance, before your presentation. Tip If you use a Bluetooth connection, be aware that Bluetooth implementations (chipsets and drivers) vary. Some perform better than others and have different maximum capabilities. When a Bluetooth chipset cannot handle the load, problems may occur. Usually disabling and enabling Bluetooth feature helps, but sometimes the device needs to be restarted. In case you encounter repeated connection problems, follow these steps: On your control device, disable Bluetooth, wait 10 seconds, and enable it again. All devices will reconnect. If the problem is not solved, restart the Oculus Go headset that has trouble connecting. If the problem is still not solved, restart also the control device. If the problem is still not solved, try to use a smaller amount of devices. Bluetooth 4.x compatible control devices typically allow connecting to 4-8 devices simultaneously. Warning The Oculus accompanying app uses Bluetooth for communicating with the Oculus Go headset. When the app is running, it tends to keep a Bluetooth connection open. Or, automatically opens a new connection when the headset appears within range. This can interfere with LiveSYNC when you try to connect to a channel using Bluetooth. The Oculus app does not provide a method for manually disconnecting. Also, it does not automatically disconnect when it is sent to the background. Thus, we recommend that you kill the Oculus app after use to disconnect it from the headset : Launch the recent applications menu (a.k.a task list / overview). How to do this depends on the Android version and phone brand. Often it is a simple square icon or an icon that resembles two rectangles overlapping each other. In portrait orientation, the button is located at the bottom of the screen next to the home button. Scroll through the apps until you find the Oculus app. Drag the app off the screen. In portrait orientation, swipe it to the right. When it disappears from the screen it will be closed and the Bluetooth connection will be freed. Presentation test When devices are connected you can test presenting by using bundled demo content: On your control device, switch to the Player tab from the bottom bar. Drag an item from the Content tab to the Presentation area at the center of the screen, and drop it there. On the headset, the playback command will be received, requested media item loaded, and the image rendered on screen. The headset will notify the control device of success or failure. (For example, if the content file could not be found). The control device shows a live view from the headset in the Devices tab and in the Mosaic tab. This allows monitoring the headset's operation. Leaving a presentation To leave an ongoing presentation, follow these steps: Press the Back button (arrow to left) from your Oculus Go headset's hand remote controller. A confirmation dialog will appear. After selecting OK , the headset will disconnect from the control device and return to Home . Simultaneously, the headset will disappear from the control device's Mosaic and Devices tabs. Other connected devices (if any) are not affected. Note The presenter can stop the presentation from the control device by pressing the Home icon (top left corner). A confirmation dialog will appear before the presentation is stopped. This method disconnects all connected viewing devices. Copying content files The LiveSYNC app contains sample files for testing. You can, of course, present also your own content. To do this, copy your content files (photos, videos, hotspot icons) to each device that will be used in your presentation . This means the control device and every viewing device. Read more from the User Guide . Next, we will go through how to copy content files to your Oculus Go headset. Windows PC Connect your Oculus Go headset to your PC using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Windows detects the headset and you choose to handle this device by exploring its files, it appears as a new device in Explorer . For example, on Windows 10 you'll find it under This PC -> VR-Headset . Copy your content files and folders to your Oculus Go just like you'd copy them into a USB flash drive. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your PC does not recognize the headset when you connect it with a USB cable, check that Developer Mode isn't enabled in your headset. On the Oculus companion app, navigate to Settings -> Your Oculus Go -> More Settings -> Developer Mode . Read more from Tips & Tricks . If it is enabled, the device will be detected in a different a USB mode, and it will not appear in Explorer. You can temporarily disable Developer Mode . Or, use the Android Debug Bridge (ADB) command line tool for transferring files. Mac Install Android File Transfer app for transferring files: https://www.android.com/filetransfer/ Connect your Oculus Go headset to your Mac using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Android File Transfer detects the headset, a new file management window appears. Copy your content files and folders to your Oculus Go by dragging them from your Mac's Finder window. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your headset is not detected and you are using a USB hub or an extension cable, try connecting the headset's cable directly to your computer. If this doesn't help, try connecting the cable to a different USB port. Storage Permission Check permissions in case you cannot find the LiveSYNC folder under Movies , or the LiveSYNC app cannot find your own files from that folder. /LiveSYNC folder is automatically generated when the app is run (if the folder is not found). To be able to create the folder under /Movies , the LiveSYNC app needs permission from the user to access the file system (read/write). This permission is first asked when you attempt to join a presentation. The permission remains until you manually remove it or re-install the app. You can also set the permission on your headset via Oculus Home as follows: From the bottom bar, select Navigate tab and then Library . From the left menu, select Apps . Find the LiveSYNC app from the app grid. Tap the three dots next to text LiveSYNC Oculus Go , and select Permissions from the appearing menu. Note the position of the switches. Both switches should be enabled: Location is required by Bluetooth connectivity and Storage for file access. Run the LiveSYNC app again to trigger folder creation. Note You can also use Explorer on Windows or Android File Transfer on Mac to create the folder yourself. LiveSYNC will still need the Storage permission for reading the files from this location. File naming conventions The folder /Movies/LiveSYNC is the content root folder for the LiveSYNC app on all Android-based devices (Oculus Go runs on Android OS). When you copy your content files there, make sure to use exactly the same filenames that you use on your control device . For example, the names are case sensitive: MyVideo.mp4 is not the same as myvideo.mp4 . If the filenames do not match, the LiveSYNC app running on your Oculus Go headset cannot find the file that the control device tells it to load. When this happens, a textual error notification will appear on the headset and also in the control device's live view for this particular headset. Notice that while the filenames must match, the file content can be different. Example Matt is using an old iPad as a control device for his presentation. The iPad can play FullHD resolution videos, but not 4K. Matt encodes two different versions of his video: 1920x960 for his iPad and 3840x1920 for his Oculus Go. He renames the files so that they have the same filename Christmas.mp4 on his iPad and on his Oculus Go. LiveSYNC detects them as the same video and correctly loads them on both devices. You can copy your 360-degree photos and videos directly to /Movies/LiveSYNC folder, but you can also create one level of subfolders. This helps in organizing your content. Notice that if you use subfolders on your control device, you must use them also on your viewing devices. Example /Movies/LiveSYNC/Christmas2018 is a valid location for your 360-degree Christmas photos from 2018. /Movies/LiveSYNC/2018/Christmas has two directory levels under the LiveSYNC root folder and is thus invalid. Info With the LiveSYNC tool, you can present normal 2D photos and videos, 2D and 3D 360-degree photos and videos, and use your own tag images. File naming conventions allow LiveSYNC to correctly detect files. For example, whether an image file is a 2D photo, a 2D 360 photo, a 3D 360 photo, or a tag icon. These rules are described in the User Guide and they apply also to LiveSYNC on Oculus Go.","title":"Preparing"},{"location":"tutorials/oculus_go_preparing/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_preparing/#preparing","text":"Next, we will go through basic preparations before a presentation. As a reminder, presenting with the LiveSYNC tool works as follows: The presenter controls the presentation with a separate control device. This is typically an iOS or an Android tablet. The presenter will reserve a communication channel for the presentation. A channel number consists of four digits from range 1000-9999. Each member of the audience uses a personal viewing device. This can be an iOS/Android phone or tablet, or GearVR/Oculus Go headset. The viewer devices will join the communication channel using the channel number.","title":"Preparing"},{"location":"tutorials/oculus_go_preparing/#channel-configuration","text":"Note Here we assume that you have already set up a channel in Director Mode on your control device (tablet). As an example, we will use channel number 5034 . Refer to User Guide for more information. To view a presentation, the headset must join the same channel that the control device is using. Follow these steps to configure a new presentation channel to your Oculus Go headset: On your Oculus Go device, start the LiveSYNC app, and select '+' from the Home screen. Point it with the hand remote controller and click the controller's selection button. Type in the channel number using the virtual numeric keypad, and click OK . The configured channel appears on the Home screen. Tip If you have multiple control devices (tablets), you can configure a separate channel for each by repeating the steps 1-3. This way you can easily choose which channel number to join ie. whose presentation to follow. Example Joan's marketing team is participating in a trade show. Their company is launching a new product. Joan's team is using a 360-degree video to showcase it at their booth. They are controlling six Oculus Go headsets with LiveSYNC. To reduce waiting time, they run two groups of three headsets in parallel. The groups are controlled with two iPads. When one of the representatives needs a break, they will temporarily connect all headsets to one iPad. Their iPads are using channels 2054 and 5039. Joan configures both channels to all six headsets. Now they can quickly swap a headset from one iPad to the other. Note Bluetooth connections are local and hence a channel number is reserved from a control device's own pool. Thus, you can reuse a configured channel as many times as you want. This is different when you use Finwe's GlobalSYNC connection (a cloud service). A channel number is reserved from a globally shared pool for a limited period of time. After your lease time expires, the channel will be returned to the pool. When this happens, you must reserve a new channel for your next presentation.","title":"Channel configuration"},{"location":"tutorials/oculus_go_preparing/#connection-test","text":"To test a configured channel, perform the following steps: On your control device (tablet), start the LiveSYNC app, and select a channel from the Home screen. The Mosaic tab will appear, showing My device item only (tablet's own view). On your Oculus Go device, select the same channel from the Home screen. Note If this is the first time you join a channel from this device, a permission dialog appears. It is shown also if you have removed file access permission. Select Allow to grant file access permission. Else, LiveSYNC app cannot access your own presentation files. Next, the Lobby appears. Audience members will wait here for a connection to the control device. The Lobby is a 360-degree environment where users can look around. The front wall contains a 2D screen panel, where notifications to the user are presented. The headset connects to the control device automatically as soon as it is available and within reach. User does not have to do anything . Note Usually, this takes only a few seconds. However, with Bluetooth technology and multiple devices, it can take up to tens of seconds. The devices share the same radio frequencies. Because of this, connection times become longer when the number of devices increases. Once the connection is established, live view from the Oculus Go headset appears on the control device's screen. The Mosaic view now contains a new item: Simultaneously on the headset, a notification tells that the device is ready for presentation: Tip If necessary, you can connect more viewing devices by repeating steps 2-4. They will all appear in the Mosaic view. You can freely mix all kinds of viewing devices: phones, tablets, and VR headsets. Note Your license type, used connection method, and used hardware set an upper limit for the number of simultaneous connections. You should test this in advance, before your presentation. Tip If you use a Bluetooth connection, be aware that Bluetooth implementations (chipsets and drivers) vary. Some perform better than others and have different maximum capabilities. When a Bluetooth chipset cannot handle the load, problems may occur. Usually disabling and enabling Bluetooth feature helps, but sometimes the device needs to be restarted. In case you encounter repeated connection problems, follow these steps: On your control device, disable Bluetooth, wait 10 seconds, and enable it again. All devices will reconnect. If the problem is not solved, restart the Oculus Go headset that has trouble connecting. If the problem is still not solved, restart also the control device. If the problem is still not solved, try to use a smaller amount of devices. Bluetooth 4.x compatible control devices typically allow connecting to 4-8 devices simultaneously. Warning The Oculus accompanying app uses Bluetooth for communicating with the Oculus Go headset. When the app is running, it tends to keep a Bluetooth connection open. Or, automatically opens a new connection when the headset appears within range. This can interfere with LiveSYNC when you try to connect to a channel using Bluetooth. The Oculus app does not provide a method for manually disconnecting. Also, it does not automatically disconnect when it is sent to the background. Thus, we recommend that you kill the Oculus app after use to disconnect it from the headset : Launch the recent applications menu (a.k.a task list / overview). How to do this depends on the Android version and phone brand. Often it is a simple square icon or an icon that resembles two rectangles overlapping each other. In portrait orientation, the button is located at the bottom of the screen next to the home button. Scroll through the apps until you find the Oculus app. Drag the app off the screen. In portrait orientation, swipe it to the right. When it disappears from the screen it will be closed and the Bluetooth connection will be freed.","title":"Connection test"},{"location":"tutorials/oculus_go_preparing/#presentation-test","text":"When devices are connected you can test presenting by using bundled demo content: On your control device, switch to the Player tab from the bottom bar. Drag an item from the Content tab to the Presentation area at the center of the screen, and drop it there. On the headset, the playback command will be received, requested media item loaded, and the image rendered on screen. The headset will notify the control device of success or failure. (For example, if the content file could not be found). The control device shows a live view from the headset in the Devices tab and in the Mosaic tab. This allows monitoring the headset's operation.","title":"Presentation test"},{"location":"tutorials/oculus_go_preparing/#leaving-a-presentation","text":"To leave an ongoing presentation, follow these steps: Press the Back button (arrow to left) from your Oculus Go headset's hand remote controller. A confirmation dialog will appear. After selecting OK , the headset will disconnect from the control device and return to Home . Simultaneously, the headset will disappear from the control device's Mosaic and Devices tabs. Other connected devices (if any) are not affected. Note The presenter can stop the presentation from the control device by pressing the Home icon (top left corner). A confirmation dialog will appear before the presentation is stopped. This method disconnects all connected viewing devices.","title":"Leaving a presentation"},{"location":"tutorials/oculus_go_preparing/#copying-content-files","text":"The LiveSYNC app contains sample files for testing. You can, of course, present also your own content. To do this, copy your content files (photos, videos, hotspot icons) to each device that will be used in your presentation . This means the control device and every viewing device. Read more from the User Guide . Next, we will go through how to copy content files to your Oculus Go headset.","title":"Copying content files"},{"location":"tutorials/oculus_go_preparing/#windows-pc","text":"Connect your Oculus Go headset to your PC using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Windows detects the headset and you choose to handle this device by exploring its files, it appears as a new device in Explorer . For example, on Windows 10 you'll find it under This PC -> VR-Headset . Copy your content files and folders to your Oculus Go just like you'd copy them into a USB flash drive. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your PC does not recognize the headset when you connect it with a USB cable, check that Developer Mode isn't enabled in your headset. On the Oculus companion app, navigate to Settings -> Your Oculus Go -> More Settings -> Developer Mode . Read more from Tips & Tricks . If it is enabled, the device will be detected in a different a USB mode, and it will not appear in Explorer. You can temporarily disable Developer Mode . Or, use the Android Debug Bridge (ADB) command line tool for transferring files.","title":"Windows PC"},{"location":"tutorials/oculus_go_preparing/#mac","text":"Install Android File Transfer app for transferring files: https://www.android.com/filetransfer/ Connect your Oculus Go headset to your Mac using the bundled USB cable. Put on your headset and select Accept to confirm you want to allow your computer to access files on the headset. Once Android File Transfer detects the headset, a new file management window appears. Copy your content files and folders to your Oculus Go by dragging them from your Mac's Finder window. The correct location is \\Movies\\LiveSYNC folder. This folder will be created automatically when the LiveSYNC app is run. Notice that the directory cannot be created if you haven't granted file access permission. Note If your headset is not detected and you are using a USB hub or an extension cable, try connecting the headset's cable directly to your computer. If this doesn't help, try connecting the cable to a different USB port.","title":"Mac"},{"location":"tutorials/oculus_go_preparing/#storage-permission","text":"Check permissions in case you cannot find the LiveSYNC folder under Movies , or the LiveSYNC app cannot find your own files from that folder. /LiveSYNC folder is automatically generated when the app is run (if the folder is not found). To be able to create the folder under /Movies , the LiveSYNC app needs permission from the user to access the file system (read/write). This permission is first asked when you attempt to join a presentation. The permission remains until you manually remove it or re-install the app. You can also set the permission on your headset via Oculus Home as follows: From the bottom bar, select Navigate tab and then Library . From the left menu, select Apps . Find the LiveSYNC app from the app grid. Tap the three dots next to text LiveSYNC Oculus Go , and select Permissions from the appearing menu. Note the position of the switches. Both switches should be enabled: Location is required by Bluetooth connectivity and Storage for file access. Run the LiveSYNC app again to trigger folder creation. Note You can also use Explorer on Windows or Android File Transfer on Mac to create the folder yourself. LiveSYNC will still need the Storage permission for reading the files from this location.","title":"Storage Permission"},{"location":"tutorials/oculus_go_preparing/#file-naming-conventions","text":"The folder /Movies/LiveSYNC is the content root folder for the LiveSYNC app on all Android-based devices (Oculus Go runs on Android OS). When you copy your content files there, make sure to use exactly the same filenames that you use on your control device . For example, the names are case sensitive: MyVideo.mp4 is not the same as myvideo.mp4 . If the filenames do not match, the LiveSYNC app running on your Oculus Go headset cannot find the file that the control device tells it to load. When this happens, a textual error notification will appear on the headset and also in the control device's live view for this particular headset. Notice that while the filenames must match, the file content can be different. Example Matt is using an old iPad as a control device for his presentation. The iPad can play FullHD resolution videos, but not 4K. Matt encodes two different versions of his video: 1920x960 for his iPad and 3840x1920 for his Oculus Go. He renames the files so that they have the same filename Christmas.mp4 on his iPad and on his Oculus Go. LiveSYNC detects them as the same video and correctly loads them on both devices. You can copy your 360-degree photos and videos directly to /Movies/LiveSYNC folder, but you can also create one level of subfolders. This helps in organizing your content. Notice that if you use subfolders on your control device, you must use them also on your viewing devices. Example /Movies/LiveSYNC/Christmas2018 is a valid location for your 360-degree Christmas photos from 2018. /Movies/LiveSYNC/2018/Christmas has two directory levels under the LiveSYNC root folder and is thus invalid. Info With the LiveSYNC tool, you can present normal 2D photos and videos, 2D and 3D 360-degree photos and videos, and use your own tag images. File naming conventions allow LiveSYNC to correctly detect files. For example, whether an image file is a 2D photo, a 2D 360 photo, a 3D 360 photo, or a tag icon. These rules are described in the User Guide and they apply also to LiveSYNC on Oculus Go.","title":"File naming conventions"},{"location":"tutorials/oculus_go_presenting/","text":"LiveSYNC on Oculus Go Presenting Presenting content with Oculus Go headsets is like presenting with other viewing devices. Connecting The first step in presenting is to connect the devices to the same LiveSYNC channel . This allows the devices to communicate with each other ie. exchange command and status messages. Connect all viewing devices to the control device as follows: On your control device, start the LiveSYNC app. From the Home screen, select a channel that you want to use. The Mosaic view appears. The control device is now waiting for viewing devices to join the channel. On each of your viewing devices, start the LiveSYNC app. From the Home screen, select the same channel that you selected on the control device. The Lobby view appears. The viewing device is now looking for the control device. It will attempt to join the specified channel. Connections are created one by one. Live views from the viewing devices appear in the control device's Mosaic view. Wait until all connections are established and you have a live view from each viewing device. Connected devices appear in the Mosaic view. It is easy to observe what every member of the audience is looking at . Note The grid is automatically resized so that maximum amount of screen real estate is used. Playing media Once viewing devices are connected, play photo and video content as follows: On your control device, navigate to Player tab using the bottom bar. Drag a media item from the Content tab to the Presentation area and drop it there. The content will be loaded and rendered on the screen. All connected viewing devices will also load the same content and render it on screen. Video playback starts automatically when all connected devices are ready to play. You can pan the view by dragging with one finger inside the Presentation area . Zoom the view using the pinch gesture within the Presentation area . Notice that panning and zooming effect locally, not to other viewing devices. The audience will pan (and zoom on supported devices) by themselves. With VR headsets such as Oculus Go, panning is performed simply by turning one's head. When playing videos, a Control Panel appears at the bottom of the Presentation area . Play , Pause and Seekbar controls work just like in any other video player. It is also possible to enable Looping to continuously repeat the video. Keep an eye on the remaining playback time so that you know when it is time to change content. Change content whenever necessary. Simply drag another media item to the Presentation area . Existing photo or video will be immediately replaced with the new one. Drag a content item to the Presentation area . Control video playback via the Control Panel, which appears and disappears automatically . Marking with tags The presenter may want to draw the attention of the audience to a specific area of the content being played. Tags can be used for marking such points-of-interest during the presentation. You can use them as follows: Find a tag that you wish to use from the Tags tab. Drag a tag from the Tags tab to the Presentation area and drop it to the position where you want it to appear. The tag will stick to the content. It will appear also on the screens of the connected viewing devices. Drag a tag to the Presentation area to mark a point-of-interest . Observing & guiding Interaction and guidance are difficult when watching 360-degree content using separate viewing devices. How can you instruct if you cannot see the other guy's view? LiveSYNC solves the problem by providing live views on the control device's screen. It works as follows: Live view from each connected device appears in the Mosaic tab, and also in the Devices area of the Player tab. Make a quick glance to see if someone in the audience appears to need guidance. Or, if someone has already asked for help, find his headset based on the name given to that headset. The names appear over the live views. Assist the user. Observe from the live view coming from his headset that the problem is solved. For example, if someone in the audience cannot find a point-of-interest, the presenter can instruct him to turn to the correct direction. If necessary, switch to Mosaic view and double tap a particular users' live view to make it full screen. In full-screen mode it is easier to follow another user's view. Double tap the view again to resize it back to normal size. Observing and guiding users is convenient using the Mosaic view . Disconnecting When your presentation is over, stop presenting as follows: Tap the Home icon at the top left corner of the screen. A confirmation dialog appears. Select OK to quit the presentation. You will return to the Home screen. All viewing devices will be disconnected from the control device. Close the LiveSYNC app on all devices and recharge the batteries. To end your presentation, tap the home icon and select OK .","title":"Presenting"},{"location":"tutorials/oculus_go_presenting/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_presenting/#presenting","text":"Presenting content with Oculus Go headsets is like presenting with other viewing devices.","title":"Presenting"},{"location":"tutorials/oculus_go_presenting/#connecting","text":"The first step in presenting is to connect the devices to the same LiveSYNC channel . This allows the devices to communicate with each other ie. exchange command and status messages. Connect all viewing devices to the control device as follows: On your control device, start the LiveSYNC app. From the Home screen, select a channel that you want to use. The Mosaic view appears. The control device is now waiting for viewing devices to join the channel. On each of your viewing devices, start the LiveSYNC app. From the Home screen, select the same channel that you selected on the control device. The Lobby view appears. The viewing device is now looking for the control device. It will attempt to join the specified channel. Connections are created one by one. Live views from the viewing devices appear in the control device's Mosaic view. Wait until all connections are established and you have a live view from each viewing device. Connected devices appear in the Mosaic view. It is easy to observe what every member of the audience is looking at . Note The grid is automatically resized so that maximum amount of screen real estate is used.","title":"Connecting"},{"location":"tutorials/oculus_go_presenting/#playing-media","text":"Once viewing devices are connected, play photo and video content as follows: On your control device, navigate to Player tab using the bottom bar. Drag a media item from the Content tab to the Presentation area and drop it there. The content will be loaded and rendered on the screen. All connected viewing devices will also load the same content and render it on screen. Video playback starts automatically when all connected devices are ready to play. You can pan the view by dragging with one finger inside the Presentation area . Zoom the view using the pinch gesture within the Presentation area . Notice that panning and zooming effect locally, not to other viewing devices. The audience will pan (and zoom on supported devices) by themselves. With VR headsets such as Oculus Go, panning is performed simply by turning one's head. When playing videos, a Control Panel appears at the bottom of the Presentation area . Play , Pause and Seekbar controls work just like in any other video player. It is also possible to enable Looping to continuously repeat the video. Keep an eye on the remaining playback time so that you know when it is time to change content. Change content whenever necessary. Simply drag another media item to the Presentation area . Existing photo or video will be immediately replaced with the new one. Drag a content item to the Presentation area . Control video playback via the Control Panel, which appears and disappears automatically .","title":"Playing media"},{"location":"tutorials/oculus_go_presenting/#marking-with-tags","text":"The presenter may want to draw the attention of the audience to a specific area of the content being played. Tags can be used for marking such points-of-interest during the presentation. You can use them as follows: Find a tag that you wish to use from the Tags tab. Drag a tag from the Tags tab to the Presentation area and drop it to the position where you want it to appear. The tag will stick to the content. It will appear also on the screens of the connected viewing devices. Drag a tag to the Presentation area to mark a point-of-interest .","title":"Marking with tags"},{"location":"tutorials/oculus_go_presenting/#observing-guiding","text":"Interaction and guidance are difficult when watching 360-degree content using separate viewing devices. How can you instruct if you cannot see the other guy's view? LiveSYNC solves the problem by providing live views on the control device's screen. It works as follows: Live view from each connected device appears in the Mosaic tab, and also in the Devices area of the Player tab. Make a quick glance to see if someone in the audience appears to need guidance. Or, if someone has already asked for help, find his headset based on the name given to that headset. The names appear over the live views. Assist the user. Observe from the live view coming from his headset that the problem is solved. For example, if someone in the audience cannot find a point-of-interest, the presenter can instruct him to turn to the correct direction. If necessary, switch to Mosaic view and double tap a particular users' live view to make it full screen. In full-screen mode it is easier to follow another user's view. Double tap the view again to resize it back to normal size. Observing and guiding users is convenient using the Mosaic view .","title":"Observing &amp; guiding"},{"location":"tutorials/oculus_go_presenting/#disconnecting","text":"When your presentation is over, stop presenting as follows: Tap the Home icon at the top left corner of the screen. A confirmation dialog appears. Select OK to quit the presentation. You will return to the Home screen. All viewing devices will be disconnected from the control device. Close the LiveSYNC app on all devices and recharge the batteries. To end your presentation, tap the home icon and select OK .","title":"Disconnecting"},{"location":"tutorials/oculus_go_setup/","text":"LiveSYNC on Oculus Go Setup The Oculus app When you receive your Oculus Go headset, unpack it and start the initial setup procedure. It might be a bit surprising, but you must install an application to your phone : To set up and connect your Oculus Go, you'll need to download the Oculus app on your supported mobile phone. With the Oculus app, you can set up your headset, browse VR games and apps and customize your device settings. (Oculus Go website) This accompanying app is mandatory during the initial setup of the headset. After that you will need it very rarely, if ever. If you plan to purchase multiple headsets, they can be all set up and configured from a single phone. Download the Oculus Go app from here: https://oculus.com/app Alternatively, you can use these direct links for Android and iOS . Note This is NOT the same app that you use with a GearVR headset. Both apps have the same name Oculus . You can differentiate them from the icon: GearVR app icon has text GearVR, Oculus Go app icon does not have any text. The Oculus Go accompanying app on an Android phone. Pairing & setup Follow these steps to pair your phone with the headset and go through the initial setup. Download the Oculus app using one of the links above, then launch the installed app. If you don't have an Oculus account yet, sign up first, and then log in. Once logged in, navigate to Settings , and select Pair New Headset . From Choose a Headset menu, select Oculus Go , and press Start Now . Turn on your Oculus Go device. Press Continue . Plug your Oculus Go into a power source. Press Continue . After search completes, found headsets are listed. If multiple devices were found nearby, choose the one with the matching serial number. The number is printed below a QR code under the fabric of the head strap, near the USB connector. Press Continue . Select a Wifi access point that will be used for connecting the headset to the network. Press Continue . Put a battery into the controller and choose with which hand you want to use your controller. Press Continue . Select language to use in VR. Press Continue . Add a payment method (a credit card or a PayPal account), or press Skip . You will need a payment method only for making purchases from the Oculus Store. Go through safety etc. information. Press Continue . When ready, you will see a dialog saying Preparing your Oculus Go... . Once it has finished, pairing and setup has completed. For more information, see the questions and answers here . Once pairing and setup has completed your headset(s) will appear in the Settings tab. First time use Go to your Oculus Go device. Make sure that power is on and then put the headset on your face. There is an infrared proximity sensor inside the headset (between the lenses). The headset will wake up automatically and you will hear a greeting sound. Adjust the head strap so that you feel comfortable. Grab the hand remote controller, look straight ahead, and point with the controller to the direction you are looking at. Then, press and hold the Oculus button (the one with the ring symbol) on the controller. This procedure will calibrate the controller. Whenever the device wakes up from sleep you must calibrate the hand remote controller. Note Every time the headset wakes up from sleep it will present the calibration dialog . Calibration is necessary, because the headset and the hand remote controller are two independent devices. They are both only aware of rotation around their own center point . Moreover, the devices do not know what actual direction they are pointing at (for example, North, South-East, etc.). Hence, they need to be calibrated by making them point to the same direction and then pressing a button to signal this. Unfortunately, this needs to be repeated every time the headset wakes up. Sensors are turned off to save power during sleep, hence they will lose the tracking. After calibration and first time use tutorial you will enter Oculus Home . Here you can install new apps, start installed apps, configure the headset, and use services offered by Oculus. The Oculus Home. Note The image above is a screenshot from the headset's internal display. The device renders a heavily distorted double barrel view on the screen. You will not see the barrels nor the distortion when you wear the headset and look through the lenses. In addition, your brain will combine left and right eye images together. You will experience VR as a 3D world, although your field-of-view in VR is narrower than in real life. In this documentation, we frequently use either double barrel view or single barrel view in screenshots from a VR headset. A single barrel view is simply a double barrel view cut in half to save screen space. Tip We recommend that you spend some time to play around with the device. Get familiar with the hand remote and Oculus Home. Try a couple of pre-installed apps. Ask your boss a permission to take it home for the weekend and watch a couple of movies from Netflix or Youtube VR. Installing LiveSYNC Once you feel familiar with Oculus Go, it is time to install the LiveSYNC app. The installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). Info LiveSYNC has not officially launched on Oculus platform yet. However, you can start using it already by installing it from our beta channel. Follow the steps below. Check your Oculus username and email. Start the Oculus accompanying app on your phone, navigate to Settings , and find the username and email (see the image below): Installing software from Oculus beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for Oculus Go. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an invite email from Oculus to that email address, accept the invite by clicking a confirmation link in the email. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options how to install the application: Using the accompanying Oculus app on your phone: tap the magnifier class icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Install on ... button. Using the Oculus Go device, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu. Start LiveSYNC Oculus Go by selecting it from the apps grid. Note Notice that the app's version number appears in the splash screen, in the small print below the LiveSYNC logo. Here you can easily check which version you are running. The LiveSYNC app is updated frequently.","title":"Setup"},{"location":"tutorials/oculus_go_setup/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_setup/#setup","text":"","title":"Setup"},{"location":"tutorials/oculus_go_setup/#the-oculus-app","text":"When you receive your Oculus Go headset, unpack it and start the initial setup procedure. It might be a bit surprising, but you must install an application to your phone : To set up and connect your Oculus Go, you'll need to download the Oculus app on your supported mobile phone. With the Oculus app, you can set up your headset, browse VR games and apps and customize your device settings. (Oculus Go website) This accompanying app is mandatory during the initial setup of the headset. After that you will need it very rarely, if ever. If you plan to purchase multiple headsets, they can be all set up and configured from a single phone. Download the Oculus Go app from here: https://oculus.com/app Alternatively, you can use these direct links for Android and iOS . Note This is NOT the same app that you use with a GearVR headset. Both apps have the same name Oculus . You can differentiate them from the icon: GearVR app icon has text GearVR, Oculus Go app icon does not have any text. The Oculus Go accompanying app on an Android phone.","title":"The Oculus app"},{"location":"tutorials/oculus_go_setup/#pairing-setup","text":"Follow these steps to pair your phone with the headset and go through the initial setup. Download the Oculus app using one of the links above, then launch the installed app. If you don't have an Oculus account yet, sign up first, and then log in. Once logged in, navigate to Settings , and select Pair New Headset . From Choose a Headset menu, select Oculus Go , and press Start Now . Turn on your Oculus Go device. Press Continue . Plug your Oculus Go into a power source. Press Continue . After search completes, found headsets are listed. If multiple devices were found nearby, choose the one with the matching serial number. The number is printed below a QR code under the fabric of the head strap, near the USB connector. Press Continue . Select a Wifi access point that will be used for connecting the headset to the network. Press Continue . Put a battery into the controller and choose with which hand you want to use your controller. Press Continue . Select language to use in VR. Press Continue . Add a payment method (a credit card or a PayPal account), or press Skip . You will need a payment method only for making purchases from the Oculus Store. Go through safety etc. information. Press Continue . When ready, you will see a dialog saying Preparing your Oculus Go... . Once it has finished, pairing and setup has completed. For more information, see the questions and answers here . Once pairing and setup has completed your headset(s) will appear in the Settings tab.","title":"Pairing &amp; setup"},{"location":"tutorials/oculus_go_setup/#first-time-use","text":"Go to your Oculus Go device. Make sure that power is on and then put the headset on your face. There is an infrared proximity sensor inside the headset (between the lenses). The headset will wake up automatically and you will hear a greeting sound. Adjust the head strap so that you feel comfortable. Grab the hand remote controller, look straight ahead, and point with the controller to the direction you are looking at. Then, press and hold the Oculus button (the one with the ring symbol) on the controller. This procedure will calibrate the controller. Whenever the device wakes up from sleep you must calibrate the hand remote controller. Note Every time the headset wakes up from sleep it will present the calibration dialog . Calibration is necessary, because the headset and the hand remote controller are two independent devices. They are both only aware of rotation around their own center point . Moreover, the devices do not know what actual direction they are pointing at (for example, North, South-East, etc.). Hence, they need to be calibrated by making them point to the same direction and then pressing a button to signal this. Unfortunately, this needs to be repeated every time the headset wakes up. Sensors are turned off to save power during sleep, hence they will lose the tracking. After calibration and first time use tutorial you will enter Oculus Home . Here you can install new apps, start installed apps, configure the headset, and use services offered by Oculus. The Oculus Home. Note The image above is a screenshot from the headset's internal display. The device renders a heavily distorted double barrel view on the screen. You will not see the barrels nor the distortion when you wear the headset and look through the lenses. In addition, your brain will combine left and right eye images together. You will experience VR as a 3D world, although your field-of-view in VR is narrower than in real life. In this documentation, we frequently use either double barrel view or single barrel view in screenshots from a VR headset. A single barrel view is simply a double barrel view cut in half to save screen space. Tip We recommend that you spend some time to play around with the device. Get familiar with the hand remote and Oculus Home. Try a couple of pre-installed apps. Ask your boss a permission to take it home for the weekend and watch a couple of movies from Netflix or Youtube VR.","title":"First time use"},{"location":"tutorials/oculus_go_setup/#installing-livesync","text":"Once you feel familiar with Oculus Go, it is time to install the LiveSYNC app. The installation can be triggered either via the accompanying Oculus app on your phone (outside VR) or via Oculus Home (inside VR). Info LiveSYNC has not officially launched on Oculus platform yet. However, you can start using it already by installing it from our beta channel. Follow the steps below. Check your Oculus username and email. Start the Oculus accompanying app on your phone, navigate to Settings , and find the username and email (see the image below): Installing software from Oculus beta channel requires an invite . Contact us and tell that you want to join LiveSYNC beta channel for Oculus Go. We need the email address from Step 1 to be able to add you . This cannot be just one of your email addresses; it has to be the one that is connected with your Oculus account. Once you receive an invite email from Oculus to that email address, accept the invite by clicking a confirmation link in the email. Notice that we must send each invite manually, so it can take a while before the email arrives. After accepting the invite you have multiple options how to install the application: Using the accompanying Oculus app on your phone: tap the magnifier class icon to open search, type livesync and select LiveSYNC Oculus Go , then click Install on ... button. You can also select Library tab from the bottom bar and see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Install on ... button. Using the Oculus Go device, select Search from the bottom bar, type livesync , select LiveSYNC Oculus Go , then click Get . You can also select Library tab from the bottom bar and then Not Installed page from the left side menu to see if LiveSYNC Oculus Go already appears in the apps list. Select it from the list and then click Get . When the installation has completed you will find LiveSYNC Oculus Go listed in the apps grid: Select Library tab from the bottom bar, and then Apps page from the left side menu. Start LiveSYNC Oculus Go by selecting it from the apps grid. Note Notice that the app's version number appears in the splash screen, in the small print below the LiveSYNC logo. Here you can easily check which version you are running. The LiveSYNC app is updated frequently.","title":"Installing LiveSYNC"},{"location":"tutorials/oculus_go_tips/","text":"LiveSYNC on Oculus Go Tips & tricks To conclude the tutorial, we will present a set of handy tips. Labeling We highly recommend that you mark your devices with labels for the following reasons: You can easily distinguish your headsets from each other, for example, to find a particular headset's live view from the Mosaic tab. You can recognize which hand remote belongs to which headset, for example when you store them together. You can get back your headsets (and content!) after a presentation elsewhere, for example when you use them in a trade show or borrow to a colleague for his presentation. You can brand them with your company/team/project logo. You can make the headsets look fancier with a bit of decoration. In general, two-three labels are placed in different positions: Oculus is using the left side of the head strap - under the fabric - for a serial number and other product info, which you should not cover. However, the right side is empty. When you place a label there, it will be hidden during normal product use. Yet, it is easy to check when you know where to look from. This is a great place to put your owner identification label. The front side of the headset has a large solid surface that is clearly visible when the headset is being used. Obviously, this is where your company logo or fancy decoration is typically placed. Warning The headset uses its front surface for head dissipation . Do not block the airflow or you will risk running your headset too hot. It will stop and you must wait until it cools down. LiveSYNC allows giving each device a name ( LiveSYNC name ). This is shown for example in the Mosaic view. It is a good idea to print a label that contains this name and put it in a place where you can easily see it when the headset is being used. If you have a small number of headsets, consider using color coding. Add something with red color to the head strap of one headset and set its name to \"Red\". This way you can find the correct headset from the Mosaic view easily. Even if it is too far for reading a textual label, or if the user is looking away from you and you cannot see the front side. Pull the hand remote's battery cover open, and put an identification label there. This way you can easily check which hand remote belongs to which headset. Labeling examples . Hand remote controllers Oculus Go headset is designed to be used with a hand-held remote controller. Its main purpose is to allow selecting items by pointing at them. In most presentations, users have no use for the controller. The presentation is controlled via the control device and users do not need to make any selections. In such a case, you may want to consider collecting all the remote controllers away . This way there will be much less hassle and no risk of someone forgetting the controller in his pocket. Remember that you still need to keep the remotes at hand, though. Whenever a headset is wakened up from sleep, its hand remote must be calibrated. This is a requirement of the Oculus platform. An assistant or the presenter himself can do this by pressing and holding the Oculus button on each hand remote one at a time. Notice that proper calibration by pointing at the correct direction is not necessary if the remotes will not be used for selections. To get rid of the calibration dialog after waking up the headset, all that is needed is a long press of the Oculus button. Note This is a known issue for Oculus. They have said that they are working on a solution to allow using the headset without performing hand remote calibration each time. However, a fix has not been released yet. Tip The Developer Mode has an interesting shortcut: to skip hand remote calibration, the user can press one of the volume keys (+/-) when the dialog appears. It is not a perfect solution as action is still required for each headset, but at least you don't need to keep the remote controllers around. Read more about the Developer Mode below. Oculus Go headset has three buttons. From left to right, Power, Volume Up, Volume Down. In Developer Mode, hand remote calibration can be skipped by pressing Volume Up/Down button . Power management We recommend that you increase the headset's timeout for dropping into sleep : Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Power Settings . Select Auto Sleep and set it to 5 minutes or Never . Reduce the frequency of hand remote calibration by adjusting the headset's sleep timeout . Tip You can also cover the proximity sensor (between the lenses) with a piece of tape. However, configuring Auto Sleep time works very well and we recommend that you use that option. Tip If you carry your headset in a bag in a sleep mode, you may also want to consider disabling Auto Wake-Up feature. The head strap tends to find its way in front of the proximity sensor, turn the headset on every now and then inside your bag, and drain the battery. Of course, you can also turn off the headset when you are not using it. Warning The headset uses its front surface for head dissipation . When you place the headset on a table, try to avoid placing its front side towards the table surface. It is tempting because in this way the headset is in a good balance. However, head dissipation suffers because there is no air flow around the front surface. Developer Mode You can easily activate and deactivate the Developer Mode for each headset from the Oculus accompanying app on your phone. To do this you must first create a developer organization (real or fake) on the Oculus Dashboard . After that, follow these steps: Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Developer Mode . Disable or enable the feature. Read more from here . Enable/disable Developer Mode by turning the switch from the Oculus accompanying app . When the Developer Mode is enabled, you can: Skip hand remote calibration by pressing volume +/- key from the headset . Copy content files back and forth using Android Debug Bridge (ADB) tool from the command line. This can be handy if you're into automating things by writing scripts. Sideload apps using the Android Debug Bridge (ADB) tool from the command line. Note When the Developer Mode is enabled, the device connects via USB cable in a different mode. It will NOT appear in the Windows Explorer. Automation The LiveSYNC app on Oculus Go supports a configuration file . The filename must be settings.ini and it must be copied to \\Movies\\LiveSYNC folder on the headset. (This is the same location where you will copy your own content files.) A configuration file is written using Windows INI file format. Here is an example of contents for a configuration file: [defaults] ; automatically opens given channel on startup, true/false default_channel_on_startup=true ; channel number to open, between 1000-9999 default_channel=1000 ; connection type to use (bluetooth or globalsync) default_connection_type=bluetooth ; LiveSYNC username to use, can be left blank default_livesync_name=Joan's This particular configuration would work as follows: When the LiveSYNC app is started, it will automatically move from Splash screen to Lobby . The user does not need to select a channel number from the Home screen. The app will attempt to connect using Bluetooth on channel number 1000. Once connected to the control device, the headset will appear as Joan's in the control device's Mosaic and Devices views. Note The configuration file must be saved as plain text. Do not save it in a format that adds formatting characters. For example, you can use the Notepad application on Windows.","title":"Tips"},{"location":"tutorials/oculus_go_tips/#livesync-on-oculus-go","text":"","title":"LiveSYNC on Oculus Go"},{"location":"tutorials/oculus_go_tips/#tips-tricks","text":"To conclude the tutorial, we will present a set of handy tips.","title":"Tips &amp; tricks"},{"location":"tutorials/oculus_go_tips/#labeling","text":"We highly recommend that you mark your devices with labels for the following reasons: You can easily distinguish your headsets from each other, for example, to find a particular headset's live view from the Mosaic tab. You can recognize which hand remote belongs to which headset, for example when you store them together. You can get back your headsets (and content!) after a presentation elsewhere, for example when you use them in a trade show or borrow to a colleague for his presentation. You can brand them with your company/team/project logo. You can make the headsets look fancier with a bit of decoration. In general, two-three labels are placed in different positions: Oculus is using the left side of the head strap - under the fabric - for a serial number and other product info, which you should not cover. However, the right side is empty. When you place a label there, it will be hidden during normal product use. Yet, it is easy to check when you know where to look from. This is a great place to put your owner identification label. The front side of the headset has a large solid surface that is clearly visible when the headset is being used. Obviously, this is where your company logo or fancy decoration is typically placed. Warning The headset uses its front surface for head dissipation . Do not block the airflow or you will risk running your headset too hot. It will stop and you must wait until it cools down. LiveSYNC allows giving each device a name ( LiveSYNC name ). This is shown for example in the Mosaic view. It is a good idea to print a label that contains this name and put it in a place where you can easily see it when the headset is being used. If you have a small number of headsets, consider using color coding. Add something with red color to the head strap of one headset and set its name to \"Red\". This way you can find the correct headset from the Mosaic view easily. Even if it is too far for reading a textual label, or if the user is looking away from you and you cannot see the front side. Pull the hand remote's battery cover open, and put an identification label there. This way you can easily check which hand remote belongs to which headset. Labeling examples .","title":"Labeling"},{"location":"tutorials/oculus_go_tips/#hand-remote-controllers","text":"Oculus Go headset is designed to be used with a hand-held remote controller. Its main purpose is to allow selecting items by pointing at them. In most presentations, users have no use for the controller. The presentation is controlled via the control device and users do not need to make any selections. In such a case, you may want to consider collecting all the remote controllers away . This way there will be much less hassle and no risk of someone forgetting the controller in his pocket. Remember that you still need to keep the remotes at hand, though. Whenever a headset is wakened up from sleep, its hand remote must be calibrated. This is a requirement of the Oculus platform. An assistant or the presenter himself can do this by pressing and holding the Oculus button on each hand remote one at a time. Notice that proper calibration by pointing at the correct direction is not necessary if the remotes will not be used for selections. To get rid of the calibration dialog after waking up the headset, all that is needed is a long press of the Oculus button. Note This is a known issue for Oculus. They have said that they are working on a solution to allow using the headset without performing hand remote calibration each time. However, a fix has not been released yet. Tip The Developer Mode has an interesting shortcut: to skip hand remote calibration, the user can press one of the volume keys (+/-) when the dialog appears. It is not a perfect solution as action is still required for each headset, but at least you don't need to keep the remote controllers around. Read more about the Developer Mode below. Oculus Go headset has three buttons. From left to right, Power, Volume Up, Volume Down. In Developer Mode, hand remote calibration can be skipped by pressing Volume Up/Down button .","title":"Hand remote controllers"},{"location":"tutorials/oculus_go_tips/#power-management","text":"We recommend that you increase the headset's timeout for dropping into sleep : Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Power Settings . Select Auto Sleep and set it to 5 minutes or Never . Reduce the frequency of hand remote calibration by adjusting the headset's sleep timeout . Tip You can also cover the proximity sensor (between the lenses) with a piece of tape. However, configuring Auto Sleep time works very well and we recommend that you use that option. Tip If you carry your headset in a bag in a sleep mode, you may also want to consider disabling Auto Wake-Up feature. The head strap tends to find its way in front of the proximity sensor, turn the headset on every now and then inside your bag, and drain the battery. Of course, you can also turn off the headset when you are not using it. Warning The headset uses its front surface for head dissipation . When you place the headset on a table, try to avoid placing its front side towards the table surface. It is tempting because in this way the headset is in a good balance. However, head dissipation suffers because there is no air flow around the front surface.","title":"Power management"},{"location":"tutorials/oculus_go_tips/#developer-mode","text":"You can easily activate and deactivate the Developer Mode for each headset from the Oculus accompanying app on your phone. To do this you must first create a developer organization (real or fake) on the Oculus Dashboard . After that, follow these steps: Make sure the headset you want to configure is turned on. On your phone, start Oculus app and navigate to Settings . Find the headset you want to configure from the list, and select it. Tap More Settings and then Developer Mode . Disable or enable the feature. Read more from here . Enable/disable Developer Mode by turning the switch from the Oculus accompanying app . When the Developer Mode is enabled, you can: Skip hand remote calibration by pressing volume +/- key from the headset . Copy content files back and forth using Android Debug Bridge (ADB) tool from the command line. This can be handy if you're into automating things by writing scripts. Sideload apps using the Android Debug Bridge (ADB) tool from the command line. Note When the Developer Mode is enabled, the device connects via USB cable in a different mode. It will NOT appear in the Windows Explorer.","title":"Developer Mode"},{"location":"tutorials/oculus_go_tips/#automation","text":"The LiveSYNC app on Oculus Go supports a configuration file . The filename must be settings.ini and it must be copied to \\Movies\\LiveSYNC folder on the headset. (This is the same location where you will copy your own content files.) A configuration file is written using Windows INI file format. Here is an example of contents for a configuration file: [defaults] ; automatically opens given channel on startup, true/false default_channel_on_startup=true ; channel number to open, between 1000-9999 default_channel=1000 ; connection type to use (bluetooth or globalsync) default_connection_type=bluetooth ; LiveSYNC username to use, can be left blank default_livesync_name=Joan's This particular configuration would work as follows: When the LiveSYNC app is started, it will automatically move from Splash screen to Lobby . The user does not need to select a channel number from the Home screen. The app will attempt to connect using Bluetooth on channel number 1000. Once connected to the control device, the headset will appear as Joan's in the control device's Mosaic and Devices views. Note The configuration file must be saved as plain text. Do not save it in a format that adds formatting characters. For example, you can use the Notepad application on Windows.","title":"Automation"},{"location":"tutorials/tutorials/","text":"Tutorials LiveSYNC on Oculus Go This tutorial focuses on using LiveSYNC on Oculus Go standalone VR headset.","title":"Index"},{"location":"tutorials/tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/tutorials/#livesync-on-oculus-go","text":"This tutorial focuses on using LiveSYNC on Oculus Go standalone VR headset.","title":"LiveSYNC on Oculus Go"},{"location":"user_guide/asset_management/","text":"How to copy presentation content to the director iPad? Connect the included USB 2.0 cable to your computer and iPad. Select device from iTunes on your computer. Select Apps/File Sharing. Select LiveSYNC. Click and drag files and folders to LiveSYNC Documents. Open LiveSYNC on iPad -> Select channel number -> Player -> and pull to refresh contents on the left side (where demo contents are located). Have a look at our video tutorial to see how files are copied. After adding video, image, and .png hotspot files to LiveSYNC pull-to-refresh and update contents of LiveSYNC.","title":"4. Managing assets"},{"location":"user_guide/asset_management/#how-to-copy-presentation-content-to-the-director-ipad","text":"Connect the included USB 2.0 cable to your computer and iPad. Select device from iTunes on your computer. Select Apps/File Sharing. Select LiveSYNC. Click and drag files and folders to LiveSYNC Documents. Open LiveSYNC on iPad -> Select channel number -> Player -> and pull to refresh contents on the left side (where demo contents are located). Have a look at our video tutorial to see how files are copied. After adding video, image, and .png hotspot files to LiveSYNC pull-to-refresh and update contents of LiveSYNC.","title":"How to copy presentation content to the director iPad?"},{"location":"user_guide/configuration/","text":"","title":"3. Configuration"},{"location":"user_guide/editor/","text":"The Editor is a tab in the control device, which provides persistency for tags. You can mark points-of-interest with simple passive tags or add interactive (selectable) hotspots. Such configurations can be saved as projects and used in presentations. They can also be opened again for editing, or exported as a report and sent to a colleague. The Editor tab is a feature in the Enterprise version of the LiveSYNC app. Users who don't have the Enterprise version do not have the Editor tab and should skip this chapter. Exporting presentation content After adding hotspot make sure you press save icon and switch to Editor tab if tags (hotspots) are added on player mode. Drag the saved content to center labeled with \u201cDrag content here\u201d on Editor tab. You can add tags and also add note, title and description and even adjust size of tag on editor mode by simply tapping the added tag on the content. An editor window will appear on the right side of the screen. If you want to export the content with all remarks, notes and tags simply press Export from top menu and Document details dialog view will appear, give document title as file name and other optional fields if needed then press OK . LiveSYNC will take snapshots and convert that pdf and will display the pdf preview for you with an option to print, save, mail and share on social media.","title":"11. Editor"},{"location":"user_guide/editor/#exporting-presentation-content","text":"After adding hotspot make sure you press save icon and switch to Editor tab if tags (hotspots) are added on player mode. Drag the saved content to center labeled with \u201cDrag content here\u201d on Editor tab. You can add tags and also add note, title and description and even adjust size of tag on editor mode by simply tapping the added tag on the content. An editor window will appear on the right side of the screen. If you want to export the content with all remarks, notes and tags simply press Export from top menu and Document details dialog view will appear, give document title as file name and other optional fields if needed then press OK . LiveSYNC will take snapshots and convert that pdf and will display the pdf preview for you with an option to print, save, mail and share on social media.","title":"Exporting presentation content"},{"location":"user_guide/hotspots/","text":"","title":"9. Tags & hotspots"},{"location":"user_guide/installing/","text":"Installing System Requirements Your hardware must meet the minimum technical specifications outlined below to run and use LiveSYNC. iOS Feature Minimum requirement Recommendation Form factor Phone or Tablet Tablet for Director mode Operating system iOS 8.0 or later iOS 12.0 or later Android Feature Minimum requirement Recommendation Form factor Phone or Tablet Tablet for Director mode Operating system Android 5.0 (API 21) or later Android 6.0 (API 23) or later Oculus Feature Minimum requirement Recommendation Form factor GearVR or Oculus Go Oculus Go Operating system Android 7.0 (API 24) or later Android 7.0 (API 24) or later Download and install LiveSYNC iOS Android Oculus","title":"2. Installing"},{"location":"user_guide/installing/#installing","text":"","title":"Installing"},{"location":"user_guide/installing/#system-requirements","text":"Your hardware must meet the minimum technical specifications outlined below to run and use LiveSYNC.","title":"System Requirements"},{"location":"user_guide/installing/#ios","text":"Feature Minimum requirement Recommendation Form factor Phone or Tablet Tablet for Director mode Operating system iOS 8.0 or later iOS 12.0 or later","title":"iOS"},{"location":"user_guide/installing/#android","text":"Feature Minimum requirement Recommendation Form factor Phone or Tablet Tablet for Director mode Operating system Android 5.0 (API 21) or later Android 6.0 (API 23) or later","title":"Android"},{"location":"user_guide/installing/#oculus","text":"Feature Minimum requirement Recommendation Form factor GearVR or Oculus Go Oculus Go Operating system Android 7.0 (API 24) or later Android 7.0 (API 24) or later","title":"Oculus"},{"location":"user_guide/installing/#download-and-install-livesync","text":"","title":"Download and install LiveSYNC"},{"location":"user_guide/installing/#ios_1","text":"","title":"iOS"},{"location":"user_guide/installing/#android_1","text":"","title":"Android"},{"location":"user_guide/installing/#oculus_1","text":"","title":"Oculus"},{"location":"user_guide/introduction/","text":"Introduction to LiveSYNC What's new What is LiveSYNC? Human Communication is Visual During the past few decades the way humans communicate with each other has permanently changed. First we learned to connect computers together to make a giant web where messages can travel from one corner of the planet to another in a fraction of a second. That was a major breakthrough in communication, although in its early days data transfer rates were practical mainly for transmitting text. Along with many technological advancements, which made the web ever faster, step-by-step we started to use it for other forms of communication that required higher data rates : photographs, voice calls, music, and finally high-definition video. As a consequence, we began to see a paradigm shift in human communication from textual to visual representation: Instead of writing long carefully thought out emails we started to draft lots of short messages, where tiny iconic images called smileys where injected to communicate common feelings via facial expressions. Then, we further reduced textual part to a simple caption or a foreword to a photograph we had taken: the image had become the message itself. And now we have moved on to video and live in the age of Youtube stars when anyone can start their own TV show with a press of a button. The world now is more visual that it has ever been in human history. 93% of all human communication is visual. ( Ref 1 ) Why are we so fond of photos and videos? What is wrong in plain old books and letters? There is clearly room for all kinds of communication methods, as the new ones do not replace the old ones; they add to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of visual kind. We can even argue that humans are visual by nature: Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. ( Ref 1 ) It's All About Presence Looking back, it seems that whenever the transfer rates of Internet connections have taken a step forward, we've immediately found a new, richer communication method that provides a better way to capture and share what is happening around us: a way that communicates presence better by adding some essential parameter to the equation that was missing before. That is how those much sought out killer apps are made again and again. Can you imagine what will be the next step on this path? One way to understand it all is to see the changes as stepping stones towards true telepresence ; a combination of technologies that allow a person to feel as if they were present in another place, maybe also in another time. This is where we are heading to, and it is not that far off anymore. We can already see that during the past five years technology has matured enough to solve one more profounding road block: the image frame that limits our field of view. Just think about it: how could you ever feel being there if you weren't even able to turn your head to look around? By minimum, you should be able to freely observe the surroundings by looking at any direction you want, at any moment of time - and experience the view and soundscape change corresponding to your head movements. The technologies that allow capturing the world surrounding us are called 360-degree photography or 360-degree videography, accompanied with spatial audio. The content is typically experienced through ordinary computer screens, smartphones, tablets, or virtual reality headsets. For this purpose, new kind of image viewers and video players have been developed to allow interacting with the content at presentation time. It is interesting to notice that moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate. We have been able to watch movies in 4K quality over Internet quite some time now, so it is not a coincident that 360-degree content is becoming popular just now when Internet is becoming fast enough to carry the amount of data required for it. Fiber and 5G networks are exactly what is needed: Consider a single frame of 4K video that contains 3840x2160=8.3M pixels. If the camera that captured the frame was zoomed out to produce a wide-angle shot the frame could contain ~90-degree field-of-view horizontally, resulting to 3840/90=~43 pixels per degree. In order to produce full 360-degree view at the same resolution (amount of pixels per degree), we'd need roughly four similar frames side-by-side, resulting to 4x3840=15360 pixels horizontally. Since full spherical image is 360 degrees horizontally but only 180 degrees vertically, we need only half of that number vertically to produce a full spherical image. Hence, to experience \"4K quality in 360-degrees\", our image frame would have to be 15360x7680=118M pixels. This is over 14 times the amount of pixels in a 4K video frame! However, if we settle with \"FullHD quality in 360-degrees\" or 1920/90=21.3 pixels per degree then our image frame size becomes 7680x3840=29.5M pixels, which is \"only\" 3.5 times the amount of pixels in a 4K video frame. In early 2019 360-degree cameras that capture video in 7680x3840 resolution are becoming commonplace, which means that we are now entering the \"FullHD\" time of 360-degree video, and \"4K\" time of 360-degree video is still a few years away. We can also calculate it like this: a compressed 4K video requires roughly 40 Mbit/s bitrate, hence scaling up with factor 3.5 yields 140 Mbit/s data rate for \"FullHD quality in 360-degrees\", which could pass through best current mobile LTE networks that offer 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for \"4K quality in 360-degrees\". We will need one gigabit Internet connection to view 360-degree video in perceived quality that we know as \"4K\". Upcoming mobile 5G networks promise such speeds. This is the big picture and the part where LiveSYNC as a tool enters the scene. 360-degree imaging can be now considered largely a solved problem, both from content production and presentation point of view, and we are mostly improving quality now. It is mature and cost-effective enough already to be used in daily activities. We can also see the fog disappearing and revealing the next stepping stone: while 360-degree video allows the viewer to turn around, volumetric video will remove the limitation that prevents the viewer from taking a step inside the world captured with cameras, and thus will make us feel being present even more. That technology is now taking its first steps and needs a few more years to begin to \"work\", and some more before the cameras and networks catch up. There is no reason to wait, though: the future will always bring better, more advanced technology. So let us consider what it means that we can now let go of the limitations of image frame, after 200 years of photography. That is already a miracle of its own! Removing the Frame The advances in technology are pushing another paradigm shift: many features of a photo that have been traditionally decided in advance or by latest when the camera shutter closes, can now be changed afterwards - up to the point where the person viewing the content becomes the decision maker. For example, retouching images is so commonplace that photoshopping has become a household word. Light-field cameras allow focusing the image after it has been taken - a stunt that seems almost impossible. Another miracle is 360-degree video, where every direction is captured simultaneously and there are no traces of a camera nor cameramen: the point of view simply seems to float in the air. This has profound effects as we will soon see. Traditionally, a photographer or a videographer has decided where to point his camera and how to frame the shot: what is necessary and what can be left out. This decision is locked at the time of capture, and becomes a permanent limitation: at viewing time it is not possible to see beyond the frame, in fact the only possibility to re-frame the shot is to limit the viewing angle even more by cropping the image. Imagine a zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what happens when you keep zooming out? You will see more and more of the surrounding area become visible in the image, although heavily distorted. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. While we may never be able to physically manufacture a lens that could capture everything at once, digital image processing comes to the rescue and effectively combines the images captured through two or more lenses by stitching them together to a (more-or-less) seamless representation. 360-degree photo and video panoramas (also known as immersive videos , spherical videos , or VR videos ) capture the world in every direction. During playback it is the viewer who has control of the viewing direction: he can freely turn around and even look up to the sky or down to his feet. In fact, we are so used to the frame that limits the view that it feels like magic when you experience this freedom the very first time. Making the most out of such content requires new kinds of tools. By minimum, a photo/video player application must support special projections that are used for storing wide-angle panoramic content, and provide means for the viewer to pan the image during playback. LiveSYNC is such a tool, but also much more, as you will soon learn. While traditional photos are great for creating memories and videos are perfect for telling stories, the main driving force for shooting something in 360\u00b0 is the feeling of presence that you get when viewing the content. Even more immersive experiences can be achieved when 360\u00b0 video is combined with 360\u00b0 audio (also known as positional or spatial audio), and viewed through a virtual reality (VR) headset. You'll feel like being there . This experience is in fact so immersive that it also isolates the viewer from the rest of the world. It becomes a real problem when you are presenting content to another person and need to communicate with him during playback, for example, to give instructions, ask for an opinion, or simply to share the experience by chatting about it as the story unfolds. Yet, not being able to see what he sees forces you to continuously ask the viewer to explain what's on the screen. How awkward! LiveSYNC removes this barrier by creating a screen mirroring style copy of the view from the VR headset to another device, typically a tablet computer. This view is updated in near real-time and can be further shared to a big screen, for example a TV or a projector, so that even a large audience - perhaps waiting for their own turn - can share the experience. Nobody likes waiting, though. Fortunately, with LiveSYNC you can connect to multiple headsets, play content synchronized in time, and observe the views of all of the devices simultaneously in a gorgeous video mosaic. You can also mix in traditional photos and videos, for example your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool. Yet, not all 360\u00b0 content is made for entertainment. Imagine that you are selling your summer cottage and made a set of 360\u00b0 photos and videos so that potential buyers can virtually visit the place without spending hours in a car. But wouldn't it be convenient if you could also add a few notes to the images, to highlight things you want them to notice and remember? This is a built-in feature in LiveSYNC: when presenting content, simply drag & drop icons from included clipart sets over the photo or video - or create your own signs and use them instead. Is it possible to add text too? Of course. The enterprise version of LiveSYNC goes much further: you can use a built-in editor to create a project, import media content, add interactive hotspots that can even fetch data from the network in real-time, import 2D and 3D maps with camera paths, etc. A really useful feature is to be able to export all the notes you've made as screenshots into a visual PDF report, and send it to your colleagues right away. Take a 5-minute tour on your construction site with a 360\u00b0 camera, add new work instructions, export to PDF and mail away - all in 15 minutes. That's what we call instant digitalization . This is LiveSYNC - and we are only getting started. Welcome aboard! Key Concepts TODO Understanding view mirroring How does LiveSYNC actually work? What is possible and what is not? In general, there are two ways to mirror a live view of one device's screen to another: Streaming video (screencasting) Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection. One benefit of this approach is that the mirrored view is an exact copy of what the other device is drawing on its screen. Another is that the mirroring feature can be implemented on the platform level and is thus available for all apps or the whole desktop environment. There are many downsides, too. Because it is essentially streaming high resolution video from one device to another, the method consumes a fair amount of CPU/GPU resources and power in both devices. It also requires a lot of network bandwidth when there is movement on screen, and it does not scale well: the network and the video decoder in the receiving end quickly become a bottleneck when more devices connect. Furthermore, when using a VR headset, we do not actually want to stream an exact copy of what is on screen. Instead of the distorted double barrel view we want to mirror a normal view. Streaming commands Principle: Integrate deep into the app and transmit only commands that allow the receiving end to reconstruct the view from the same assets. The benefits of this method include low CPU/GPU and power consumption, trivial use of network bandwidth, and scalability. It is also possible to render the mirrored view a bit differently compared to the source device. The downside is that all devices must have local copies of the assets required for reconstructing the view. Also, with video content the resources of the control device can become a limitation in certain use cases. A control device may not have enough hardware resources to decode and play multiple videos simultaneously. Or, the same video simultaneously from different stream positions. This is not a problem, though, when all devices watch the same content in sync. Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. Wifi does not work well enough and/or the organizers do not even allow using own Wifi hotspots. As you probably guessed already, LiveSYNC uses type 2 method: streaming commands. This approach allows operation in crowded trade shows, especially when Bluetooth technology is used for communication. Also, observing a large number of viewer devices is possible. Etc.","title":"1. Introduction"},{"location":"user_guide/introduction/#introduction-to-livesync","text":"","title":"Introduction to LiveSYNC"},{"location":"user_guide/introduction/#whats-new","text":"","title":"What's new"},{"location":"user_guide/introduction/#what-is-livesync","text":"","title":"What is LiveSYNC?"},{"location":"user_guide/introduction/#human-communication-is-visual","text":"During the past few decades the way humans communicate with each other has permanently changed. First we learned to connect computers together to make a giant web where messages can travel from one corner of the planet to another in a fraction of a second. That was a major breakthrough in communication, although in its early days data transfer rates were practical mainly for transmitting text. Along with many technological advancements, which made the web ever faster, step-by-step we started to use it for other forms of communication that required higher data rates : photographs, voice calls, music, and finally high-definition video. As a consequence, we began to see a paradigm shift in human communication from textual to visual representation: Instead of writing long carefully thought out emails we started to draft lots of short messages, where tiny iconic images called smileys where injected to communicate common feelings via facial expressions. Then, we further reduced textual part to a simple caption or a foreword to a photograph we had taken: the image had become the message itself. And now we have moved on to video and live in the age of Youtube stars when anyone can start their own TV show with a press of a button. The world now is more visual that it has ever been in human history. 93% of all human communication is visual. ( Ref 1 ) Why are we so fond of photos and videos? What is wrong in plain old books and letters? There is clearly room for all kinds of communication methods, as the new ones do not replace the old ones; they add to the options that we have. Yet, we must embrace those methods that come so natural from us: the ones of visual kind. We can even argue that humans are visual by nature: Humans process visuals 60 000x faster than text. 81% of people only skim content they read online. People learn 40% better when there are visuals. ( Ref 1 )","title":"Human Communication is Visual"},{"location":"user_guide/introduction/#its-all-about-presence","text":"Looking back, it seems that whenever the transfer rates of Internet connections have taken a step forward, we've immediately found a new, richer communication method that provides a better way to capture and share what is happening around us: a way that communicates presence better by adding some essential parameter to the equation that was missing before. That is how those much sought out killer apps are made again and again. Can you imagine what will be the next step on this path? One way to understand it all is to see the changes as stepping stones towards true telepresence ; a combination of technologies that allow a person to feel as if they were present in another place, maybe also in another time. This is where we are heading to, and it is not that far off anymore. We can already see that during the past five years technology has matured enough to solve one more profounding road block: the image frame that limits our field of view. Just think about it: how could you ever feel being there if you weren't even able to turn your head to look around? By minimum, you should be able to freely observe the surroundings by looking at any direction you want, at any moment of time - and experience the view and soundscape change corresponding to your head movements. The technologies that allow capturing the world surrounding us are called 360-degree photography or 360-degree videography, accompanied with spatial audio. The content is typically experienced through ordinary computer screens, smartphones, tablets, or virtual reality headsets. For this purpose, new kind of image viewers and video players have been developed to allow interacting with the content at presentation time. It is interesting to notice that moving from traditional video to 360-degree spherical video requires over tenfold increase in data rate. We have been able to watch movies in 4K quality over Internet quite some time now, so it is not a coincident that 360-degree content is becoming popular just now when Internet is becoming fast enough to carry the amount of data required for it. Fiber and 5G networks are exactly what is needed: Consider a single frame of 4K video that contains 3840x2160=8.3M pixels. If the camera that captured the frame was zoomed out to produce a wide-angle shot the frame could contain ~90-degree field-of-view horizontally, resulting to 3840/90=~43 pixels per degree. In order to produce full 360-degree view at the same resolution (amount of pixels per degree), we'd need roughly four similar frames side-by-side, resulting to 4x3840=15360 pixels horizontally. Since full spherical image is 360 degrees horizontally but only 180 degrees vertically, we need only half of that number vertically to produce a full spherical image. Hence, to experience \"4K quality in 360-degrees\", our image frame would have to be 15360x7680=118M pixels. This is over 14 times the amount of pixels in a 4K video frame! However, if we settle with \"FullHD quality in 360-degrees\" or 1920/90=21.3 pixels per degree then our image frame size becomes 7680x3840=29.5M pixels, which is \"only\" 3.5 times the amount of pixels in a 4K video frame. In early 2019 360-degree cameras that capture video in 7680x3840 resolution are becoming commonplace, which means that we are now entering the \"FullHD\" time of 360-degree video, and \"4K\" time of 360-degree video is still a few years away. We can also calculate it like this: a compressed 4K video requires roughly 40 Mbit/s bitrate, hence scaling up with factor 3.5 yields 140 Mbit/s data rate for \"FullHD quality in 360-degrees\", which could pass through best current mobile LTE networks that offer 300 Mbit/s peak data rate. Yet, with factor 14 the requirement becomes a whopping 560 Mbit/s for \"4K quality in 360-degrees\". We will need one gigabit Internet connection to view 360-degree video in perceived quality that we know as \"4K\". Upcoming mobile 5G networks promise such speeds. This is the big picture and the part where LiveSYNC as a tool enters the scene. 360-degree imaging can be now considered largely a solved problem, both from content production and presentation point of view, and we are mostly improving quality now. It is mature and cost-effective enough already to be used in daily activities. We can also see the fog disappearing and revealing the next stepping stone: while 360-degree video allows the viewer to turn around, volumetric video will remove the limitation that prevents the viewer from taking a step inside the world captured with cameras, and thus will make us feel being present even more. That technology is now taking its first steps and needs a few more years to begin to \"work\", and some more before the cameras and networks catch up. There is no reason to wait, though: the future will always bring better, more advanced technology. So let us consider what it means that we can now let go of the limitations of image frame, after 200 years of photography. That is already a miracle of its own!","title":"It's All About Presence"},{"location":"user_guide/introduction/#removing-the-frame","text":"The advances in technology are pushing another paradigm shift: many features of a photo that have been traditionally decided in advance or by latest when the camera shutter closes, can now be changed afterwards - up to the point where the person viewing the content becomes the decision maker. For example, retouching images is so commonplace that photoshopping has become a household word. Light-field cameras allow focusing the image after it has been taken - a stunt that seems almost impossible. Another miracle is 360-degree video, where every direction is captured simultaneously and there are no traces of a camera nor cameramen: the point of view simply seems to float in the air. This has profound effects as we will soon see. Traditionally, a photographer or a videographer has decided where to point his camera and how to frame the shot: what is necessary and what can be left out. This decision is locked at the time of capture, and becomes a permanent limitation: at viewing time it is not possible to see beyond the frame, in fact the only possibility to re-frame the shot is to limit the viewing angle even more by cropping the image. Imagine a zoom lens that has an unlimited range. If you kept zooming in, you could see a grain of dust on the surface of the Moon. But what happens when you keep zooming out? You will see more and more of the surrounding area become visible in the image, although heavily distorted. And then, the two sides of the image suddenly meet each other, producing a full 360-degree view. While we may never be able to physically manufacture a lens that could capture everything at once, digital image processing comes to the rescue and effectively combines the images captured through two or more lenses by stitching them together to a (more-or-less) seamless representation. 360-degree photo and video panoramas (also known as immersive videos , spherical videos , or VR videos ) capture the world in every direction. During playback it is the viewer who has control of the viewing direction: he can freely turn around and even look up to the sky or down to his feet. In fact, we are so used to the frame that limits the view that it feels like magic when you experience this freedom the very first time. Making the most out of such content requires new kinds of tools. By minimum, a photo/video player application must support special projections that are used for storing wide-angle panoramic content, and provide means for the viewer to pan the image during playback. LiveSYNC is such a tool, but also much more, as you will soon learn. While traditional photos are great for creating memories and videos are perfect for telling stories, the main driving force for shooting something in 360\u00b0 is the feeling of presence that you get when viewing the content. Even more immersive experiences can be achieved when 360\u00b0 video is combined with 360\u00b0 audio (also known as positional or spatial audio), and viewed through a virtual reality (VR) headset. You'll feel like being there . This experience is in fact so immersive that it also isolates the viewer from the rest of the world. It becomes a real problem when you are presenting content to another person and need to communicate with him during playback, for example, to give instructions, ask for an opinion, or simply to share the experience by chatting about it as the story unfolds. Yet, not being able to see what he sees forces you to continuously ask the viewer to explain what's on the screen. How awkward! LiveSYNC removes this barrier by creating a screen mirroring style copy of the view from the VR headset to another device, typically a tablet computer. This view is updated in near real-time and can be further shared to a big screen, for example a TV or a projector, so that even a large audience - perhaps waiting for their own turn - can share the experience. Nobody likes waiting, though. Fortunately, with LiveSYNC you can connect to multiple headsets, play content synchronized in time, and observe the views of all of the devices simultaneously in a gorgeous video mosaic. You can also mix in traditional photos and videos, for example your company's slide deck and promo video. This makes LiveSYNC a versatile all-around presentation tool. Yet, not all 360\u00b0 content is made for entertainment. Imagine that you are selling your summer cottage and made a set of 360\u00b0 photos and videos so that potential buyers can virtually visit the place without spending hours in a car. But wouldn't it be convenient if you could also add a few notes to the images, to highlight things you want them to notice and remember? This is a built-in feature in LiveSYNC: when presenting content, simply drag & drop icons from included clipart sets over the photo or video - or create your own signs and use them instead. Is it possible to add text too? Of course. The enterprise version of LiveSYNC goes much further: you can use a built-in editor to create a project, import media content, add interactive hotspots that can even fetch data from the network in real-time, import 2D and 3D maps with camera paths, etc. A really useful feature is to be able to export all the notes you've made as screenshots into a visual PDF report, and send it to your colleagues right away. Take a 5-minute tour on your construction site with a 360\u00b0 camera, add new work instructions, export to PDF and mail away - all in 15 minutes. That's what we call instant digitalization . This is LiveSYNC - and we are only getting started. Welcome aboard!","title":"Removing the Frame"},{"location":"user_guide/introduction/#key-concepts","text":"TODO","title":"Key Concepts"},{"location":"user_guide/introduction/#understanding-view-mirroring","text":"How does LiveSYNC actually work? What is possible and what is not? In general, there are two ways to mirror a live view of one device's screen to another: Streaming video (screencasting) Principle: Digitally record screenshots from the device's screen, encode them into a video stream, and send this stream to another device over a network connection. One benefit of this approach is that the mirrored view is an exact copy of what the other device is drawing on its screen. Another is that the mirroring feature can be implemented on the platform level and is thus available for all apps or the whole desktop environment. There are many downsides, too. Because it is essentially streaming high resolution video from one device to another, the method consumes a fair amount of CPU/GPU resources and power in both devices. It also requires a lot of network bandwidth when there is movement on screen, and it does not scale well: the network and the video decoder in the receiving end quickly become a bottleneck when more devices connect. Furthermore, when using a VR headset, we do not actually want to stream an exact copy of what is on screen. Instead of the distorted double barrel view we want to mirror a normal view. Streaming commands Principle: Integrate deep into the app and transmit only commands that allow the receiving end to reconstruct the view from the same assets. The benefits of this method include low CPU/GPU and power consumption, trivial use of network bandwidth, and scalability. It is also possible to render the mirrored view a bit differently compared to the source device. The downside is that all devices must have local copies of the assets required for reconstructing the view. Also, with video content the resources of the control device can become a limitation in certain use cases. A control device may not have enough hardware resources to decode and play multiple videos simultaneously. Or, the same video simultaneously from different stream positions. This is not a problem, though, when all devices watch the same content in sync. Many devices have type 1 screen mirroring built-in. This is often handy for mirroring your phone's screen on TV at home. Or, in a meeting room at the office. However, mirroring a view by streaming video over Wifi becomes often impossible in crowded places. Wifi does not work well enough and/or the organizers do not even allow using own Wifi hotspots. As you probably guessed already, LiveSYNC uses type 2 method: streaming commands. This approach allows operation in crowded trade shows, especially when Bluetooth technology is used for communication. Also, observing a large number of viewer devices is possible. Etc.","title":"Understanding view mirroring"},{"location":"user_guide/photo_content/","text":"","title":"7. Photo content"},{"location":"user_guide/presenting/","text":"","title":"6. Presenting"},{"location":"user_guide/troubleshooting/","text":"A quick checklist Install LiveSYNC app for your director device (iPad) and all client devices Ensure that Bluetooth is enabled on all devices On your iPad, set up a new presentation channel as Director and take note the 4-digit channel number that is given to you. On your client devices, set up the presentation channel as Audience and make sure to use the 4-digit channel number you got in previous step. On your iPad, select your presentation channel (\"Control\") to go to Director mode, you'll enter the Mosaic view first. On your client devices, select the presentation channel (\"Join) to go to Audience mode, you'll enter the waiting room first. Shortly, the devices should connect automatically ie. the clients should appear on Director's Mosaic view, and clients should notify that presentation begins soon. Once connected, on your iPad move to Player view and drag one of the default contents to Presentation view (PowerPark or Olos), it should play on all connected devices. Note Notice that when you use your own content, you need to copy it to Director devices AND all Audience devices before you can present them.. In case you have connection issues try these quick fixes in order: Double check that Bluetooth is enabled and you are not in Airplane mode. Turn Bluetooth off and back on shortly after, especially on the iPad. Check that your device supports Bluetooth 4.0 (should not be possible to install from store if it doesn't). If your own content files do not work: Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to device's maximum size. On iOS devices, make sure to use stereo audio - not multi-channel If your audience devices do not connect. Make sure Bluetooth is enabled on all devices. In temporary connection problems, first disable and re-enable bluetooth on the director device, then (if necessary) on the client device.","title":"12. Troubleshooting"},{"location":"user_guide/troubleshooting/#a-quick-checklist","text":"Install LiveSYNC app for your director device (iPad) and all client devices Ensure that Bluetooth is enabled on all devices On your iPad, set up a new presentation channel as Director and take note the 4-digit channel number that is given to you. On your client devices, set up the presentation channel as Audience and make sure to use the 4-digit channel number you got in previous step. On your iPad, select your presentation channel (\"Control\") to go to Director mode, you'll enter the Mosaic view first. On your client devices, select the presentation channel (\"Join) to go to Audience mode, you'll enter the waiting room first. Shortly, the devices should connect automatically ie. the clients should appear on Director's Mosaic view, and clients should notify that presentation begins soon. Once connected, on your iPad move to Player view and drag one of the default contents to Presentation view (PowerPark or Olos), it should play on all connected devices. Note Notice that when you use your own content, you need to copy it to Director devices AND all Audience devices before you can present them..","title":"A quick checklist"},{"location":"user_guide/troubleshooting/#in-case-you-have-connection-issues-try-these-quick-fixes-in-order","text":"Double check that Bluetooth is enabled and you are not in Airplane mode. Turn Bluetooth off and back on shortly after, especially on the iPad. Check that your device supports Bluetooth 4.0 (should not be possible to install from store if it doesn't).","title":"In case you have connection issues try these quick fixes in order:"},{"location":"user_guide/troubleshooting/#if-your-own-content-files-do-not-work","text":"Make sure to use .mp4 format videos, .jpg format photos, and .png format hotspot images. If some of your devices do not support 4K video (3840x1920), you can copy there a lower quality variant (1920x960). Photos will be automatically scaled to device's maximum size. On iOS devices, make sure to use stereo audio - not multi-channel If your audience devices do not connect. Make sure Bluetooth is enabled on all devices. In temporary connection problems, first disable and re-enable bluetooth on the director device, then (if necessary) on the client device.","title":"If your own content files do not work:"},{"location":"user_guide/user_guide/","text":"User Guide Welcome to the LiveSYNC User Guide ! Choose a topic from below to find answers, get step-by-step instructions, and develop your skills. Warning The User Guide is currently under construction. 1. Introduction 2. Installing 3. Configuration 4. Managing assets 5. Workspace 6. Presenting 7. Photo content 8. Video content 9. Tags & hotspots 10. Workflows 11. Editor 12. Troubleshooting","title":"Index"},{"location":"user_guide/user_guide/#user-guide","text":"Welcome to the LiveSYNC User Guide ! Choose a topic from below to find answers, get step-by-step instructions, and develop your skills. Warning The User Guide is currently under construction.","title":"User Guide"},{"location":"user_guide/user_guide/#1-introduction","text":"","title":"1. Introduction"},{"location":"user_guide/user_guide/#2-installing","text":"","title":"2. Installing"},{"location":"user_guide/user_guide/#3-configuration","text":"","title":"3. Configuration"},{"location":"user_guide/user_guide/#4-managing-assets","text":"","title":"4. Managing assets"},{"location":"user_guide/user_guide/#5-workspace","text":"","title":"5. Workspace"},{"location":"user_guide/user_guide/#6-presenting","text":"","title":"6. Presenting"},{"location":"user_guide/user_guide/#7-photo-content","text":"","title":"7. Photo content"},{"location":"user_guide/user_guide/#8-video-content","text":"","title":"8. Video content"},{"location":"user_guide/user_guide/#9-tags-hotspots","text":"","title":"9. Tags &amp; hotspots"},{"location":"user_guide/user_guide/#10-workflows","text":"","title":"10. Workflows"},{"location":"user_guide/user_guide/#11-editor","text":"","title":"11. Editor"},{"location":"user_guide/user_guide/#12-troubleshooting","text":"","title":"12. Troubleshooting"},{"location":"user_guide/video_content/","text":"","title":"8. Video content"},{"location":"user_guide/workflows/","text":"","title":"10. Workflows"},{"location":"user_guide/workspace/","text":"","title":"5. Workspace"}]}